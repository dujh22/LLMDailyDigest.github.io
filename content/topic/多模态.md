+++
title = '多模态'
date = 2025-08-12T00:00:00+08:00
draft = false
toc = true
+++

# 多模态

1. 上海人工智能实验室、北京理工大学、上海创智学院、东京大学等机构 **聚焦世界生成的第一步——世界探索** ，联合推出一个**持续迭代的高质量视频数据集项目——Sekai**[ https://mp.weixin.qq.com/s/gNcdw9cu7LDXowtrlrtx-g](https://mp.weixin.qq.com/s/gNcdw9cu7LDXowtrlrtx-g)
2. 谢赛宁团队新作：不用提示词精准实现3D画面控制[https://mp.weixin.qq.com/s/QxCaooIDYWFtQMFr_Otqdw](https://mp.weixin.qq.com/s/QxCaooIDYWFtQMFr_Otqdw)
3. GitHub一周2000星！国产统一图像生成模型神器升级，理解质量双up，还学会了“反思”[https://mp.weixin.qq.com/s/WpDHa_YxZIWT4xrXA9sAjA](https://mp.weixin.qq.com/s/WpDHa_YxZIWT4xrXA9sAjA)新进展来自智源研究院：一模支持文生图、图像编辑、主题驱动图像生成的 **OmniGen** ，2.0新版本正式发布。
4. 大模型时代，通用视觉模型将何去何从？[https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q](https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q)** 清华大学自动化系鲁继文团队**最近发表于 IJCV 的综述论文系统梳理了该方向的研究进展，涵盖输入统一方法、任务通用策略、模型框架设计、模型评测应用等内容，希望能为未来视觉模型的发展提供参考与启发。
5. 2025-06-30 16:13:39 Monday ｜ 充分激发模态协作，MokA量身打造MLLM微调新范式

* 论文标题：MokA: Multimodal Low-Rank Adaptation for MLLMs
* 论文链接：https://arxiv.org/abs/2506.05191
* 项目主页：https://gewu-lab.github.io/MokA

MokA 在结构上继承了 LoRA 的核心思想，以保持高效的优点。但基于多模态场景对于 A、B 投影矩阵的角色进行了重新定义。如上图所示，MokA 包括三个关键模块：模态特异的 A 矩阵，跨模态注意力机制和模态共享的 B 矩阵。

6. 2025-06-13 18:17:54 Friday | 何恺明改进了谢赛宁的REPA：极大简化但性能依旧强悍https://mp.weixin.qq.com/s/t-fjuuLsCO0kywKrflve9w
7. CVPR 2025 Award Candidate | 英伟达等Difix3D+：用单步扩散模型修复 3D 重建伪影 机器之心 2025年06月23日 12:05

https://mp.weixin.qq.com/s/C9XQZDuI1D9mQmyiSsOTvg

来自英伟达的研究团队联合提出了一种创新方案 —— Difix3D+，通过单步扩散模型对 3D 渲染结果进行 “图像修复”，显著提升新视角图像的质量和一致性

## 综述

2025-07-03 09:45:11 Thursday ｜ 回顾并总结纯视觉范式下的通用视觉模型研究 https://mp.weixin.qq.com/s/yJ6U367pd1o6pqzEs1xi_Q

1. **清华大学自动化系鲁继文团队**最近发表于 IJCV 的综述论文系统梳理了该方向的研究进展，涵盖输入统一方法、任务通用策略、模型框架设计、模型评测应用等内容，希望能为未来视觉模型的发展提供参考与启发。
2. 论文标题：Vision Generalist Model: A Survey
3. 论文链接：https://arxiv.org/abs/2506.09954
4. 挑战：如何优化统一框架设计、提高训练效率和应对大规模数据等挑战

2025-07-02 13:03:30 Wednesday ｜ UofT、UBC、MIT和复旦等联合发布：扩散模型驱动的异常检测与生成全面综述 https://mp.weixin.qq.com/s/smDyNhlGS-SpM2Wt34J-ng

## 模型

#### [小红书开源首个多模态大模型dots.vlm1，性能直追SOTA！](https://mp.weixin.qq.com/s/aftyEJCZleUGAp0s5NBk3w)

1. 小红书人文智能实验室（Humane Intelligence Lab，hi lab）在昨天低调开源了视觉语言模型dots.vlm1

   1. Github Repo：https://github.com/rednote-hilab/dots.vlm1
   2. Huggingface Model：https://huggingface.co/rednote-hilab/dots.vlm1.inst
   3. Demo ：https://huggingface.co/spaces/rednote-hilab/dots-vlm1-demo
2. 不论是空间关系理解、复杂图表推理、OCR识别、高考题评测、STEM难题、写诗等各个方面，dots.vlm1的表现都远超预期。
3. dots.vlm1由三个核心组件构成：一个全自研的12亿参数的NaViT视觉编码器、一个轻量级的MLP适配器，以及DeepSeek V3 MoE大语言模型。这一架构通过三阶段流程进行训练：**第一阶段：视觉编码器预训练** ：NaViT编码器从头训练，旨在最大化对多样视觉数据的感知能力。一般来说，编码器是否自研是VLM模型性能的分水岭。dots.vlm1再次验证了这一点。**第二阶段：VLM预训练** ：将视觉编码器与DeepSeek V3 LLM联合训练，使用大规模、多样化的多模态数据集。**第三阶段：VLM后训练** ：通过有监督微调（SFT）增强模型的泛化能力，仅使用任务多样的数据进行训练。

## 图片理解与生成

2025-07-03 11:28:29 Thursday ｜ 超CLIP准确率11%！伯克利港大阐明「LLM文本-视觉」对齐深层机制 https://mp.weixin.qq.com/s/YCOGZBHbkdmcUF92xHOHYQ

近期，相关工作尝试将预训练的大语言模型（LLM）作为文本编码器融入多模态对齐框架，并在分类和检索任务上观察到性能提升。

然而，性能提升背后的机制尚不清晰，几个关键问题仍未得到系统解答：

* 能力提升的本质：LLM文本编码器的加入究竟增强了多模态模型的哪些具体能力？
* 数据特征的适配：在哪些类型的训练数据上，LLM文本编码器表现更优，原因为何？
* 关键组件的贡献：LLM文本编码器的哪些设计选择对跨模态对齐至关重要？
* 训练流程的简化：若使用LLM作为固定文本编码器，传统对比学习框架能否进一步优化？

来自UC伯克利和香港大学的研究团队在最新工作LIFT（Language-Image Alignment with Fixed Text Encoders）中，对上述问题进行了系统性解答。

论文链接：https://arxiv.org/pdf/2506.04209

项目代码：https://github.com/Jingfeng0705/LIFT

该方法采用极简训练范式——直接冻结预训练LLM作为文本编码器，仅优化图像编码器。

2. 2025-07-01 10:50:12 Tuesday ｜ 用好视觉Attention局部性，清华、字节提出Token Reorder，无损实现5倍稀疏、4比特量化 https://mp.weixin.qq.com/s/2ZWAdLD-_XdOz3kL3GRNjw

提出了一种简单且硬件友好的离线 “Token重排” 方案以实现注意力模式的统一化，并设计了针对性的稀疏与量化方法，配合高效的 CUDA 系统设计，展现了更优异的算法性能保持与硬件效率提升

论文标题：PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models

论文链接：https://arxiv.org/abs/2506.16054

项目主页：https://a-suozhang.xyz/paroattn.github.io/

3. 2025-06-30 15:49:57 Monday ｜ ICML 2025 Spotlight | 新理论框架解锁流匹配模型的引导生成 https://mp.weixin.qq.com/s/c4rDuHgOfLF3n3TxWXv2mw

对生成模型的 **能量引导** （energy guidance）是一种可控的生成方法，它构造灵活，适用于各种任务，且允许无额外训练条件生成模型。同时 **流匹配** （flow matching）框架作为一种生成模型，近期在分子生成、图片生成等领域中已经展现出巨大潜力。

* 本工作首次提出了流匹配能量引导理论框架。
* 在本框架指导下，本工作提出三大类无需训练的实用流匹配能量引导算法，并可将经典扩散模型能量引导算法包含为特例。
* 本工作给出了各个流匹配能量引导算法性能的理论分析和实验比较，为实际应用提供指导。
* 论文标题：On the Guidance of Flow Matching
* 论文链接：https://arxiv.org/abs/2502.02150
* 项目地址：https://github.com/AI4Science-WestlakeU/flow_guidance

4. 2025-06-23 12:06:35 Monday ｜ 开源版MetaQuery来了！OpenUni用1.1B参数媲美BLIP3-o-8B，数据代码完全开源 https://mp.weixin.qq.com/s/AsKAqA8NJE-S132cfM44kg

随着 GPT-4o 展现出令人印象深刻的多模态能力，将视觉理解和图像生成统一到单一模型中已成为 AI 领域的研究趋势（如MetaQuery 和 BLIP3-o ）。

南洋理工大学 S-Lab 和商汤科技的研究团队推出 OpenUni，一个开源版 MetaQuery，仅用 1.1B 参数达到 8B 模型性能，更将代码、权重、数据全部开源！

技术报告： OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation

机构： 南洋理工大学 S-Lab、商汤科技新加坡研究院

作者： Size Wu*,  Zhonghua Wu*, Zerui Gong* (* 同等贡献), Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy

开源代码：

https://github.com/wusize/OpenUni

联系方式： size001@e.ntu.edu.sg

## 视频

2025-07-03 10:24:07 Thursday ｜ 画到哪，动到哪！字节跳动发布视频生成「神笔马良」ATI，已开源！ https://mp.weixin.qq.com/s/plT9DzxmpAjQv8u87S8wlQ

字节跳动提出了 **ATI** ——一种全新的、以「轨迹为指令」的可控视频生成框架。ATI 的核心理念是： **将用户在输入图像上手绘的任意轨迹，转化为驱动物体与摄像机运动的显式控制信号，并以统一的潜在空间建模方式注入视频生成过程。** 这使得视频创作从「参数调控」转变为「可视化创意」，让用户「画到哪，动到哪」，以直观方式实现帧级精准控制。

Title：ATI: Any Trajectory Instruction for Controllable Video Generation

Paper：https://arxiv.org/pdf/2505.22944

Project page：https://anytraj.github.io/

Github：https://github.com/bytedance/ATI

Hugging Face：https://huggingface.co/bytedance-research/ATI

ComfyUI：https://github.com/kijai/ComfyUI-WanVideoWrapper

2. 2025-06-30 16:02:37 Monday ｜ 无需训练，即插即用，2倍GPU端到端推理加速——视频扩散模型加速方法DraftAttention https://mp.weixin.qq.com/s/VbWe5u5WdQfKLYOohzCZaA

现有视频生成加速方法，如 Sparse VideoGen（https://arxiv.org/abs/2502.01776）和 AdaSpa（https://arxiv.org/abs/2502.21079），多采用稀疏注意力机制，在 GPU 上实现了一定程度的端到端加速

Adobe Research 等机构的研究团队提出了一种 **无需训练、即插即用的，基于动态稀疏注意力的视频扩散模型加速方法 ——DraftAttention** ，显著降低了注意力机制的计算开销，并且在几乎不损失生成质量的前提下，实现高达 2 倍的 GPU 端到端推理加速。

论文标题：

DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance

arXiv 地址：

https://arxiv.org/abs/2505.14708

GitHub 主页：

https://github.com/shawnricecake/draft-attention

3. 2025-06-20 11:24:24 Friday ｜ 极低成本微调大规模预训练视频模型https://mp.weixin.qq.com/s/MCOrbgJvqWwFnYmavSde6w

论文标题：Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach

FVDM 论文：https://arxiv.org/abs/2410.03160

Pusa 主页 / 代码库: https://github.com/Yaofang-Liu/Pusa-VidGen

#### FastWan系视频生成模型

1. [一夜颠覆Sora神话，H200单卡5秒出片！全华人团队开源AI引爆视频圈](https://mp.weixin.qq.com/s/gx76pTlySt0Wd0vnhdby3Q)
   1. 论文地址：https://arxiv.org/pdf/2505.13389
   2. 它的核心采用了「稀疏蒸馏」全新的训练方案，实现了高效生成，让视频去噪速度实现70倍飙升。

## 视频+声音

2025-06-28 18:46:35 Saturday ｜ 音画同步，AI视频也能有完美「原声音」，可灵AI刚上线的！https://mp.weixin.qq.com/s/8h5sZUIxsmce9GRRMDkEBg

他们提出的多模态视频生音效模型名叫 Kling-Foley，能够通过大模型自动生成与视频内容同步的高质量立体声音频。

Kling-Foley 支持基于视频内容与可选文本提示自动生成与视频画面语义相关、时间同步的高质量立体声音频，涵盖音效、背景音乐等多种类型声音内容。它支持生成任意时长的音频内容，还具备立体声渲染的能力，支持空间定向的声源建模和渲染。

* 论文：https://www.arxiv.org/pdf/2506.19774
* 项目主页：https://klingfoley.github.io/Kling-Foley/
* GitHub 链接：https://github.com/klingfoley/Kling-Foley
* Benchmark：https://huggingface.co/datasets/klingfoley/Kling-Audio-Eval

## 3D

1. [拿下3D生成行业新标杆！昆仑万维Matrix-3D新模型鲨疯了，一张图建模游戏场景](https://mp.weixin.qq.com/s/h8WVaV_3CXKmHc0UdrLGTA)

## 长度扩展

2025-06-30 16:18:28 Monday ｜ 打破长视频理解瓶颈：HoPE混合位置编码提升VLM长度泛化能力 https://mp.weixin.qq.com/s/KQHGw8_v0rEY8pS7jufRbQ

现有多模态 RoPE 泛化能力不足的原因之一是保留 RoPE 中所有频率对长上下文语义建模有负面影响。基于此分析，他们提出的混合位置编码（HoPE, Hybrid of Position Embedding）大幅提升了 VLM 的长度泛化能力，在长视频理解和检索等任务中达到最优表现。

* 论文标题：HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models
* arXiv 链接：https://arxiv.org/pdf/2505.20444
* 代码链接：https://github.com/hrlics/HoPE

## 语音

13. 2025-06-18 08:20:08 Wednesday ｜

首个全面梳理语音大模型发展脉络的权威综述，入选ACL 2025主会

https://mp.weixin.qq.com/s/sIa9qIzPuykCysAVgeGxew

2. 2025-07-01 10:56:55 Tuesday ｜ ACL 2025 | AI字幕慢半拍，不知道大家在笑什么？新方法让同传性能直逼离线翻译 https://mp.weixin.qq.com/s/laRnBuuTfF-olWKv42TAgw

该方法**将同传任务巧妙地建模为序贯决策过程，通过优化完整的决策序列，显著提升了翻译质量，同时有效控制了延迟，其性能直逼、甚至在某些方面超越了同等大小的离线翻译模型。**

* 论文标题: SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation
* 论文链接：https://arxiv.org/pdf/2505.20622
