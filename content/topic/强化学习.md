+++
title = '强化学习'
date = 2025-08-08T00:00:00+08:00
draft = false
toc = true
+++

# 强化学习

1. 2025-07-18 20:24:54 Friday ｜ 真相大白！RL不是魔法，随机奖励有效只是由于数据泄露！ https://mp.weixin.qq.com/s/J7dzjfHMHyvQNpKQFpoq7w
2. 🌈 2025-07-18 14:07:23 Friday｜强化学习的两个「大坑」，终于被两篇ICLR论文给解决了 https://mp.weixin.qq.com/s/myqak4eAAQ7ONKJKOFxk8g
3. 2025-07-17 10:59:26 Thursday ｜ 字节跳动Seed最新强化学习配方POLARIS开源 4B 模型数学推理接近 235B 表现
4. 2025-06-30 19:00:34 Monday ｜ 强化学习也能预训练？效果可提升20倍，华人新作引爆RL新范式! https://mp.weixin.qq.com/s/WyJuhjkmreZ2clSw1XvHiw

研究人员构建的模型将「意图」编码为潜在变量，并通过「流匹配」（flow matching）来预测未来状态的访问概率。

论文地址：https://arxiv.org/abs/2506.08902

博客地址：https://chongyi-zheng.github.io/infom/

由于普通流匹配方法无法拼接多个状态转换，研究者引入基于SARSA的时序差分流匹配损失进行改进。

5. 2025-06-24 11:28:01 Tuesday ｜ CPGD：只训练数学，却在物理化学生物战胜o1！新强化学习算法带来显著性能提升，还缓解训练崩溃问题 https://mp.weixin.qq.com/s/RDUPagBn8l00P7dNPHjBcA

相比于传统GRPO、RLOO等算法显著缓解了训练不稳定（甚至崩溃）的问题，并带来显著性能提升。

基于OpenRLHF，团队构建了一个高效、可扩展的多模态强化学习框架，支持Qwen-VL、InternVL等多种模型与RL算法，包括GRPO、REINFORCE++、RLOO，以及提出的新型RL算法CPGD，并已成功训练出Qwen2.5VL-32B、InternVL2.5-38B等大型模型。

该框架相较于已有方案（如R1-V），具备更强的可扩展性与稳定性，为大规模多模态强化学习提供了基础设施支撑。

*开源代码：*

*https://github.com/ModalMinds/MM-EUREKA*

*https://github.com/ModalMinds/MM-EUREKA/tree/mm-prm*

*技术报告：*

*https://arxiv.org/abs/2503.07365*

*https://arxiv.org/abs/2505.12504*

*https://arxiv.org/abs/2505.13427*

*MMK12数据集：*

*https://huggingface.co/datasets/FanqingM/MMK12*

*模型权重：*

*https://huggingface.co/FanqingM/MM-Eureka-Qwen-7B*

*https://huggingface.co/FanqingM/MM-Eureka-Qwen-32B*

*https://huggingface.co/Zkkkai/CPGD-7B*

6. Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions

标题： 使用强化学习训练大型语言模型以解释人类决策

链接：https://arxiv.org/abs/2505.11614

7. 2025-06-12 10:54:10 Thursday | 「Next-Token」范式改变！刚刚，强化学习预训练来了https://mp.weixin.qq.com/s/UABVUoHYTDlFWWNvD5R9Og
8. 2025-06-12 12:01:12 Thursday ｜

 Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning  for LLM Reasoning
 **标题** ： 一致的路径通向真理：LLM推理的自我奖励强化学习
 **链接** ：https://arxiv.org/abs/2506.08745

 **作者** ： Kongcheng Zhang,  Qi Yao,  Shunyu Liu,  Yingjie Wang,  Baisheng Lai,  Jieping Ye,  Mingli Song,  Dacheng Tao
 **摘要** ：强化学习（RL）的最新进展突出了其在复杂推理任务中的潜力，但有效的训练往往依赖于外部监督，这限制了其更广泛的适用性。在这项工作中，我们提出了一种新的自我奖励强化学习框架，通过利用不同推理轨迹之间的中间推理状态的一致性来增强大型语言模型（LLM）推理。我们的关键见解是， **正确的反应往往表现出一致的轨迹模式，在模型的可能性：他们的中间推理状态往往会收敛到自己的最终答案（高一致性）** ，与其他候选人的偏差最小（低波动性）。受这一观察的启发，我们引入了CoVo，这是一种内在的奖励机制，它通过强大的向量空间聚合策略整合了一致性和波动性，并辅以好奇心奖金来促进多样化的探索。CoVo使LLM能够以自我奖励的方式执行RL，为在没有外部监督的情况下学习推理提供了一条可扩展的途径。在不同的推理基准上进行的大量实验表明，CoVo实现了与监督RL相当甚至超过监督RL的性能。我们的代码可在https://github.com/sastpg/CoVo上获得。

9. 2025-06-12 12:10:11 Thursday |

 Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and  Significance-based Reinforcement Learning
 **标题** ： 宾果：通过动态和基于重要性的强化学习提高LLM的高效推理
 **链接** ：https://arxiv.org/abs/2506.08125

 **作者** ： Hanbing Liu,  Lang Cao,  Yuanyi Ren,  Mengyu Zhou,  Haoyu Dong,  Xiaojun Ma,  Shi Han,  Dongmei Zhang
 **摘要** ：大型语言模型已经展示了令人印象深刻的推理能力，但由于不必要的冗长或冗余输出，它们通常效率低下。虽然许多研究已经探索了强化学习（RL）来增强推理能力，但大多数主要集中在提高准确性上，对**推理效率**的关注有限。一些现有的方法引入了直接基于长度的奖励来鼓励简洁，但这通常会导致准确性的明显下降。在本文中，我们提出了Bingo，这是一个RL框架，它推进了基于长度的奖励设计，以提高推理效率。Bingo包含两个关键机制：一个是显著性长度奖励，它逐渐引导模型只减少不显著的标记;另一个是动态长度奖励，它最初鼓励对困难问题进行详细推理，但随着时间的推移而衰减，以提高整体效率。在多个推理基准上的实验表明，Bingo提高了准确性和效率。它在RL中优于vanilla reward和其他几个基于长度的奖励基线，在准确性和效率之间实现了有利的权衡。这些结果强调了明确训练LLM进行有效推理的潜力。

10. 2025-06-13 18:30:02 Friday ｜

PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative  Verifier
 **标题** ： PAC：以政策作为生成验证者的多轮强化LLM自我纠正
 **链接** ：https://arxiv.org/abs/2506.10406

 **作者** ： Yuhua Jiang,  Yuwen Xiong,  Yufeng Yuan,  Chao Xin,  Wenyuan Xu,  Yu Yue,  Qianchuan Zhao,  Lin Yan
 **摘要** ：大型语言模型（LLM）在复杂的推理任务中表现出了令人印象深刻的能力，但它们仍然难以可靠地验证自己输出的正确性。针对这一验证挑战的现有解决方案通常依赖于单独的验证器模型或需要多级自校正训练管道，这限制了可扩展性。在本文中，我们提出了作为生成验证器（PAG）的策略，这是一个简单而有效的框架，可以通过在统一的多轮强化学习（RL）范式中交替策略和验证器角色，使LLM能够自我纠正。不同于以往的方法，总是产生第二次尝试，无论模型的信心，PAG引入了一个选择性的修订机制：模型修改其答案，只有当自己的生成验证步骤检测到错误。这种先验证再修改的工作流程不仅可以缓解模型崩溃，还可以共同增强推理和验证能力。在不同的推理基准上进行的大量实验突出了PAG的双重进步：作为一种策略，它提高了直接生成和自校正的准确性;作为一种验证器，它的自验证优于自一致性。
6. 2025-06-18 10:43:39 Wednesday ｜

Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own  Reasoning for Open-Ended Tasks
 **标题** ： 直接推理优化：LLM可以奖励和完善他们自己的推理，用于开放式任务
 **链接** ：https://arxiv.org/abs/2506.13351

 **作者** ： Yifei Xu,  Tusher Chakraborty,  Srinagesh Sharma,  Leonardo Nunes,  Emre Kıcıman,  Songwu Lu,  Ranveer Chandra
 **摘要** ：大型语言模型（LLM）的最新进展在数学和编程等结构化任务中展示了令人印象深刻的推理能力，这主要是由具有可验证奖励的强化学习（RLVR）驱动的，它使用基于结果的信号，这些信号是可扩展的，有效的，并且对奖励黑客具有鲁棒性。然而，由于缺乏通用的、可验证的奖励信号，将类似的技术应用于开放式长形式推理任务仍然具有挑战性。为了解决这个问题，我们提出了 **直接推理优化（DRO），这是一种强化学习框架，用于在开放式，特别是长形式的推理任务上微调LLM，由一个新的奖励信号指导：推理反射奖励（R3）** 。在其核心，R3选择性地识别和强调参考结果中的关键标记，这些标记反映了模型先前的思想链推理的影响，从而在细粒度级别上捕获推理和参考结果之间的一致性。至关重要的是，R3是使用正在优化的相同模型在内部计算的，从而实现完全独立的训练设置。此外，我们引入了一个动态的数据过滤策略，基于R3的开放式推理任务，降低成本，同时提高下游性能。我们在两个不同的数据集上评估了DRO- ParaRev，一个长形式的段落修订任务，和FinQA，一个面向数学的QA基准-并表明它始终优于强大的基线，同时在开放式和结构化领域中保持广泛适用。

7. 2025-06-19 20:43:53 Thursday ｜

Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models
 **标题** ： 自适应引导加速推理模型的强化学习
 **链接** ：https://arxiv.org/abs/2506.13923

 **作者** ： Vaskar Nath,  Elaine Lau,  Anisha Gunjal,  Manasi Sharma,  Nikhil Baharte,  Sean Hendryx
 **摘要** ：我们研究了通过可验证奖励强化学习（RLVR）训练的推理模型可以学习解决新问题的过程。我们发现RLVR通过两种主要方式驱动性能：（1）通过将pass@$k$压缩为pass@1;（2）通过“能力增益”，模型学习解决以前即使在高k$下也无法解决的新问题。我们发现，虽然能力增益存在于不同的模型规模，学习解决新问题主要是通过自我升华。我们在0.5B到72 B的模型规模上展示了这些发现，涉及超过500，000个推理问题，其中包括数学，科学和代码领域的提示和可验证的最终答案。我们进一步表明，我们可以通过利用自然语言指导模型在上下文中考虑，同时仍然需要模型从头开始推导解决方案链，来显着提高通过@$k$率。基于这些见解，我们得出$\text{Guide}$ -一类新的在线训练算法。$\text{Guide}$自适应地将提示合并到模型的上下文中，以解决所有展开最初都不正确的问题，并调整“偏离策略”轨迹的重要性采样率，以便优化提示不再存在的上下文中的策略。我们描述了GRPO和PPO的$\text{Guide}$的变体，并根据经验表明，7 B和32 B参数模型上的Guide-GRPO比普通模型提高了泛化能力，在数学基准测试中的宏观平均改善高达4$\%$。我们包括仔细的消融分析$\text{Guide}$的组件和理论分析指南的学习效率。
8. 2025-06-26 14:35:16 Thursday ｜

**#6 OctoThinker：训练中期激励强化学习规模化 [PDF** **(14)** **] [复制] [Kimi** **(10)**  **] [相关] ** **[#6](https://arxiv.org/abs/2506.20512)****[OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://papers.cool/arxiv/2506.20512)**** [PDF(14)] [Copy] [Kimi(10)] [REL]**

作者：王增志，周凡，李雪峰，刘鹏飞

不同基础语言模型家族（如 Llama 和 Qwen）在强化学习（RL）后训练阶段会表现出差异化行为，尤其在推理密集型任务上。什么样的基础语言模型适合强化学习？深入理解这一问题对开发下一代可扩展 RL 的基础模型至关重要。本研究探讨了中期训练策略如何塑造 RL 动态特性，重点关注 Qwen 和 Llama 两个代表性模型家族。我们的研究发现：（1）高质量数学语料（如 MegaMath-Web-Pro）能显著提升基础模型和 RL 性能，而现有替代方案（如 FineMath-4plus）则效果不佳；（2）进一步添加问答式数据（特别是长思维链推理示例）可增强 RL 效果，且指令数据能进一步释放这种增益；（3）长思维链虽能提升推理深度，但也会导致模型响应冗长和 RL 训练不稳定，凸显数据格式化的重要性；（4）扩大中期训练规模始终能带来更强的下游 RL 性能。 基于这些洞见，我们提出了一种两阶段中期训练策略"稳定后衰减"：基础模型首先以恒定学习率在 2000 亿 token 上进行训练，随后在三个专注于思维链的分支上以衰减学习率完成 200 亿 token 训练。由此诞生的 OctoThinker 模型家族展现出强大的强化学习兼容性，缩小了与更适配强化学习的模型家族（如 Qwen）之间的性能差距。我们希望这项工作能为强化学习时代的基础模型预训练策略提供参考。为支持后续研究，我们开源了模型及精选的超过 700 亿 token 数学推理密集型语料库（即 MegaMath-Web-Pro-Max）。

学科分类：计算与语言 , 人工智能 , 机器学习

发布时间：2025 年 6 月 25 日 14:58:13（UTC）

## 大模型训练

香港大学NLP团队联合字节跳动Seed、复旦大学发布名为Polaris的强化学习训练配方：通过Scaling RL，Polaris让4B模型的数学推理能力（AIME25上取得79.4，AIME24上取得81.2）超越了一众商业大模型[https://mp.weixin.qq.com/s/RvIy5Ssfakl92aVc9fczJw](https://mp.weixin.qq.com/s/RvIy5Ssfakl92aVc9fczJw)

🌈🌈 2025-06-23 13:00:43 Monday ｜ 英伟达团队提出全新训练方法——ProRL，成功将RL扩展到2000步

https://mp.weixin.qq.com/s/WwLzBO-EZLwsZM9weaGufQ

论文链接：https://arxiv.org/abs/2505.24864

并且，它通过跨领域训练数据，包括数学、代码、STEM、谜题、指令遵循，实现了泛化能力。

这就是强化学习的Scaling Law：强化学习训练越长，LLM推理能力越强。

> **（1）强化学习在扩展模型推理边界（以pass@128衡量）方面的效果，与  基础模型的初始能力  密切相关。**
>
> **（2）强化学习确实能够显著扩展模型的推理能力，尤其是在那些超出基础模型原有能力范围的高难度任务上。 **
>
> **（3）强化学确实可以扩展LLM推理边界，能够推广到训练中未见的分布外任务。 **
>
> **（4）新方法ProRL不仅提高了平均pass@1，还足以弥补训练中可能带来的输出方差增加，从而整体提升pass@k上限，推动推理能力的实质跃升。**

### 长序列训练

2025-06-24 11:20:29 Tuesday ｜ 无损减少80%激活值内存，提升5倍训练序列长度，仅需两行代码 https://mp.weixin.qq.com/s/YqQYtKqzCVFKzr3Z7bYHsg

来自港中文（深圳）和上海交通大学的团队提出 **StreamBP** 算法。通过对链式法则进行线性分解和分步计算，StreamBP 将大语言模型训练所需的激活值内存（logits 和 layer activation）降低至梯度检查点（gradient checkpointing）的 20% 左右。

## 奖励模型

研究“奖励模型欺骗”现象是否存在普遍性

**奖励模型也能Scaling！上海AI Lab突破强化学习短板，提出策略判别学习新范式**[https://mp.weixin.qq.com/s/hU3MKn82o1sMy4CK-CUUug](https://mp.weixin.qq.com/s/hU3MKn82o1sMy4CK-CUUug)

2025-07-03 11:43:11 Thursday ｜

**[Skywork-Reward-V2：通过 Human-AI Synergy 扩展首选项数据管理](https://papers.cool/arxiv/2507.01352)** **[PDF(4)]** **[Copy]** **[Kimi(2)]** **[REL]**

 **Authors** : [Chris Yuhao Liu](https://arxiv.org/search/?searchtype=author&query=Chris%20Yuhao%20Liu), [Liang Zeng](https://arxiv.org/search/?searchtype=author&query=Liang%20Zeng), [Yuzhen Xiao](https://arxiv.org/search/?searchtype=author&query=Yuzhen%20Xiao), [Jujie He](https://arxiv.org/search/?searchtype=author&query=Jujie%20He), [Jiacai Liu](https://arxiv.org/search/?searchtype=author&query=Jiacai%20Liu), [Chaojie Wang](https://arxiv.org/search/?searchtype=author&query=Chaojie%20Wang), [Rui Yan](https://arxiv.org/search/?searchtype=author&query=Rui%20Yan), [Wei Shen](https://arxiv.org/search/?searchtype=author&query=Wei%20Shen), [Fuxiang Zhang](https://arxiv.org/search/?searchtype=author&query=Fuxiang%20Zhang), [Jiacheng Xu](https://arxiv.org/search/?searchtype=author&query=Jiacheng%20Xu), [Yang Liu](https://arxiv.org/search/?searchtype=author&query=Yang%20Liu), [Yahui Zhou](https://arxiv.org/search/?searchtype=author&query=Yahui%20Zhou)

尽管奖励模型 （RM） 在人类反馈强化学习 （RLHF） 中发挥着关键作用，但当前最先进的开放式 RM 在大多数现有评估基准上表现不佳，未能捕捉到细微而复杂的人类偏好。即使是采用高级训练技术的方法也没有产生有意义的性能改进。我们假设这种脆弱性主要源于偏好数据集的局限性，这些数据集通常范围狭窄、综合标记或缺乏严格的质量控制。为了应对这些挑战，我们提出了一个包含 4000 万个偏好对的大规模偏好数据集，名为 SynPref-40M。为了实现大规模的数据管理，我们设计了一个人类-AI 协同的两阶段管道，该管道利用了人工注释质量和 AI 可扩展性的互补优势。在此管道中，人工提供经过验证的注释，而大型语言模型根据人工指导执行自动管理。在这种偏好混合物上进行训练，我们引入了 Skywork-Reward-V2，这是一套由 8 个奖励模型组成的套件，范围从 0.6B 到 8B 参数，在 SynPref-40M 中精心策划的 2600 万个偏好对子集上进行训练。我们证明 Skywork-Reward-V2 在广泛的功能中具有多功能性，包括与人类偏好保持一致、客观正确性、安全性、抗风格偏见和 N 最佳扩展，在七个主要奖励模型基准中实现最先进的性能。消融研究证实，我们方法的有效性不仅源于数据规模，还源于高质量的策展。Skywork-Reward-V2 系列代表了开放奖励模型的重大进展，突出了现有偏好数据集尚未开发的潜力，并展示了人类与 AI 的管理协同作用如何显著提高数据质量。

 **科目** :  **[计算和语言](https://papers.cool/arxiv/cs.CL)** , [人工智能](https://papers.cool/arxiv/cs.AI), [机器学习](https://papers.cool/arxiv/cs.LG)

 **发布** ： 2025-07-02 04：40：29 UTC

4. 2025-07-02 17:21:08 Wednesday｜

**[隐式奖励作为桥梁：SFT 和 DPO 连接的统一视图](https://papers.cool/arxiv/2507.00018)** **[PDF(16)]** **[Copy]** **[Kimi(11)]** **[REL]**

 **Authors** : [Bo Wang](https://arxiv.org/search/?searchtype=author&query=Bo%20Wang), [Qinyuan Cheng](https://arxiv.org/search/?searchtype=author&query=Qinyuan%20Cheng), [Runyu Peng](https://arxiv.org/search/?searchtype=author&query=Runyu%20Peng), [Rong Bao](https://arxiv.org/search/?searchtype=author&query=Rong%20Bao), [Peiji Li](https://arxiv.org/search/?searchtype=author&query=Peiji%20Li), [Qipeng Guo](https://arxiv.org/search/?searchtype=author&query=Qipeng%20Guo), [Linyang Li](https://arxiv.org/search/?searchtype=author&query=Linyang%20Li), [Zhiyuan Zeng](https://arxiv.org/search/?searchtype=author&query=Zhiyuan%20Zeng), [Yunhua Zhou](https://arxiv.org/search/?searchtype=author&query=Yunhua%20Zhou), [Xipeng Qiu](https://arxiv.org/search/?searchtype=author&query=Xipeng%20Qiu)

训练后过程是将预先训练的语言模型与实际任务相结合的重要阶段，从演示或偏好信号中学习在这种适应中起着至关重要的作用。我们提出了一个统一的理论框架，在大型语言模型 （LLM） 训练后桥接监督微调 （SFT） 和偏好学习。通过严格的数学推导，我们证明了 SFT 和偏好学习方法（如直接偏好优化 （DPO））都在相同的最优策略奖励子空间内运行，其中 SFT 代表了隐式奖励学习的一种特殊情况。我们的分析揭示了传统 SFT 的一个关键限制：在优化期间，分布匹配中的 KL 散度项相对于策略变得恒定，无法限制模型更新。为了解决这个问题，我们提出了一种简单而有效的学习率降低方法，该方法可以显著提高性能（在教学后续任务中提高 \textbf{25\%} 相对增益和 \textbf{6\%} 绝对胜率增加）。此外，我们 **从各种 f 发散函数中推导出替代 SFT 目标** ，这些函数在优化过程中保留 KL 项，进一步提高了 DPO 后模型的性能。最后，我们将 LLM logits 和 Q-functions 之间的理论关系从偏好学习扩展到 SFT 上下文，提供数学推导和实验验证。

 **科目** :  **[机器学习](https://papers.cool/arxiv/cs.LG)** , [人工智能](https://papers.cool/arxiv/cs.AI), [计算和语言](https://papers.cool/arxiv/cs.CL)

 **发布** ： 2025-06-15 05：42：29 UTC

5. 2025-07-02 17:20:00 Wednesday ｜

**[通过奖励引导数据集蒸馏增强 SLM 中的推理能力](https://papers.cool/arxiv/2507.00054)** **[PDF(2)]** **[Copy]** **[Kimi(3)]** **[REL]**

 **Author** : [Shreyansh Padarha](https://arxiv.org/search/?searchtype=author&query=Shreyansh%20Padarha)

将大型语言模型 （LLM） 的熟练程度压缩并传授为更易于部署和更高效的小语言模型 （SLM） 的推动得益于知识蒸馏 （KD） 技术的改进。这些技术允许较小的学生模型从更有能力和更大的教师模型的回答中学习。然而，提炼通常围绕学生模型展开，仅复制教师的分布内反应，限制了其普遍性。这种限制在推理任务中被放大，并且计算成本可能很高。在这项研究中，我们提出了 AdvDistill，一个奖励导向的数据集蒸馏框架。我们为每个提示使用老师的多代（回复），并根据基于规则的验证者分配奖励。这些变化且正态分布的奖励在训练学生模型时用作权重。我们的方法及其随后的行为分析表明，学生模型在数学和复杂推理任务中的表现有了显着提高，展示了在数据集蒸馏过程中纳入奖励机制的有效性和好处。

 **科目** :  **[人工智能](https://papers.cool/arxiv/cs.AI)** , [计算和语言](https://papers.cool/arxiv/cs.CL), [机器学习](https://papers.cool/arxiv/cs.LG)

 **发布** ： 2025-06-25 20：07：47 UTC

6. 2025-07-01 12:05:40 Tuesday ｜

 **[通才奖励模型：在大型语言模型中发现](https://papers.cool/arxiv/2506.23235)** **[PDF(7)]** **[Copy]** **[Kimi(7)]** **[REL]**

 **Authors** : [Yi-Chen Li](https://arxiv.org/search/?searchtype=author&query=Yi-Chen%20Li), [Tian Xu](https://arxiv.org/search/?searchtype=author&query=Tian%20Xu), [Yang Yu](https://arxiv.org/search/?searchtype=author&query=Yang%20Yu), [Xuqin Zhang](https://arxiv.org/search/?searchtype=author&query=Xuqin%20Zhang), [Xiong-Hui Chen](https://arxiv.org/search/?searchtype=author&query=Xiong-Hui%20Chen), [Zhongxiang Ling](https://arxiv.org/search/?searchtype=author&query=Zhongxiang%20Ling), [Ningjing Chao](https://arxiv.org/search/?searchtype=author&query=Ningjing%20Chao), [Lei Yuan](https://arxiv.org/search/?searchtype=author&query=Lei%20Yuan), [Zhi-Hua Zhou](https://arxiv.org/search/?searchtype=author&query=Zhi-Hua%20Zhou)

大型语言模型 （LLM） 的对齐在很大程度上取决于在昂贵的人类偏好数据上训练的奖励模型。虽然最近的工作探索了通过 AI 反馈绕过这一成本，但这些方法通常缺乏严格的理论基础。在本文中，我们发现一个强大的通才奖励模型已经潜伏在任何通过标准下一个代币预测训练的 LLM 中。我们证明，这种内生奖励不是启发式的，而是理论上等同于通过离线逆强化学习学习的奖励函数。这种连接使我们能够直接从基础（预先训练或监督微调）模型中获得高质量的奖励信号，而无需任何进一步的训练。至关重要的是，我们还证明，与基本模型相比，使用这种内生奖励的后续强化学习会导致具有可证明的更优越误差边界的策略。据我们所知，这是强化学习对 LLM 有效性的第一个理论证明。我们的实验验证了这一理论，表明我们的方法不仅优于现有的 LLM as-a-judge 方法，而且还可以超越明确训练的奖励模型。这些发现表明，奖励建模阶段可以被一种有原则的方法所取代，该方法可以获取在预训练期间已经捕获的知识，这预示着 LLM 对齐和多模态模型的更高效、更强大和可扩展的范式。

 **主题** : **[计算和语言](https://papers.cool/arxiv/cs.CL)**

 **发布** ： 2025-06-29 13：45：54 UTC

7. 🌈 🌈 2025-06-27 12:57:34 Friday ｜ 北京大学知识计算实验室联合腾讯微信模式识别中心、William&Mary、西湖大学等机构提出的**RewardAnything**突破了这一瓶颈——通过让奖励模型直接理解自然语言描述的评判原则，实现了从”死记硬背”到”融会贯通”的范式跃迁。https://mp.weixin.qq.com/s/sJI0TpYnuLQKtwaJdo2t6w
8. 20250604｜From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling
   1. 标题： 从数学推理到代码：测试时间缩放中过程奖励模型的推广
   2. 链接：[https://arxiv.org/abs/2506.00027](https://arxiv.org/abs/2506.00027)
9. 2025-06-09 10:45:51 Monday ｜ R-Search: Empowering LLM Reasoning with Search via Multi-Reward  Reinforcement Learning
   **标题** ： R-Search：通过多奖励强化学习通过搜索增强LLM推理
   **链接** ：https://arxiv.org/abs/2506.04185
   **摘要** ：大型语言模型（LLM）在多步和长链推理方面取得了显着进展。然而，扩展它们的推理能力以包含与搜索的深度交互仍然是一个不小的挑战，因为模型通常无法识别最佳的推理-搜索交互轨迹，导致次优响应。我们提出了R-Search，这是一种 **用于推理搜索集成的新型强化学习框架** ，旨在使LLM能够自主执行具有深度搜索交互的多步推理，并通过多奖励信号学习最佳推理搜索交互轨迹，从而提高复杂逻辑和知识密集型任务的响应质量。R-Search引导LLM动态决定何时检索或推理，同时全局集成关键证据以增强推理和搜索之间的深层知识交互。在RL训练过程中，R-Search提供多阶段、多类型的奖励，共同优化推理搜索轨迹。在七个数据集上的实验表明，R-Search比高级RAG基线的性能高出32.2%（域内）和25.1%（域外）。代码和数据可在https://github.com/QingFei1/R-Search上获得。
10. 2025-06-13 18:49:19 Friday ｜

 Provably Learning from Language Feedback
 **标题** ： 可证明从语言反馈中学习
 **链接** ：https://arxiv.org/abs/2506.10341

 **作者** ： Wanqiao Xu,  Allen Nie,  Ruijie Zheng,  Aditya Modi,  Adith Swaminathan,  Ching-An Cheng
 **摘要** ：通过观察和语言反馈进行交互式学习是一个越来越多的研究领域，这是由大型语言模型（LLM）代理的出现所驱动的。虽然令人印象深刻的经验证明已经显示，到目前为止，这些决策问题的原则框架仍然缺乏。在本文中，我们形式化的学习语言反馈（LLF）问题，断言足够的假设，使学习尽管潜在的奖励，并引入$\textit{transfer eluder dimension}$作为一个复杂的措施来表征LLF问题的难度。我们发现，转移逃避维度捕捉的直觉，在反馈中的信息改变学习的LLF问题的复杂性。我们展示了从丰富的语言反馈中学习比从奖励中学习快得多的案例。我们开发了一个无遗憾的算法，称为$\texttt{HELiX}$，可证明解决LLF问题，通过顺序的相互作用，与性能保证，规模与转移逃避维度的问题。在几个经验领域，我们表明，$\texttt{HELiX}$表现良好，即使反复提示LLM不可靠的工作。我们的贡献标志着根据通用语言反馈设计有原则的交互式学习算法的第一步。

11. 2025-06-23 11:32:17 Monday |

  **[arXiv:2506.16712 ](https://arxiv.org/abs/2506.16712)** [ **[pdf](https://arxiv.org/pdf/2506.16712)** ,  **[html](https://arxiv.org/html/2506.16712v1)** ,  **[other](https://arxiv.org/format/2506.16712)** ]

**ReasonGRM：通过大型推理模型增强生成式奖励模型**

陈斌，高新泽，胡传瑞，于鹏航，张华，鲍冰昆

学科分类：计算与语言（cs.CL）；人工智能（cs.AI）

12. 2025-06-23 11:33:01 Monday |

arXiv:2506.16024 [pdf 版，html 版，其他格式]

**从通用奖励到定向奖励：在开放式长文本生成任务中超越 GPT-4**

郭志涵、吴杰乐、崔文倩、张一飞、胡敏达、王雨菲、金国庆

学科分类：计算与语言（cs.CL）；人工智能（cs.AI）

### 理论研究

2025-07-03 10:18:33 Thursday ｜ 周志华团队新作：LLM中存在奖励模型，首次理论证明RL对LLM有效性 https://mp.weixin.qq.com/s/OOoAqaT_hnfh2rcKkxrZWw

来自南京大学的研究者发现， **一个强大的通用奖励模型并非需要构建，而是可以挖掘出来的** ， 因为它已经潜在地存在于通过标准的下一个 Token 预测训练的任何语言模型中，称之为「内源性奖励（endogenous reward）」。

本文的核心贡献是为这一观点提供严格的理论基础。本文证明了可以从标准的下一个 Token 预测目标中恢复出一种特定形式的离线逆强化学习（IRL）奖励函数，该目标用于预训练和监督微调（SFT）。这一见解能够超越启发式方法，并建立一种原则性的方法，来引出语言模型在训练过程中隐式学习到的奖励函数。

据了解，这是首次理论证明强化学习在 LLM 中的有效性。广泛实验验证了这一理论，表明 **这种内源性奖励不仅优于现有的 LLM-as-a-judge 方法** ，**而且可以超越那些通过昂贵的人类标注数据显式训练的奖励模型的表现。**

论文标题： GENERALIST REWARD MODELS: FOUND INSIDE LARGE LANGUAGE MODELS

论文链接：https://arxiv.org/pdf/2506.23235

由于本评估的方法无需训练，因此本评估将其与其他无需训练的方法进行对比：生成式验证器（Generative Verifier）、GenRM-Pairwise 和 GenRM-Pointwise 。

### 过程奖励模型PRM

https://arxiv.org/abs/2504.16828 具有思考能力的流程奖励模型

[穆罕默德·哈利法 ](https://arxiv.org/search/cs?searchtype=author&query=Khalifa,+M)、[ 里沙布·阿加瓦尔 ](https://arxiv.org/search/cs?searchtype=author&query=Agarwal,+R)、[ 拉贾努根·洛格斯瓦兰 ](https://arxiv.org/search/cs?searchtype=author&query=Logeswaran,+L)、[ 金在谦 ](https://arxiv.org/search/cs?searchtype=author&query=Kim,+J)、[ 彭浩 ](https://arxiv.org/search/cs?searchtype=author&query=Peng,+H)、[ 李文泰 ](https://arxiv.org/search/cs?searchtype=author&query=Lee,+M)、[ 李洪乐 ](https://arxiv.org/search/cs?searchtype=author&query=Lee,+H)、[ 王璐](https://arxiv.org/search/cs?searchtype=author&query=Wang,+L)

> 逐步验证器——也称为过程奖励模型（PRMs）——是测试时扩展的关键要素。PRMs 需要步骤级监督，导致其训练成本高昂。本研究旨在构建数据高效的 PRMs，将其表述为可生成验证思维链（CoT）的逐步骤奖励模型，以验证解决方案中的每个步骤。我们提出 ThinkPRM，这是一种长思维链验证器，其微调所需的过程标签数量比判别式 PRMs 少几个数量级。该方法充分发挥了长 CoT 模型固有的推理能力，在仅使用 PRM800K 中 1%过程标签的情况下，于多个具有挑战性的基准测试中超越了 LLM-as-a-Judge 和判别式验证器。具体而言，ThinkPRM 在 ProcessBench、MATH-500 和 AIME '24 的最佳 N 选择和奖励引导搜索中均优于基线模型。在 GPQA-Diamond 子集和 LiveCodeBench 的跨领域评估中，我们的 PRM 分别以 8%和 4.5%的优势超越了使用完整 PRM800K 训练的判别式验证器。 最后，在相同 token 预算下，ThinkPRM 相比 LLM-as-a-Judge 能更高效地扩展验证计算能力，在 ProcessBench 子集上以 7.2%的优势超越后者。我们的工作凸显了生成式长链思维过程奖励模型的价值——这类模型既能扩展测试时的验证计算，又只需极少的训练监督。代码、数据和模型将在[此 https 网址](https://github.com/mukhal/thinkprm)发布。

> **会议主题** ：奖励模型验证器研讨
>
> #### 一、验证器类型
>
> 1. **结果验证器（Outcome Verifiers）**
>    1. 验证完整解决方案的最终答案，通过花费更多计算资源采样并验证解决方案，构建可扩展的计算系统。
>    2. 例如：验证叠加态结果或最终答案的正确性。
> 2. **过程验证器（Process Verifiers）**
>    1. 核心为“过程奖励模型（Process Reward Model）”，在推理步骤级别区分正确与错误，而非完整解决方案级别。
>    2. 可通过回溯或树搜索（如MCTS、波束搜索）实现计算扩展，OpenAI的相关研究表明，细粒度训练的过程验证器性能显著优于仅基于结果训练的模型。
>
> #### 二、过程验证器的训练挑战
>
> 3. **标注成本高昂**
>    1. 需步骤级二元标签（正确/错误），例如OpenAI曾邀请数学专家对解决方案逐步骤标注至首次错误，过程繁琐且成本极高，严重限制了PRM的可扩展性。
> 4. **数据依赖问题**
>    1. 传统判别式PRM需大量标注数据，而人工标注难以满足大规模需求。
>
> #### 三、步骤标签获取方案
>
> 5. **对齐法（Alignment Approach）**
>    1. 通过对齐正确与错误解决方案，定位不匹配的步骤并标注为“错误”。
>    2. 例如：对比正确解与错误解的步骤，不匹配处即为错误步骤，可用于训练验证器。
> 6. **概率法（Probabilistic Approach）**
>    1. 基于步骤生成完整推导路径，计算到达正确答案的概率，以此评估步骤正确性。
>    2. 问题：依赖模型推导能力，若模型本身性能不足，可能误判正确步骤为错误，引入标签噪声。
>
> #### 四、生成式验证器（GenRM）的进展与局限
>
> 7. **核心机制**
>    1. 通过训练生成模型（如微调的聊天机器人）生成验证思维链，提取“正确/错误”概率作为评分。
>    2. 优势：相比判别式模型，推理计算扩展性更强（如相同采样预算下性能更优）。
> 8. **局限性**
>    1. 仅支持结果验证，无法用于树搜索等需要部分解的场景；
>    2. 需数千个黄金解决方案用于训练，且缺乏跨领域泛化能力（仅在基础数学等简单任务上验证）。
>
> #### 五、Think PRM：生成式过程验证器的改进
>
> 9. **模型设计**
>    1. 输入问题与候选解，生成细粒度的验证思维链，逐步骤判断正确性（区别于仅输出最终标签的结果验证器）。
> 10. **关键实验结论**
>     1. **数据效率** ：仅用8000个步骤标签训练的Think PRM（14B模型），性能优于使用70万标签训练的判别式PRM；
>     2. **计算扩展性** ：在“最佳N选”（Best of N）场景中，通过加权多数投票排序解决方案，准确率超越基线模型；
>     3. **跨领域泛化** ：在物理问答（GPQA）、编程评估（CodeBench）等领域表现优于判别式PRM。
> 11. **局限性**
>     1. 推理效率低于判别式模型（需生成较长思维链）；
>     2. 存在“过度自信”问题（评分集中于0或1，缺乏不确定性表征）；
>     3. “步骤标签干扰”：前一步骤的判断可能偏置后续标签（如首步错误易导致后续误判）。
>
> #### 六、未来方向
>
> 12. **强化学习（RL）优化**
>     1. 通过RL或偏好学习进一步提升验证器性能，但需设计可靠的奖励信号以避免“奖励博弈”。
> 13. **混合策略**
>     1. 根据问题难度动态切换判别式与生成式PRM，平衡效率与准确性。
>
> #### 七、问答环节
>
> * 提问：是否考虑用RL训练验证器？
> * 回答：RL是自然下一步，近期已有研究尝试，关键在于定义有效奖励信号，预计可优化搜索策略与验证思维。
>
> #### 八、结语
>
> * 说话人A邀请参会者通过邮件进一步讨论，或在10月加拿大语言建模会议（Callum）的工作坊面聊。
> * 说话人B邀请说话人A访问北京，期待线下交流。

https://arxiv.org/abs/2305.14934

GRACE：基于判别器引导的思维链推理

[穆罕默德·哈利法 ](https://arxiv.org/search/cs?searchtype=author&query=Khalifa,+M), [拉贾努根·洛格斯瓦兰 ](https://arxiv.org/search/cs?searchtype=author&query=Logeswaran,+L), [文泰·李 ](https://arxiv.org/search/cs?searchtype=author&query=Lee,+M), [洪乐·李 ](https://arxiv.org/search/cs?searchtype=author&query=Lee,+H), [陆望](https://arxiv.org/search/cs?searchtype=author&query=Wang,+L)

> 在多步推理任务中（例如思维链推理），语言模型（LM）很容易为错误的推理步骤分配高概率。因此，优化解可能性的解码策略往往会产生错误答案。为解决这一问题，我们提出了一种基于正确性判别器引导的思维链推理方法（GRACE），这种逐步解码方法能引导解码过程生成正确的推理步骤。GRACE 采用了一个通过对比正确与错误步骤训练的判别器，在解码过程中根据候选步骤的正确性进行评分。值得注意的是，GRACE 仅需对语言模型进行采样，无需训练或微调语言模型。基于 FLAN-T5 和 LLaMA 系列模型的实验表明，在四项数学推理和两项符号推理任务中，GRACE 在多数情况下相比贪婪解码、验证器和自一致性方法都取得了显著性能提升。当与自一致性方法结合使用时，GRACE 以较大优势超越了所有基线方法。 人类与 LLM 在 GSM8K 上的评估表明，GRACE 不仅提高了最终答案的准确率，还提升了中间推理的正确性。我们的实现代码可通过\url{[ 此链接 ](https://github.com/mukhal/grace)}获取。

https://arxiv.org/abs/2312.08935

Math-Shepherd：无需人工标注逐步验证与强化 LLMs

王培毅、李磊、邵志宏、徐瑞祥、戴大迈、李逸飞、陈德利、吴洋、隋志芳

> 本文提出了一种创新的过程导向型数学过程奖励模型\textbf{Math-Shepherd}，该模型能够为数学问题解答的每个步骤分配奖励分数。Math-Shepherd 的训练通过自动构建的过程监督数据实现，突破了现有工作中对人工标注高度依赖的瓶颈。我们探索了 Math-Shepherd 在两种场景下的有效性：1）\textit{验证}：利用 Math-Shepherd 对大型语言模型（LLMs）生成的多个输出进行重排序；2）\textit{强化学习}：采用 Math-Shepherd 通过逐步近端策略优化（PPO）来增强 LLMs。实验表明，结合 Math-Shepherd 的一系列开源 LLMs 展现出卓越性能。例如，采用 Math-Shepherd 的逐步 PPO 显著提升了 Mistral-7B 的准确率（GSM8K 数据集从 77.9\%提升至 84.1\%，MATH 数据集从 28.6\%提升至 33.0\%）。若进一步通过 Math-Shepherd 进行验证，在 GSM8K 和 MATH 数据集上的准确率可分别提升至 89.1\%和 43.5\%。我们相信，自动化的过程监督对 LLMs 的未来发展具有重要潜力。

https://arxiv.org/abs/2408.15240

生成式验证器：将奖励建模视为下一词元预测

张伦君，阿里安·侯赛尼，赫里蒂克·班萨尔，梅兰·卡泽米，阿维拉尔·库马尔，里沙布·阿加瓦尔

> 验证器或奖励模型常被用于提升大语言模型（LLMs）的推理性能。常见的方法是 Best-of-N 策略，即由 LLM 生成 N 个候选解决方案，经验证器排序后选择最优解。虽然基于 LLM 的验证器通常被训练为判别式分类器来评分解决方案，但这种方式并未充分利用预训练 LLMs 的文本生成能力。为突破这一限制，我们提出改用普遍采用的下一词预测目标来训练验证器，使其同时具备验证和解决方案生成能力。与标准验证器相比，这类生成式验证器（GenRM）能继承 LLMs 的多种优势：它们与指令微调无缝集成，支持思维链推理，并能通过多数表决机制利用额外测试时计算资源来优化验证效果。实验表明，GenRM 在判别式验证器、DPO 验证器和 LLM-as-a-Judge 等基准上均表现更优，使用 Best-of-N 策略时性能提升显著——算法任务达到 5% 45.3%，GSM8K 数学题达到 73% 93.4%。 在易到难的泛化场景中，我们观察到 MATH 数据集上准确率提升 28%至 44.6%，MMLU 抽象代数部分提升 37.9%至 53.5%。此外，研究发现使用合成验证原理训练 GenRM 足以识别数学问题中的细微错误。最后，我们证明 GenRM 的扩展性在模型规模和测试计算量方面表现优异。

![](https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=ZmZkZjQ4ZjcyMDZlMDMxNmU4NmZjYjk5MDJlY2QzYjhfMUtQZW9NOUpmZHFKRnZLNGMyMHJIMkd1RFhMQUEzdFRfVG9rZW46S0lPcGJxVGc0b0JWMDd4djJkYmNqRGI3bmxmXzE3NTQ1NDE0MDA6MTc1NDU0NTAwMF9WNA)

![](https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=ODRiYjdlZWViZTdhNTcyODljNjkwNjI1Njg3MTllMWRfZlM4ZDFudDFJaUduTE1PczFNdjVWSlQ0Nms2NWlTYmlfVG9rZW46VXhSemIwZkppb0owazh4MHJMSGNqWXc1bkJjXzE3NTQ1NDE0MDA6MTc1NDU0NTAwMF9WNA)

![](https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MzRiODRmYzliN2M0M2ZlMTkzOGMzNTM3ZGI5ZDU4YjBfZXpZT2xkblJjZFduejdoc0JUWmQzdFRyR1VJTGlnNFBfVG9rZW46TmN5UmJhdFFvb2JtV1J4QlAza2NmYjhpbkpjXzE3NTQ1NDE0MDA6MTc1NDU0NTAwMF9WNA)

![](https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MTM3ZjAyNTIzYTRiNmQyM2U3OTc5M2RmZTVlN2M4MDdfZVdmTml6NW8xcXphWXo1ZFozdUtlMUtyYUVIbG9iWnVfVG9rZW46SVBHNmJ4SWRsb1B1bmV4am1xM2NONTFVbk5mXzE3NTQ1NDE0MDA6MTc1NDU0NTAwMF9WNA)

![](https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MjczNmYxOGExMzU4MjQxMTJmM2YyZTI3ZDRkMDk5MDhfYlJqaXJWVFNZT3I2QVdnVTlsQVpScW5iOTZaa3I2QlhfVG9rZW46V1F3UmJMZDJibzBZb3Z4Sm9ndWMwb2ZrbjE0XzE3NTQ1NDE0MDA6MTc1NDU0NTAwMF9WNA)

![](https://zhipu-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=M2Q3NGFmMDYxYThlYjMyZGViNWEzN2RlYzZjMDUyOWRfdTBjbjRTa1pmTm9GWjVWSVZ2WnVtWlpzbk15WnJVQ0RfVG9rZW46Rk1aZ2JDeDBzb1pldGR4aVJXU2M5ZEtobmloXzE3NTQ1NDE0MDA6MTc1NDU0NTAwMF9WNA)

2. 2025-06-23 11:07:56 Monday | https://arxiv.org/abs/2506.15498

SPARE：基于参考引导评估的单次标注框架——面向自动化过程监督与奖励建模

Md Imbesat Hassan Rizvi, Xiaodan Zhu, Iryna Gurevych

过程式或分步监督在提升大语言模型（LLMs）的复杂多步推理能力方面发挥着关键作用。然而，如何实现高效、高质量的**自动化过程标注**仍面临重大挑战。为此，我们提出基于参考引导评估的单次标注框架（SPARE），该创新性结构化框架通过将每个解题步骤与参考解决方案中的一个或多个步骤对齐，并辅以明确的评估依据，实现了单次逐步骤标注。实验表明，参考引导的步骤级评估能有效促进四个跨领域数据集（涵盖数学推理、多跳组合问答和空间推理三大领域）的过程监督。相较于基线方法，SPARE 在以下场景显著提升推理性能：（1）在离线强化学习设置中微调模型以实现推理阶段的贪婪解码；（2）训练奖励模型以对多个 LLM 生成输出进行排序/聚合。 此外，SPARE 在具有挑战性的数学数据集上展现出与基于树搜索的自动标注方法相媲美的性能，同时效率提升 2.6 倍，仅需 38%的运行时间。我们公开了代码库及训练好的 SPARE-PRM 模型，以促进后续研究和结果复现。

### 生成奖励模型

2025-06-28 17:27:00 Saturday ｜ 通才奖励建模的推理时间缩放

提出了一个叫做 **SPCT方法** （Self-Principled Critique Tuning）的方法——首次提出通过在线强化学习（RL）优化原则和批判生成，实现推理时扩展。

 **生成式奖励模型** （GRM）：它采用点式生成奖励模型（Pointwise GRM），通过生成文本形式的奖励（如critiques）而非单一标量值，支持灵活输入（单响应、多响应）和推理时扩展。

SPCT是一个两阶段的过程，它们分别是：

* **拒绝式微调（Rejective Fine-Tuning）** ：冷启动阶段，通过采样和拒绝策略生成初始数据。
* **基于规则的在线RL** ：使用规则化奖励函数优化原则和批判的生成，鼓励模型区分最佳响应。

 **推理时扩展技术：** 先是通过多次采样生成多样化的原则和批判，投票聚合最终奖励，扩展奖励空间。再训练一个辅助模型过滤低质量采样，进一步提升扩展效果。

论文地址：https://arxiv.org/abs/2504.02495

### 奖励模型评估

20250604｜RewardBench 2: Advancing Reward Model Evaluation

1. 标题： RewardBench 2：高级奖励模型评估
2. 链接：[https://arxiv.org/abs/2506.01937](https://arxiv.org/abs/2506.01937)

2025-06-24 13:02:03 Tuesday ｜

ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models
 **链接** ：https://arxiv.org/abs/2506.16712

 **作者** ： Xinzge Gao, Chuanrui Hu, Penghang Yu, Hua Zhang, Bing-Kun Bao
 **摘要** ：生成奖励模型（GRM）在捕获人类偏好方面提供了比标量奖励模型更大的灵活性，但其有效性受到推理能力差的限制。这通常会导致不完整或过度推测的推理路径，导致幻觉或在复杂任务中丢失关键信息。我们用ReasonGRM来解决这个挑战，这是一个三阶段生成奖励建模框架。在第一阶段，Zero-RL用于生成简洁的、结果导向的推理路径，以减少关键遗漏的可能性。在第二阶段，我们引入了一种新的评价指标，$R^\star$，它的分数推理路径的生成可能性的基础上。这有利于以最少的探索获得正确答案的路径，有助于减少训练过程中容易产生幻觉的数据。在最后阶段，通过对具有挑战性的示例进行强化学习来进一步改进模型，以增强其偏好辨别能力。在三个公共基准测试上的实验表明，ReasonGRM实现了具有竞争力或最先进的性能，平均比之前最好的GRM高出1.8%，比GPT-4 o等专有模型高出5.6%。这些结果证明了推理感知训练的有效性，并强调了高质量的合理选择对可靠偏好建模的重要性。

## RLVR 可验证奖励的强化学习

2025-07-01 12:11:14 Tuesday ｜

**[Jan-nano 技术报告](https://papers.cool/arxiv/2506.22760)** **[PDF(1)]** **[Copy]** **[Kimi()]** **[REL]**

 **Authors** : [Alan Dao](https://arxiv.org/search/?searchtype=author&query=Alan%20Dao), [Dinh Bach Vu](https://arxiv.org/search/?searchtype=author&query=Dinh%20Bach%20Vu)

大多数语言模型都面临着一个基本的权衡，即强大的功能需要大量的计算资源。我们用 Jan-nano 打破了这个限制，Jan-nano 是一个 4B 参数语言模型，它通过激进的专业化重新定义了效率：它不是试图了解一切，而是掌握了立即找到任何东西的艺术。使用我们新颖的多阶段 RLVR 系统从 Qwen3-4B 进行微调，该系统完全消除了对下一个标记预测训练 （SFT） 的依赖，在消费类硬件上运行时，Jan-nano 在 MCP 集成的 SimpleQA 基准测试中实现了 83.2%。Jan-nano 拥有 128K 的上下文长度，证明了智能与规模无关，而与策略有关。

 **主题** : **[计算和语言](https://papers.cool/arxiv/cs.CL)**

 **发布** ： 2025-06-28 05：44：57 UTC

2. 2025-06-19 20:36:57 Thursday ｜

Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes  Correct Reasoning in Base LLMs
 **标题** ： 具有可验证奖励的强化学习激励基本LLM中的正确推理
 **链接** ：https://arxiv.org/abs/2506.14245

 **作者** ： Xumeng Wen,  Zihan Liu,  Shun Zheng,  Zhijian Xu,  Shengyu Ye,  Zhirong Wu,  Xiao Liang,  Yang Wang,  Junjie Li,  Ziming Miao,  Jiang Bian,  Mao Yang
 **备注** ：Preprint
 **摘要** ：具有可验证奖励的强化学习（RLVR）已经成为提高大型语言模型（LLM）推理能力的一种有前途的范例。然而，一个关键的悖论云其功效：RLVR调整的模型往往表现不佳的基础模型的$Pass@K$度量的解决方案的发现，导致假设RLVR只是重新加权现有的推理路径的推理多样性的成本。在这项工作中，我们通过确定问题的来源来解决这个矛盾：$Pass@K$度量本身是一个有缺陷的推理度量，因为它认为正确的最终答案可能来自不准确或不完整的思想链（CoTs）。为了解决这个问题，我们引入了一个更精确的评估指标，$CoT$-$Pass@K$，它要求推理路径和最终答案都是正确的。我们提供了一个新的理论基础，形式化的RLVR，不同于传统的RL，是独特的结构，以激励逻辑的完整性。我们的实证结果是支持的：使用$CoT$-$Pass@K$，我们观察到RLVR可以激励正确推理的泛化为所有值的$K$。此外，通过分析训练动态，我们发现这种增强的推理能力出现在训练过程的早期，并顺利推广。我们的工作为RLVR的作用提供了一个清晰的视角，为其评估提供了一种更可靠的方法，并证实了它真正推进机器推理的潜力。

3. 2025-06-23 10:46:10 Monday ｜ 基于可验证奖励训练接地 LLMs 的经验总结

生成可靠且可信的响应仍然是大型语言模型（LLMs）面临的关键挑战。虽然基于引用的检索增强生成（RAG）技术展现出潜力，但经过指令调优的模型即使在简单场景中也常出现失误：遗漏明确陈述的答案、错误引用或在证据可用时拒绝回答。本研究探索了如何通过强化学习（RL）和内部推理机制来增强 LLMs 的可靠性。我们采用 GRPO（组相对策略优化）方法，利用可验证的结果型奖励机制训练模型，该机制针对答案准确性、引用充分性和拒绝质量进行优化，且无需黄金推理轨迹或昂贵的人工标注。通过在 ASQA、QAMPARI、ELI5 和 ExpertQA 数据集上的全面实验表明，推理增强模型显著优于纯指令调优版本，尤其在处理不可回答问题与生成高引用质量响应方面表现突出。采用两阶段训练方案——先优化答案和引用行为，再优化拒绝机制——通过稳定学习信号进一步提升了模型的可靠性。 此外，我们通过 GPT-4 蒸馏方法重新审视指令调优，发现将其与 GRPO 结合能提升长文本生成式问答任务的性能。总体而言，我们的研究结果凸显了推理能力、分阶段优化以及结果导向的强化学习对于构建更具可验证性和可靠性的 LLMs 的重要价值。 https://arxiv.org/abs/2506.15522

4. 🙋❓ 2025-06-28 18:17:38 Saturday ｜ 突破通用领域推理的瓶颈！清华NLP实验室强化学习新研究RLPR https://mp.weixin.qq.com/s/B11Ef8YwOzPHZn1SXdpp7w

基于参考概率奖励的强化学习（Reinforcement Learning with Reference Probability Reward， **RLPR** ）。

论文标题：RLPR: Extrapolating RLVR to General Domains without Verifiers

论文地址：https://github.com/OpenBMB/RLPR/blob/main/RLPR_paper.pdf

GitHub 仓库：https://github.com/OpenBMB/RLPR

过使用**参考答案的生成概率均值**作为奖励。这种方法能够有效地应对自然语言固有的复杂多样性。

#### [IFDECORATOR：用可验证奖励包装指令跟随强化学习](https://papers.cool/arxiv/2508.04632)

[#7](https://arxiv.org/abs/2508.04632)[IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards ](https://papers.cool/arxiv/2508.04632)

可验证奖励强化学习（RLVR）提升了大型语言模型（LLMs）的指令执行能力，但由于难度评估不足，训练效率较低。此外，RLVR 容易出现过度优化现象，即 LLMs 利用验证捷径而未能与用户指令的实际意图保持一致。我们提出了指令执行装饰器（IFDecorator）框架，将 RLVR 训练封装成一个稳健且样本高效的流程。该框架包含三个部分：

（1）合作-对抗数据飞轮，协同进化指令和混合验证，生成逐步更具挑战性的指令-验证对；

（2）IntentCheck，执行意图对齐的旁路模块；

（3）触发线，一种通过陷阱指令检测奖励作弊的诊断机制，能够触发并捕捉捷径利用行为。

我们的 Qwen2.5-32B-Instruct-IFDecorator 在 IFEval 上达到 87.43%的准确率，优于更大规模的专有模型如 GPT-4o。此外，我们在 FollowBench 上也展示了显著提升，同时保持了模型的通用能力。 我们的触发机制显示奖励作弊率显著降低。我们将发布模型、代码和数据以供未来研究。

发布时间：2025-08-06 17:00:54 UTC

## 数学

2025-06-27 14:21:00 Friday ｜

**[桥接 LLM 的离线和在线强化学习](https://papers.cool/arxiv/2506.21495)** **[PDF(7)]** **[Copy]** **[Kimi(2)]** **[REL]**

[#6](https://arxiv.org/abs/2506.21495) **[Bridging Offline and Online Reinforcement Learning for LLMs](https://papers.cool/arxiv/2506.21495)** **[PDF(7)]** **[Copy]** **[Kimi(2)]** **[REL]**

作者：杰克·兰钱汀、安吉莉卡·陈、贾尼斯·兰、李贤、斯瓦纳迪普·萨哈、王天璐、徐静、余平、袁伟哲、杰森·E·韦斯顿、赛因巴亚尔·苏赫巴托尔、伊利亚·库利科夫

我们研究了强化学习方法在从离线过渡到半在线再到完全在线状态时对大型语言模型进行微调的有效性，以完成可验证和不可验证的任务。我们的实验涵盖可验证数学和不可验证教学的训练，然后对两者进行了一组基准评估。在这些设置中，我们广泛比较了在线和半在线 Direct Preference Optimization 和 Group Reward Policy Optimization 目标，并令人惊讶地发现这些变体之间的性能和收敛性相似，它们都明显优于离线方法。我们提供了对训练动力学和超参数选择策略的详细分析，以实现最佳结果。最后，我们表明，具有可验证和不可验证奖励的多任务处理共同提高了两种任务类型的性能。

20250604｜Response-Level Rewards Are All You Need for Online Reinforcement  Learning in LLMs: A Mathematical Perspective
**标题** ： 响应级别奖励就是LLM在线强化学习所需的一切：数学的角度
**链接** ：https://arxiv.org/abs/2506.02553
**摘要** ：我们研究了大型语言模型（LLM）强化学习中的一个常见挑战：零回报假设，其中非终端动作（即，中间令牌生成）接收零任务特定的立即奖励，而只有最终令牌接收整个响应的奖励。这种假设在实践中经常出现，因为在LLM应用程序中获得精确的令牌级奖励通常是困难或不可行的。在这项工作中，我们提供了一个统一的理论观点。我们引入了 **轨迹策略梯度定理** ，该定理表明，对于REINFORCE和Actor-Critic家族中的算法，基于真实的未知令牌级奖励的策略梯度可以仅使用响应级奖励模型进行无偏估计，而不管零奖励假设是否成立。这一结果表明，PPO、GRPO、ReMax和RLOO等广泛使用的方法本质上具有对令牌级奖励信号建模的能力，为响应级奖励方法提供了理论依据。我们的研究结果为更实用，更有效的LLM微调铺平了道路，允许开发人员将训练算法视为黑箱，并专注于使用辅助子模型改进响应级奖励模型。我们还对流行的RL和非RL方法进行了详细分析，比较了它们在常见LLM任务中的理论基础和实践优势。最后，我们提出了一种新的算法：令牌增强策略优化（TRePO），这是一种理论上的方法，比PPO更简单，在内存效率上与GRPO相匹配，并具有广泛的适用性。

🌈 2025-06-10 10:15:31 Tuesday ｜ 推理模型如何攻克数学难题？Epoch AI新研究发现，o3-mini-high不仅具备渊博学识，还会基于直觉解题。然而，它的推理风格过于依赖直觉，缺乏严谨性和创造力，甚至偶尔「投机取巧」。https://mp.weixin.qq.com/s/Bi-EpYpJ_7damcdRPtzCmA  https://mp.weixin.qq.com/s/IfTQQ-XAFDxap6B7_OHC1g

1. ❗这里提到了alphaproof，有必要学习一下 [AlphaProof 技术解析 | IMO 系列](https://zhuanlan.zhihu.com/p/2711128759)
   1. 是用类似 AlphaZero 的办法训的（和自己下围棋），所以应该用了 MCTS
   2. finetune 了 gemini 来做自动形式化（autoformalization），把自然语言的问题转换为形式语言。
   3. 要用 lean 这种形式化证明的工具，这样子就可以获得来自 proof assistant 的精确的反馈。
2. AlphaGeometry 和 Alphaproof 还是挺不一样的，它的推理部分应该用了很多传统方法(也用了吴文俊提出的方法作为基准 吴文俊，把几何问题转换成代数问题来自动解决)。
   1. 它的点在于合成数据(而非利用人类数据)和生成辅助线(这个传统方法不大会)。
   2. 它合成数据的时候先采样一堆前提，然后用传统方法在上面一通推理，可以得到一堆结论对于每个结论都可以找到一个最小的前提集合可以恰好推出它，这样就造出来一道几何题。其中前提集合里和结论无关的就是辅助线，然后就可以拿来给llm学学学了。
3. DeepMind 的 FunSearch 通过搜索优先级的方法，发现了目前最大的 Cap Set。Cap Set 问题是一个组合数学问题，FunSearch 的成功在此问题上展示了其在离散数学领域的强大推理能力。

### 多模态数学

2025-06-26 13:36:30 Thursday ｜让多模态大模型「想明白再画」！港大等开源GoT-R1：强化学习解锁视觉生成推理新范式 https://mp.weixin.qq.com/s/sPdhPeNMkdBLjtLzI5urwg

该新框架通过引入强化学习，显著增强了多模态大模型在视觉生成任务中的语义 - 空间推理能力，使其能够超越预定义模板，自主探索和学习更优的推理策略。GoT 和 GoT-R1 已全面开源。

* GoT arxiv:https://arxiv.org/pdf/2503.10639
* GoT github:https://github.com/rongyaofang/GoT
* GoT-R1 arxiv: https://arxiv.org/pdf/2505.17022
* GoT-R1 github:https://github.com/gogoduan/GoT-R1

2. 2025-06-24 13:31:26 Tuesday ｜

GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning
 **链接** ：https://arxiv.org/abs/2506.16141

 **作者** ：Yuying Ge, Rui Wang, Yixiao Ge, Junhao Cheng, Ying Shan, Xihui Liu
 **备注** ：         Code released at: this https URL
 **摘要** ：最近的强化学习方法，如结果监督GRPO，已经在大型语言模型（LLM）中推进了思想链推理，但它们对多模态LLM（MLLM）的适应尚未探索。为了解决MLLM后期训练方法缺乏严格评估的问题，我们引入了SEED-Bench-R1，这是一个需要平衡感知和推理的复杂现实视频基准。它提供了一个大型的训练集，并在三个不断升级的挑战中评估泛化：分布，跨环境和跨环境任务场景。使用SEED-Bench-R1，我们发现标准GRPO在提高答案准确性的同时，往往会降低推理步骤和答案之间的逻辑一致性，只有57.9%的一致率。这源于奖励信号只关注最终答案、鼓励捷径以及限制探索的严格KL惩罚。为了解决这个问题，我们提出了GRPO-CARE，这是一个一致性感知RL框架，在没有明确监督的情况下优化答案正确性和推理一致性。GRPO-CARE引入了两层奖励：（1）对答案正确性的基本奖励，以及（2）自适应一致性奖励，通过比较模型的推理到答案的可能性（通过一个缓慢进化的参考模型）与群体对等体来计算。这种双重机制放大了对正确和逻辑一致的推理路径的奖励。用这种自适应奖金取代KL惩罚，GRPO-CARE在SEED-Bench-R1上的表现优于标准GRPO，在最难的评估水平上实现了6.7%的性能增益，在一致性上提高了24.5%。它还显示出强大的可移植性，提高了不同视频理解基准的模型性能。我们的工作贡献了一个系统设计的基准和一个可推广的培训后框架，推进了更可解释和更强大的MLLM的发展。

## 推理

#### 结构化探索框架：First Return, Entropy-Eliciting Explore（FR3E）

1. [字节&amp;MAP重塑大模型推理算法优化重点，强化学习重在高效探索助力LLM提升上限](https://mp.weixin.qq.com/s/8XfVIRaLfgViBCPNVew0jA)
   1. 强化学习（RL）范式虽然显著提升了大语言模型（LLM）在复杂任务中的表现，但其在实际应用中仍面临传统RL框架下固有的探索难题。一个普遍存在的现象是：在训练过程中，模型的熵值迅速下降，推理路径趋于固化，导致“利用（exploitation）”远超“探索（exploration）”，严重失衡。
   2. 受OpenAI经典论文《First Return, Then Explore》中“先返回，再探索”思想的启发，来自字节跳动、MAP，曼彻斯特大学的联合团队提出了一种全新的结构化探索框架：First Return, Entropy-Eliciting Explore（FR3E）。

      1. 该方法通过识别推理轨迹中具有高不确定性的关键token，并以此为锚点引导后续的多样化展开，系统性地重建了LLM在强化学习中的探索机制，旨在实现利用与探索之间的动态平衡，从而释放RL训练的更高潜力。
      2. FR3E的算法框架分为两个阶段：
         1. 第一阶段：First Return
         2. 第二阶段：Entropy-Eliciting Explore：FR3E在GRPO++（融合了拒绝采样与Clip-Higher机制的GRPO变体）的基础上，进一步引入动态优势调制机制，以更精细地调控学习信号
   3. 论文地址：https://arxiv.org/pdf/2507.07017

#### GRPO

2025-07-03 11:45:44 Thursday ｜

**[使用 Masked Step Advantage 进行自我引导的过程奖励优化，用于过程强化学习](https://papers.cool/arxiv/2507.01551)** **[PDF(5)]** **[Copy]** **[Kimi()]** **[REL]**

 **Authors** : [Wu Fei](https://arxiv.org/search/?searchtype=author&query=Wu%20Fei), [Hao Kong](https://arxiv.org/search/?searchtype=author&query=Hao%20Kong), [Shuxian Liang](https://arxiv.org/search/?searchtype=author&query=Shuxian%20Liang), [Yang Lin](https://arxiv.org/search/?searchtype=author&query=Yang%20Lin), [Yibo Yang](https://arxiv.org/search/?searchtype=author&query=Yibo%20Yang), [Jing Tang](https://arxiv.org/search/?searchtype=author&query=Jing%20Tang), [Lei Chen](https://arxiv.org/search/?searchtype=author&query=Lei%20Chen), [Xiansheng Hua](https://arxiv.org/search/?searchtype=author&query=Xiansheng%20Hua)

过程强化学习~（PRL） 在增强大型语言模型~（LLM）的推理能力方面已经显示出相当大的潜力。然而，引入额外的流程奖励模型会产生大量的计算开销，并且没有统一的流程级优势估计理论框架。为了弥合这一差距，我们提出了 \textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward \textbf{O}ptimization~（\textbf{SPRO}），这是一个新颖的框架，通过两个关键创新实现了过程感知 RL： **（1） 我们首先从理论上证明了过程奖励可以从策略模型本身本质上得出，以及 （2） 我们引入了定义明确的累积过程奖励和 \textbf{M}asked \textbf{S}tep \textbf{A}dvantage （\textbf{MSA}）， 这有助于在共享提示采样组中进行严格的逐步行动优势估计** 。我们的实验结果表明， **SPRO 的性能优于 vaniila GRPO，训练效率提高了 3.4 倍，测试准确率提高了 17.5%** 。此外，SPRO 在整个训练过程中保持稳定且较高的策略熵，同时将平均响应长度减少约 1/3，证明对奖励黑客行为进行了充分的探索和预防。值得注意的是，与结果监督的 RL 方法（如 GRPO）相比，SPRO 不会产生额外的计算开销，这有利于工业实施。

 **科目** :  **[机器学习](https://papers.cool/arxiv/cs.LG)** , [人工智能](https://papers.cool/arxiv/cs.AI), [计算和语言](https://papers.cool/arxiv/cs.CL)

 **发布** ： 2025-07-02 10：05：14 UTC

2. 20250604｜Critique-GRPO: Advancing LLM Reasoning with Natural Language and  Numerical Feedback
   **标题** ： Critique-GRPO：利用自然语言和数字反馈推进LLM推理
   **链接** ：https://arxiv.org/abs/2506.03106
   **摘要** ：最近，带有数值反馈的强化学习（RL）的进展，如标量奖励，显着增强了大型语言模型（LLM）的复杂推理能力。尽管取得了这一成功，我们确定了RL遇到的三个关键挑战， **只有数字反馈** ：性能高原， **自我反思的有效性有限** ，和 **持续的失败** 。然后，我们证明，RL微调模型，即使表现出性能平台，可以产生正确的改进持续失败的问题，通过利用自然语言反馈的形式的批评。基于这一见解，我们提出了Critique-GRPO，这是一个在线RL框架，它集成了自然语言和数值反馈，以实现有效的策略优化。CRitique-GRPO使LLM能够在保持探索的同时 **从初始响应和批评指导的改进中学习** 。使用**Qwen2.5- 7 B-Base**和Qwen 3 -8B-Base进行的大量实验表明，Critique-GRPO在8个具有挑战性的数学、STEM和一般推理任务中始终优于基于监督学习和基于RL的微调方法，分别将平均pass@1分数提高了约4.5%和5%。值得注意的是，Critique-GRPO超越了在线RL中包含专家演示的强大基线。进一步的分析揭示了关于策略探索的两个关键见解：（1）更高的熵并不总是保证从探索中有效学习，（2）更长的响应不一定会导致更有效的探索。
3. 2025-06-20 11:39:57 Friday ｜ DPO与GRPO谁更胜一筹？港中文、北大等联合发布首个系统性对比研究 https://mp.weixin.qq.com/s/bv6biTQUCr49gvu7PmECTg

 **该研究首次对 GRPO 和 DPO 算法在自回归图像生成中的应用进行了全面深入的比较** ，不仅评估了它们在域内（in-domain）和域外（out-of-domain）的性能，还细致探究了不同奖励模型及扩展策略对其能力的影响。

论文标题：Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO

论文链接：https://arxiv.org/abs/2505.17017

代码链接：https://github.com/ZiyuGuo99/Image-Generation-CoT

#### Qwen3 模型提出GSPO 算法

1. [DeepSeek的GRPO会导致模型崩溃？看下Qwen3新范式GSPO](https://mp.weixin.qq.com/s/YSlp-SXzi7bSW2Y-shJ8ww)
   1. 发表了一篇博客，题为《Qwen Team Proposes GSPO for Qwen3, Claims DeepSeek's GRPO is Ill-Posed》，对 Qwen 团队为 Qwen3 模型提出的 GSPO 算法进行了详尽的介绍与分析。 https://blog.netmind.ai/article/Qwen_Team_Proposes_GSPO_for_Qwen3%2C_Claims_DeepSeek's_GRPO_is_Ill-Posed
   2. 组序列策略优化（Group Sequence Policy Optimization, GSPO）。GSPO 的方法有两点创新：将重要性采样从 token 级别提升到序列级别，并通过序列长度进行归一化处理；显著降低了方差，同时消除了对「路由技巧」（如 Routing Replay）等辅助策略的依赖；


## 垂直领域

### 具身

2025-07-01 12:06:54 Tuesday ｜

**[通过强化学习在 LLM 中释放隐身任务规划能力](https://papers.cool/arxiv/2506.23127)** **[PDF()]** **[Copy]** **[Kimi()]** **[REL]**

 **Authors** : [Zhaoye Fei](https://arxiv.org/search/?searchtype=author&query=Zhaoye%20Fei), [Li Ji](https://arxiv.org/search/?searchtype=author&query=Li%20Ji), [Siyin Wang](https://arxiv.org/search/?searchtype=author&query=Siyin%20Wang), [Junhao Shi](https://arxiv.org/search/?searchtype=author&query=Junhao%20Shi), [Jingjing Gong](https://arxiv.org/search/?searchtype=author&query=Jingjing%20Gong), [Xipeng Qiu](https://arxiv.org/search/?searchtype=author&query=Xipeng%20Qiu)

大型语言模型 （LLM） 在各种任务中都表现出了卓越的能力，但它们在需要持续理解环境和生成行动的具身任务规划场景中面临着重大挑战。现有方法基于静态知识生成开环动作脚本，因此很难学习动作和环境反馈之间的因果关系，尤其是在部分可观察的环境中。我们介绍了 Embodied Planner-R1，这是一种新颖的结果驱动强化学习框架，使 LLM 能够在最少的监督下通过自主探索来开发交互功能。我们的框架融合了三个关键创新：（1） 没有人工注释，我们采用纯强化学习和分组推出，通过并行探索结合环境内交互;（2） 完成驱动的稀疏奖励;（3） 交互式策略优化 （IPO），用于从分组轨迹中高效学习。在两个具有挑战性的基于文本的 Embodied 规划基准测试中，Embodied Planner-R1 在 ALFWorld 上实现了令人印象深刻的 97.78% 和 79.92% 的完成率，大大超过了以前的方法，并且在以前从未见过的环境中仅下降了 -3.66%，证明了强大的泛化性。

 **科目** :  **[计算和语言](https://papers.cool/arxiv/cs.CL)** , [人工智能](https://papers.cool/arxiv/cs.AI)

### 攻击

2025-07-01 12:10:48 Tuesday｜

**[教授模型在思维链推理中用语言解释奖励黑客攻击](https://papers.cool/arxiv/2506.22777)** **[PDF()]** **[Copy]** **[Kimi(1)]** **[REL]**

 **Authors** : [Miles Turpin](https://arxiv.org/search/?searchtype=author&query=Miles%20Turpin), [Andy Arditi](https://arxiv.org/search/?searchtype=author&query=Andy%20Arditi), [Marvin Li](https://arxiv.org/search/?searchtype=author&query=Marvin%20Li), [Joe Benton](https://arxiv.org/search/?searchtype=author&query=Joe%20Benton), [Julian Michael](https://arxiv.org/search/?searchtype=author&query=Julian%20Michael)

使用 RL 训练的语言模型可以参与奖励黑客攻击，即利用无意的策略获得高回报，而不会在其思维链推理中揭示这种行为，从而使检测变得困难，并为高风险应用程序带来风险。我们提出了语言微调 （VFT），这是一种 RL 之前的干预，它训练模型在受到提示提示的影响时明确承认它们——指向错误答案的提示（例如，“斯坦福大学教授认为答案是 A”）。为了评估 VFT，我们随后在保留的提示提示表明哪些错误答案将获得高奖励的环境中使用 RL 训练模型，从而激励模型通过利用提示而不是正确推理来奖励黑客。我们测量模型在不用语言表达的情况下利用这些线索的频率。在 RL 之后，只有 6% 的 VFT 训练模型的响应由未检测到的奖励黑客攻击组成。相比之下，当我们在没有 VFT 的情况下执行 RL 时，未被发现的奖励黑客攻击率高达 88%;通过去偏倚基线干预，这一比例进一步增加到 99%。VFT 通过大幅提高模型表达线索影响的频率来实现这一点——从 VFT 后的 8% 到 42%，RL 后高达 94%——而即使在 RL 之后，基线仍然很低（10% 和 1%）。我们的结果表明，在 RL 之前训练模型明确地用语言表达奖励黑客行为可以显着提高其检测能力，从而为实现更透明、更安全的 AI 系统提供了一条实用的途径。

 **科目** :  **[计算和语言](https://papers.cool/arxiv/cs.CL)** , [人工智能](https://papers.cool/arxiv/cs.AI)

 **发布** ： 2025-06-28 06：37：10 UTC
