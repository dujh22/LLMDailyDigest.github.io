+++
title = '元'
date = 2025-08-07T00:00:00+08:00
draft = false
toc = true
+++

# 元

1. 2025-06-12 12:05:13 Thursday｜ Large Language Models Have Intrinsic Meta-Cognition, but Need a Good  Lens
   **标题** ： 大型语言模型具有内在的元认知，但需要良好的镜头
   **链接** ：https://arxiv.org/abs/2506.08410

 **作者** ： Ziyang Ma,  Qingyue Yuan,  Zhenglin Wang,  Deyu Zhou
 **备注** ：Preprint
 **摘要** ：以前的研究主要集中在大型语言模型（LLM）的认知错误检测能力上，通常会促使它们分析推理链中的错误。然而，很少有研究考察LLM的**元认知**能力（例如，他们的自我意识的步骤错误），这是至关重要的，他们的可靠性。尽管对LLM自我评价的研究提出了**困惑度**等能够反映答案正确性的测量指标，并将其作为元认知的透镜，但缺乏对LLM自我评价的步骤分析和适应性研究。本文研究了如何用现有的评价方法来评价法学硕士元认知，以及如何改进这些评价方法。具体来说，我们提出了AutoMeco，一个 **自动化的元认知评估框架** ，用于对现有的镜头进行基准测试。此外，提出了一种无需训练的马尔可夫内在奖励调整策略MIRA，以提高当前的元认知镜头。在三个数学推理数据集和三个LLM上的实验结果表明，AutoMeco验证方法与Best-of-N验证方法相比具有一定的合理性。此外，MIRA可以更好地评估LLM的元认知能力。

## 认知能力

2025-06-30 19:58:27 Monday ｜

**[了解大型推理模型的认知习惯](https://papers.cool/arxiv/2506.21571)** **[PDF()]** **[Copy]** **[Kimi(2)]** **[REL]**

 **Authors** : [Jianshuo Dong](https://arxiv.org/search/?searchtype=author&query=Jianshuo%20Dong), [Yujia Fu](https://arxiv.org/search/?searchtype=author&query=Yujia%20Fu), [Chuanrui Hu](https://arxiv.org/search/?searchtype=author&query=Chuanrui%20Hu), [Chao Zhang](https://arxiv.org/search/?searchtype=author&query=Chao%20Zhang), [Han Qiu](https://arxiv.org/search/?searchtype=author&query=Han%20Qiu)

大型推理模型 （LRM） 在产生最终响应之前自主产生推理思维链 （CoT），为解释和监控模型行为提供了一种很有前途的方法。受到对某些 CoT 模式（例如，“等等，我错过了什么吗？”）在任务中不断出现的观察的启发，我们探讨了 LRM 是否表现出类似人类的认知习惯。以 Habits of Mind（一个与成功解决人类问题相关的成熟认知习惯框架）为基础，我们引入了 CogTest，这是一个旨在评估 LRM 认知习惯的原则性基准。CogTest 包括 16 个认知习惯，每个习惯由 25 个不同的任务实例化，并采用循证优先提取方法以确保可靠的习惯识别。通过 CogTest，我们对 16 种广泛使用的 LLM（13 种 LRM 和 3 种非推理 LLM）进行了全面评估。我们的研究结果表明，与传统的 LLM 不同，LRM 不仅表现出类似人类的习惯，而且还会根据不同的任务自适应地部署它们。更细粒度的分析进一步揭示了 LRM 认知习惯概况的相似性和差异性模式，特别是某些科间相似性（例如，Qwen-3 模型和 DeepSeek-R1）。将研究扩展到与安全相关的任务，我们观察到某些习惯，例如承担负责任的风险，与有害反应的产生密切相关。这些发现表明，研究 LRM 的 CoT 中的持续行为模式是更深入地理解 LLM 不当行为的宝贵一步。代码可在以下网址获得：https://github.com/jianshuod/CogTest。

 **科目** :  **[计算和语言](https://papers.cool/arxiv/cs.CL)** , [人工智能](https://papers.cool/arxiv/cs.AI), [密码学和安全性](https://papers.cool/arxiv/cs.CR)

2. 2025-06-13 18:43:14 Friday｜

 Scientists' First Exam: Probing Cognitive Abilities of MLLM via  Perception, Understanding, and Reasoning
 **标题** ： 科学家的第一次考试：通过感知，理解和推理探索MLLM的认知能力
 **链接** ：https://arxiv.org/abs/2506.10521

 **作者** ： Yuhao Zhou,  Yiheng Wang,  Xuming He,  Ruoyao Xiao,  Zhiwei Li,  Qiantai Feng,  Zijie Guo,  Yuejin Yang,  Hao Wu,  Wenxuan Huang,  Jiaqi Wei,  Dan Si,  Xiuqi Yao,  Jia Bu,  Haiwen Huang,  Tianfan Fu,  Shixiang Tang,  Ben Fei,  Dongzhan Zhou,  Fenghua Ling,  Yan Lu,  Siqi Sun,  Chenhui Li,  Guanjie Zheng,  Jiancheng Lv,  Wenlong Zhang,  Lei Bai
 **备注** ：82 pages
 **摘要** ：科学发现越来越依赖于基于信息密集型科学数据和特定领域专业知识的复杂多模态推理。在专家级科学基准的支持下，科学多模态大型语言模型（MLLM）有可能在现实的工作流程中显着增强这一发现过程。然而，目前的科学基准大多侧重于评估的知识理解能力的MLLM，导致其感知和推理能力的评估不足。为了解决这一差距，我们提出了科学家的第一次考试（SFE）基准，旨在通过三个相互关联的水平来评估MLLM的科学认知能力：科学信号感知，科学属性理解，科学比较推理。具体而言，SFE包括三种问题类型的830个专家验证的VQA对，跨越五个高价值学科的66个多模态任务。大量的实验表明，目前最先进的GPT-o3和InternVL-3在SFE上仅达到34.08%和26.52%，这突出了MLLM在科学领域的改进空间。我们希望在SFE中获得的见解将促进人工智能增强的科学发现的进一步发展。
2. 2025-06-18 10:42:19 Wednesday ｜

Unveiling the Learning Mind of Language Models: A Cognitive Framework  and Empirical Study
 **标题** ： 揭开语言模型的学习心态：认知框架和实证研究
 **链接** ：https://arxiv.org/abs/2506.13464

 **作者** ： Zhengyu Hu,  Jianxun Lian,  Zheyuan Xiao,  Seraphina Zhang,  Tianfu Wang,  Nicholas Jing Yuan,  Xing Xie,  Hui Xiong
 **摘要** ：大型语言模型（LLM）在数学、编码和推理等任务中表现出了令人印象深刻的能力，但它们的**学习能力**对于适应动态环境和获取新知识至关重要，仍然没有得到充分的探索。在这项工作中，我们通过引入一个受**认知心理学和教育启发**的框架来解决这一差距。具体来说，我们将一般学习能力分解为三个不同的、互补的维度： **从教师那里学习（通过明确的指导获得知识），从概念中学习（内化抽象结构并推广到新的背景），从经验中学习（通过积累的探索和反馈进行适应）** 。我们在三个学习维度上进行了全面的实证研究，并确定了一些有见地的发现，如 **（i）互动改善学习;（ii）概念理解是规模涌现的，有利于更大的模型;（iii）LLM是有效的Few-Shot学习者，但不是多镜头学习者** 。基于我们的框架和实证研究结果，我们引入了一个基准，提供了一个统一的和现实的评价LLM的一般学习能力在三个学习认知维度。它能够提供诊断见解，并支持评估和开发更具适应性和人性化的模型。
3. 2025-06-18 10:45:36 Wednesday ｜

MotiveBench: How Far Are We From Human-Like Motivational Reasoning in  Large Language Models?
 **标题** ： MoativeBench：我们距离大型语言模型中的类人动机推理还有多远？
 **链接** ：https://arxiv.org/abs/2506.13065

 **作者** ： Xixian Yong,  Jianxun Lian,  Xiaoyuan Yi,  Xiao Zhou,  Xing Xie
 **摘要** ：大型语言模型（LLM）已被广泛采用为各种场景中的Agent框架的核心，例如社会模拟和AI同伴。然而，它们**能在多大程度上复制类似人类的动机**仍然是一个未被探索的问题。现有的基准受到简单化的场景和缺乏字符身份的限制，导致与现实世界的情况下的信息不对称。为了解决这一差距，我们提出了MotiveBench，它由200个丰富的情境场景和600个推理任务组成，涵盖了多个层次的动机。使用MotiveBench，我们对七个流行的模型家族进行了广泛的实验，比较了每个家族中不同的尺度和版本。结果表明，即使是最先进的LLM仍然无法实现类似人类的动机推理。我们的分析揭示了关键的发现，包括LLM在推理“爱与归属”动机时所面临的困难，以及他们过度理性和理想主义的倾向。这些见解突出了未来LLM人性化研究的一个有希望的方向。数据集、基准和代码可在https://aka.ms/motivebench上获得。

4. 2025-06-18 10:50:06 Wednesday ｜

 Eliciting Reasoning in Language Models with Cognitive Tools
 **标题** ： 用认知工具在语言模型中激发推理
 **链接** ：https://arxiv.org/abs/2506.12115

 **作者** ： Brown Ebouky,  Andrea Bartezzaghi,  Mattia Rigotti
 **备注** ：22 pages, 2 figures
 **摘要** ：最近出现的推理模型，如OpenAI的o 1，引起了AI社区对封闭模型中这些功能的机制的兴奋猜测，随后是一系列复制工作，特别是来自开源社区的复制工作。这些猜测在很大程度上被DeepSeek-R1的演示所解决，即 **思想链和强化学习（RL）可以有效地在基本LLM之上复制推理** 。然而，它仍然是有价值的探索替代方法，从理论上引出推理，可以帮助阐明潜在的机制，以及提供额外的方法，可以提供互补的好处。   在这里，我们建立在认知心理学和认知架构的长期文献， **假设推理产生于一组模块化的，预定的认知操作的协调，顺序执行。至关重要的是，我们在现代代理工具调用框架内实现了这一关键思想** 。特别是，我们赋予一个LLM与一个小的“认知工具”封装特定的推理操作，每个LLM本身执行。令人惊讶的是，这种简单的策略在标准数学推理基准测试中的性能与基本LLM相比有了相当大的提高，无论是封闭的还是开放的权重模型。例如，为GPT-4.1提供我们的“认知工具”，将其在AIME 2024上的pass@1性能从26.7%提高到43.3%，使其非常接近o 1-preview的性能。   除了它的实际影响，这个演示有助于辩论的作用后培训方法在引发推理LLM与预培训过程中获得的内在能力的作用，以及后培训是否仅仅揭示了这些潜在的能力。

5. 2025-06-18 10:52:01 Wednesday ｜
   **标题** ： 抽象、对齐、预测：基于认知归纳推理的零杆姿态检测
   **链接** ：https://arxiv.org/abs/2506.13470

 **作者** ： Jun Ma,  Fuqiang Niu,  Dong Li,  Jinzhou Cao,  Genan Dai,  Bowen Zhang
 **摘要** ：Zero-shot姿态检测（ZSSD）旨在识别文本对先前看不见的目标的姿态，这是一种传统监督模型由于依赖于标记数据和浅层词汇线索而经常失败的设置。受人类认知推理的启发，我们提出了认知归纳推理框架（CIRF），它从未标记的文本中抽象出可转移的推理模式，并将其编码为概念级逻辑。为了将这些模式与输入参数相结合，我们引入了一个模式增强的图核模型（SEGKM），动态地调整本地和全局推理结构。在SemEval-2016、VAST和COVID-19-Stance基准上的实验表明，CIRF建立了新的最先进的结果，在macro-F1中分别比强ZSSD基线高出1.0、4.5和3.3个百分点，并且在标记示例减少70%的情况下实现了相当的准确性。我们将在发布时发布完整的代码。

6. 2025-06-26 14:33:29 Thursday ｜

**#1 你内心有许多狼：运用认知模型解读 LLMs 中的价值权衡 [PDF** **()] [复制] [Kimi** **(2)** **] [相关]**

**[Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://papers.cool/arxiv/2506.20666)** **[PDF()]** **[Copy]** **[Kimi(2)]** **[REL]**

 **Authors** : [Sonia K. Murthy](https://arxiv.org/search/?searchtype=author&query=Sonia%20K.%20Murthy), [Rosie Zhao](https://arxiv.org/search/?searchtype=author&query=Rosie%20Zhao), [Jennifer Hu](https://arxiv.org/search/?searchtype=author&query=Jennifer%20Hu), [Sham Kakade](https://arxiv.org/search/?searchtype=author&query=Sham%20Kakade), [Markus Wulfmeier](https://arxiv.org/search/?searchtype=author&query=Markus%20Wulfmeier), [Peng Qian](https://arxiv.org/search/?searchtype=author&query=Peng%20Qian), [Tomer Ullman](https://arxiv.org/search/?searchtype=author&query=Tomer%20Ullman)

作者：索尼娅·K·穆尔蒂、罗西·赵、詹妮弗·胡、沙姆·卡卡德、马库斯·沃尔夫迈尔、钱鹏、托默·厄尔曼

日常社交情境中，人们常常需要权衡相互冲突的目标——既要传达严酷真相，又要维系信任关系，同时还需顾及他人感受。这种价值权衡是人类决策与语言运用的核心特征，然而目前用于解读 LLMs 中此类动态多元价值概念的工具仍十分有限。认知科学领域提出的"认知模型"通过量化说话者在选择行为或话语时对竞争性效用函数的权重分配，为人类的价值权衡提供了形式化描述。本研究采用前沿的礼貌言语认知模型，系统评估 LLMs 在多大程度上再现了类人的价值权衡机制。我们通过双重维度展开分析：前沿黑盒模型中的推理"努力程度"差异，以及开源模型在强化学习后训练阶段的表现动态。研究发现，推理模型普遍呈现信息效用高于社交效用的特征，这一现象在数学推理能力较强的开源模型中尤为显著。 我们从 LLMs 训练动态中发现，相较于反馈数据集或对齐方法，基础模型和预训练数据的选择在训练初期就会导致效用值发生大幅变化，并产生持续影响。研究表明，我们的方法能灵敏响应快速演变的 LLM 生态系统的多元特性，这些发现有助于：构建其他高层行为的假设框架、优化推理模型的训练方案、在模型训练过程中更精准地调控不同价值间的权衡。

学科分类: 计算与语言 , 人工智能

发布时间: 2025-06-25 17:58:12 UTC

### 创造力

2025-06-26 14:07:46 Thursday ｜ 人类创造力的核心机制，AI已经开始掌握了 | 北大CogSci 2025（Oral）

https://mp.weixin.qq.com/s/ZkkVx3TXUIiwe-XNyyWj3Q

论文地址：https://ppyyqq.github.io/aicc/

这种从 **“识别”→“解释”→“引申”的三层认知过程** ，正是人类创造力的核心机制。而现在，AI也开始掌握这种能力了。

对此，受认知科学家Margaret Boden和认知科学中的“概念融合理论”（Conceptual Blending Theory）的启发，研究团队首次提出了一个面向AI系统的分层评估框架——IEI框架 *（Identification–Explanation–Implication）* 。

该框架将组合创造力分解为三个层次：

* **识别（Identification）** ：能否正确分解输入概念的基本元素；
* **解释（Explanation）** ：能否发现概念间的潜在关联；
* **引申（Implication）** ：能否理解超越原始输入的语义内涵。

## 元学习

#### 使用元学习在维基百科上检测袜子

2025-06-13 18:45:13 Friday｜

Detecting Sockpuppetry on Wikipedia Using Meta-Learning
 **标题** ： 使用元学习在维基百科上检测袜子
 **链接** ：https://arxiv.org/abs/2506.10314

 **作者** ： Luc Raszewski,  Christine De Kock
 **备注** ：Accepted to ACL 2025
 **摘要** ：维基百科上的恶意sockpuppet检测对于保护互联网上可靠信息的访问和防止虚假信息的传播至关重要。以前的机器学习方法依赖于风格和元数据特征，但没有优先考虑对作者特定行为的适应性。因此，他们很难有效地建模特定sockpuppet-group的行为，特别是当文本数据有限时。为了解决这个问题，我们提出了元学习的应用，这是一种机器学习技术，旨在 **通过跨多个任务训练模型来提高数据稀缺环境中的性能** 。元学习优化了快速适应新sockpuppet-group写作风格的模型。我们的研究结果表明，与预先训练的模型相比， **元学习显著提高了预测的精度** ，标志着在开放编辑平台上打击sockpuppetry的进步。我们发布了一个新的sockpuppet调查数据集，以促进sockpuppetry和元学习领域的未来研究。

#### 🌈 75%预训练数据都能删！Jeff Dean新作：全自动筛除低质量数据

 2025-06-19 20:29:54 Thursday ｜

https://mp.weixin.qq.com/s/lMsivtY3aBiDq3eknSVZug

Google DeepMind团队开发的DataRater可以全自动评估数据质量，通过元学习自动筛选有价值的数据，提升模型训练效率。DataRater使用元梯度优化，能有效减少训练计算量，提高模型性能，尤其在低质量数据集上效果显著，且能跨不同模型规模进行泛化。

最近，Google DeepMind的研究人员发布了一个数据质量评估框架DataRater，可用于估计任意数据对最终训练效果的价值，即数据质量。

论文链接：https://arxiv.org/pdf/2505.17895

DataRater的核心思路是使用「元学习」来自动学习筛选或混合数据流的标准，以一种数据驱动的方式，让数据展现出本身的价值。

#### 多一步：超越单次反向传播的元学习模型编辑

[#49](https://arxiv.org/abs/2508.04012)[Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://papers.cool/arxiv/2508.04012)

1. 大型语言模型（LLMs）支撑着许多人工智能应用，但其静态特性使得更新知识代价高昂。模型编辑通过有针对性的参数修改注入新信息，提供了一种高效的替代方案。特别是，基于元学习的模型编辑（MLBME）方法在编辑效果和效率方面表现出显著优势。
   1. 尽管如此，我们发现 MLBME 在低数据场景下表现不佳，其训练效率也受到 KL 散度计算的瓶颈限制。
   2. 为了解决这些问题，我们提出了 S tep M ore Edit （ SMEdit ），这是一种新颖的 MLBME 方法，采用 M ultiple B ackpro P agation S teps（ MBPS ）以提升有限监督下的编辑性能，并在权重更新上引入范数正则化以提高训练效率。在两个数据集和两个 LLMs 上的实验结果表明，SMEdit 优于之前的 MLBME 基线方法，且 MBPS 策略可以无缝集成到现有方法中，进一步提升其性能。我们的代码将很快发布。
2. 2025-08-06 01:54:58 UTC

## **元优化**

研究人员采用元梯度方法，通过随机梯度法来大致优化参数：

计算损失函数（外层损失，outer loss）相对于参数的梯度，其梯度通过反向传播经过多次针对模型参数的优化更新（内层损失，inner loss）来计算。

元学习目标：假设训练数据集和测试数据集具有相同的分布，DataRater算法无需针对某个特定的下游任务进行优化，只需要最大化给定数据集的训练效率。

## 元认知

 **培养「元认知」：** 教模型反思自己的学习过程，比如为什么选择某个答案、如何改进自己的思考方式，帮助模型在AI时代保持主动性。

关键在于引导模型将AI作为辅助，而不是替代。

#### **[思考思考：SAGE-nano 的自我感知语言模型的逆推理](https://papers.cool/arxiv/2507.00092)**

2025-07-02 17:19:17 Wednesday｜

 **Authors** : [Basab Jha](https://arxiv.org/search/?searchtype=author&query=Basab%20Jha), [Firoj Paudel](https://arxiv.org/search/?searchtype=author&query=Firoj%20Paudel), [Ujjwal Puri](https://arxiv.org/search/?searchtype=author&query=Ujjwal%20Puri), [Zhang Yuting](https://arxiv.org/search/?searchtype=author&query=Zhang%20Yuting), [Choi Donghyuk](https://arxiv.org/search/?searchtype=author&query=Choi%20Donghyuk), [Wang Junhao](https://arxiv.org/search/?searchtype=author&query=Wang%20Junhao)

大型语言模型 （LLM） 在使用 Chain-of-Thought （CoT） 提示解决复杂推理任务方面表现出了卓越的能力，但它们的决策过程仍然有些黑匣子。我们介绍了 textbfinverse 推理，这是一种新颖的范式，使 LLM 能够在事后分解和解释自己的推理链。我们的方法用于 SAGE-nano，一个 40 亿个参数的推理模型，它采用元认知结构，通过注意力过程进行反射，以识别主要决策点并生成对推理选择的解释。虽然典型的 CoT 方法针对前向推理生成，但逆向推理提供了关于为什么选择特定推理链而不是其他推理链的见解。通过对 AQUA-RAT、CommonsenseQA 和定制基准测试的逻辑推理谜题、数学问题和道德困境的全面测试，我们证明 SAGE-nano 在推理准确性（AQUA-RAT 上为 74.6%）和解释质量（92.1% 的人类偏好分数）方面都处于领先地位，并提供的性能几乎与 Claude-3.5 Sonnet 或 GPT-4o 等模型相当。我们的贡献是：（i） 第一个通过逆向推理进行 LLM 自我反思的严格框架，（ii） 一个新颖的元学习框架来逆转注意力流，（iii） 推理透明度的综合评估框架，以及 （iv） 证据表明使用逆向推理增加推理可以提高可解释性和推理性能。我们的工作为透明的 AI 系统开辟了新的途径，并缩小了 AI 安全、教育和科学发现方面的重大差距。

 **科目** :  **[人工智能](https://papers.cool/arxiv/cs.AI)** , [计算和语言](https://papers.cool/arxiv/cs.CL), [机器学习](https://papers.cool/arxiv/cs.LG)

 **发布** ： 2025-06-30 09：53：41 UTC

#### 从“顿悟时刻”到可控思维：通过解耦推理与控制迈向大型推理模型中的元认知推理

[#7](https://arxiv.org/abs/2508.04460)[From &#34;Aha Moments&#34; to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://papers.cool/arxiv/2508.04460)

大型推理模型（LRMs）通过自发展现诸如逐步推理、反思和回溯等认知行为，展示了复杂推理的潜在能力，这些行为通常被称为“顿悟时刻”。然而，这种涌现行为仍然缺乏监管和控制，常常导致过度思考，即模型在达到可靠结论后仍继续生成冗余的推理内容。这导致计算成本过高和延迟增加，限制了 LRMs 的实际应用。

1. 根本原因在于缺乏内在的调节机制，当前模型无法监控并自适应管理其推理过程，以决定何时继续、回溯或终止。
2. 为解决这一问题，我们提出了元认知推理框架（MERA），该框架明确将思考过程分解为独立的推理和控制组件，从而实现控制策略的独立优化。 具体来说，MERA 引入了一种基于接管的数据构建机制，该机制在推理过程中识别关键决策点，并将控制信号的生成委托给辅助 LLMs，从而实现高质量推理控制数据的构建。
3. 此外，通过监督微调实现了结构化的推理-控制分离，使模型能够生成明确的推理轨迹并获得初步的元认知控制能力。最后，MERA 采用了控制段策略优化（Control-Segment Policy Optimization，CSPO），该方法结合了分段的组相对策略优化（Group Relative Policy Optimization，GRPO）和控制掩码机制，以优化控制行为的学习，同时最大限度地减少无关内容的干扰。
4. 在多个推理基准上的实验表明，使用 MERA 训练的模型在推理效率和准确性方面均有所提升。

发布时间：2025-08-06 13:59:17 UTC

## 思考SAE

2025-06-23 12:11:35 Monday ｜ 大模型到底是怎么「思考」的？第一篇系统性综述SAE的文章来了 https://mp.weixin.qq.com/s/DNg4O4pirbiT56PP_6VtAQ

一种叫做 Sparse Autoencoder（简称 SAE） 的新兴技术正迅速崛起，成为当前最热门的 mechanistic interpretability（机制可解释性） 路线之一。最近，我们撰写并发布了第一篇系统性的 SAE 综述文章，对该领域的技术、演化和未来挑战做了全面梳理，供关注大模型透明性、可控性和解释性的研究者参考。

论文题目：

A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models

论文地址：

https://arxiv.org/pdf/2503.05613

## 心智理论ToM / 意识

2025-07-01 12:08:41 Tuesday｜

**[代理对代理心智理论：在大型语言模型中测试对话者意识](https://papers.cool/arxiv/2506.22957)** **[PDF()]** **[Copy]** **[Kimi()]** **[REL]**

 **Authors** : [Younwoo Choi](https://arxiv.org/search/?searchtype=author&query=Younwoo%20Choi), [Changling Li](https://arxiv.org/search/?searchtype=author&query=Changling%20Li), [Yongjin Yang](https://arxiv.org/search/?searchtype=author&query=Yongjin%20Yang), [Zhijing Jin](https://arxiv.org/search/?searchtype=author&query=Zhijing%20Jin)

随着大型语言模型 （LLM） 越来越多地集成到多智能体和人类 AI 系统中，了解它们对自我情境和对话伙伴的感知对于确保可靠的性能和强大的安全性至关重要。虽然之前的工作已经广泛研究了态势感知，这是指 LLM 识别其作阶段和限制的能力，但它在很大程度上忽视了识别和适应对话伙伴的身份和特征的补充能力。在本文中，我们将后一种能力正式化为对话者意识，并首次系统地评估了它在当代 LLM 中的出现。我们研究了三个维度的对话者推理——推理模式、语言风格和对齐偏好——并表明 LLM 可靠地识别了同一家庭的同伴和某些突出的模型家庭，例如 GPT 和 Claude。为了证明其实际意义，我们开发了三个案例研究，其中对话者的意识既通过及时适应增强了多 LLM 合作，又引入了新的对齐和安全漏洞，包括奖励黑客行为和越狱易感性增加。我们的研究结果强调了 LLM 中身份敏感行为的双重前景和危险，强调了进一步了解对话者意识和多代理部署中新保障措施的必要性。我们的代码在 https://github.com/younwoochoi/InterlocutorAwarenessLLM 是开源的。

 **科目** :  **[计算和语言](https://papers.cool/arxiv/cs.CL)** , [人工智能](https://papers.cool/arxiv/cs.AI), [计算机与社会](https://papers.cool/arxiv/cs.CY), [多智能体系统](https://papers.cool/arxiv/cs.MA)

2. 2025-07-01 12:07:34 Tuesday ｜

 **[SoMi-ToM： 评估具身社会互动中的多视角心智理论](https://papers.cool/arxiv/2506.23046)** **[PDF()]** **[Copy]** **[Kimi()]** **[REL]**

 **Authors** : [Xianzhe Fan](https://arxiv.org/search/?searchtype=author&query=Xianzhe%20Fan), [Xuhui Zhou](https://arxiv.org/search/?searchtype=author&query=Xuhui%20Zhou), [Chuanyang Jin](https://arxiv.org/search/?searchtype=author&query=Chuanyang%20Jin), [Kolby Nottingham](https://arxiv.org/search/?searchtype=author&query=Kolby%20Nottingham), [Hao Zhu](https://arxiv.org/search/?searchtype=author&query=Hao%20Zhu), [Maarten Sap](https://arxiv.org/search/?searchtype=author&query=Maarten%20Sap)

人类通过在动态的现实世界社交互动中感知周围的环境，不断推断他人的状态、目标和行为。但是，大多数 Theory of Mind （ToM） 基准测试仅评估基于文本的静态场景，与真实交互相比，这些场景存在很大差距。我们提出了 SoMi-ToM 基准，旨在评估具身多智能体复杂社交互动中的多视角 ToM。该基准测试基于交互环境 SoMi 生成的丰富多模态交互数据，涵盖不同的制作目标和社交关系。我们的框架支持多层次评估：（1） 第一人称评估在任务期间提供来自第一人称视角的多模态（视觉、对话、动作等）输入，以进行实时状态推理，（2） 第三人称评估在任务后提供完整的第三人称视角视频和文本记录，用于目标和行为推理。这种评估方法允许从主观即时体验和客观的全局观察中更全面地检查模型的 ToM 能力。我们构建了一个具有挑战性的数据集，其中包含 35 个第三人称视角视频、363 个第一人称视角图像和 1225 个专家注释的多项选择题（三个选项）。在这个数据集上，我们系统地评估了人类受试者和几个最先进的大型视觉语言模型 （LVLM） 的表现。结果表明，LVLM 在 SoMi-ToM 上的表现明显差于人类：人类与模型之间的平均准确率差距在第一人称评估中为 40.1%，在第三人称评估中为 26.4%。这表明未来的 LVLM 需要进一步提高他们在具体、复杂的社交互动中的 ToM 能力。

 **科目** :  **[计算和语言](https://papers.cool/arxiv/cs.CL)** , [人工智能](https://papers.cool/arxiv/cs.AI), [计算机视觉和模式识别](https://papers.cool/arxiv/cs.CV), [机器人](https://papers.cool/arxiv/cs.RO)

 **发布** ： 2025-06-29 00：54：13 UTC

3. 2025-06-26 14:41:04 Thursday ｜

**#37 多智能体推理与心智理论解密基准测试 [PDF** **(3)** **] [复制] [Kimi** **(7)**  **] [相关]** **[#37](https://arxiv.org/abs/2506.20664)****[The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://papers.cool/arxiv/2506.20664)**** [PDF(3)] [Copy] [Kimi(7)] [REL]**

作者：安德烈·卢普、蒂蒙·威利、雅各布·福斯特

随着大型语言模型(LLMs)获得自主行为能力，它们将需要应对复杂的多智能体场景，在合作与竞争环境中与人类用户及其他智能体进行交互。这需要新的推理技能，其中最关键的是心智理论(ToM)——即对其他智能体"心理"状态进行推理的能力。然而目前对 LLMs 的心智理论及其他多智能体能力认知甚少，现有基准测试存在范围狭窄、数据泄露、饱和度高及缺乏交互性等问题。为此我们提出 Decrypto——一个基于游戏的基准测试框架，其设计灵感源自认知科学、计算语用学和多智能体强化学习。该框架在其他所有维度都尽可能简化，消除了常见于其他基准测试的混杂因素。据我们所知，这也是首个支持设计交互式心智理论实验的平台。我们通过对前沿 LLMs 的综合实证评估、鲁棒性研究以及人机对抗实验验证了该基准设计。研究发现，LLMs 的游戏表现落后于人类及简单的词嵌入基线模型。 我们随后在 Decrypto 中复现了两个经典认知科学实验的变体，用以评估三项关键心智理论能力。令人惊讶的是，当前最先进的推理模型在这些任务上的表现竟显著逊色于早期版本。这一发现表明 Decrypto 填补了当前推理与心智理论评估的关键空白，为开发更优秀的人工智能体开辟了新路径。

主题：人工智能、计算与语言、人机交互、多智能体系统

4. 2025-06-27 14:28:17 Friday

**[来自有限视图的空间心智建模](https://papers.cool/arxiv/2506.21458)** **[PDF(6)]** **[Copy]** **[Kimi(2)]** **[REL]**

作者：殷柏乔、王启能、张平月、张建树、王康瑞、王子涵、张洁玉、Keshigeyan Chandrasegaran、刘涵、Ranjay Krishna、谢赛宁、李曼玲、吴佳俊、李飞飞

视觉语言模型 （VLM） 能否像人类一样仅从几个视图中想象整个场景？人类形成空间心智模型，即看不见空间的内部表征，以推理布局、视角和运动。我们新的 MindCube 基准测试在 3,268 张图像中提出了 21,154 个问题，揭示了这一关键差距，即现有 VLM 表现出近乎随机的性能。使用 MindCube，我们系统地评估了 VLM 通过表示位置（认知映射）、方向（换位思考）和动力学（“假设”运动的心理模拟）来构建强大的空间心智模型的能力。然后，我们探索了三种方法来帮助 VLM 近似空间心智模型，包括看不见的中间视图、自然语言推理链和认知地图。显著的改进来自一种协同方法，即 “map-then-reason”，该方法联合训练模型首先生成认知地图，然后对其进行推理。通过训练模型对这些内部映射进行推理，我们将准确率从 37.8% 提高到 60.8% （+23.0%）。添加强化学习将性能进一步推高至 70.7% （+32.9%）。我们的主要见解是，这种空间心智模型的脚手架，积极构建和利用具有灵活推理过程的内部结构化空间表示，显着提高了对不可观察空间的理解。

5. 2025-07-01 12:32:09 Tuesday ｜

**[“意识”可以从大型语言模型 （LLM） 的内部状态中观察到吗？使用综合信息理论和 Span 表示分析剖析从 Theory of Mind 测试中获得的 LLM 表示](https://papers.cool/arxiv/2506.22516)** **[PDF()]** **[Copy]** **[Kimi()]** **[REL]**

 **Author** : [Jingkai Li](https://arxiv.org/search/?searchtype=author&query=Jingkai%20Li)

综合信息论 （IIT） 为解释意识现象提供了一个定量框架，假设有意识系统由通过因果属性整合的元素组成。我们将 IIT 3.0 和 4.0（该框架的最新版本）应用于大型语言模型 （LLM） 表示序列，分析从现有心智理论 （ToM） 测试结果得出的数据。我们的研究系统地调查了 ToM 测试表现的差异，当以 LLM 表示时，是否可以通过 IIT 估计来揭示，即 Φmax （IIT 3.0）、Φ （IIT 4.0）、概念信息 （IIT 3.0） 和Φ-结构 （IIT 4.0）。此外，我们将这些指标与 Span Representations 进行比较，独立于任何意识估计。这项额外的努力旨在区分 LLM 表征空间中潜在的 “意识 ”现象和固有的分离。我们进行了全面的实验，检查了 LLM 转换器层的变化和刺激的语言跨度。我们的结果表明，当代基于 Transformer 的 LLM 表示序列缺乏观察到的 “意识” 现象的统计显着指标，但在 spatio-排列分析。附录和代码可作为补充材料获得，网址为：https://doi.org/10.1016/j.nlp.2025.100163。

6. 2025-07-01 12:35:12 Tuesday ｜

**[心理语言学词特征：一种评估 LLM 与人类一致性的新方法](https://papers.cool/arxiv/2506.22439)** **[PDF()]** **[Copy]** **[Kimi()]** **[REL]**

 **Authors** : [Javier Conde](https://arxiv.org/search/?searchtype=author&query=Javier%20Conde), [Miguel González](https://arxiv.org/search/?searchtype=author&query=Miguel%20Gonz%C3%A1lez), [María Grandury](https://arxiv.org/search/?searchtype=author&query=Mar%C3%ADa%20Grandury), [Gonzalo Martínez](https://arxiv.org/search/?searchtype=author&query=Gonzalo%20Mart%C3%ADnez), [Pedro Reviriego](https://arxiv.org/search/?searchtype=author&query=Pedro%20Reviriego), [Mar Brysbaert](https://arxiv.org/search/?searchtype=author&query=Mar%20Brysbaert)

到目前为止，对 LLM 的评估主要集中在他们执行不同任务的能力，例如推理、问答、释义或翻译。对于大多数这些任务，可以使用客观指标（如正确答案的数量）来衡量性能。然而，其他语言特征并不容易量化。例如，与给定单词相关的唤醒、具体或性别，以及我们用感官体验单词并将它们与特定含义联系起来的程度。心理语言学已经对这些特征进行了多年的研究，对人类进行了大规模实验，以产生数千个单词的评分。这提供了一个机会，可以评估 LLM 与这些单词特征的人类评级的一致性，从而利用涵盖大量单词中许多不同语言特征的现有研究。在本文中，我们评估了一组代表性的 LLM 与两个心理语言数据集上的人类评分的一致性：格拉斯哥和兰开斯特规范。这些数据集涵盖数千个单词的 13 个特征。结果显示，在评估的格拉斯哥规范（唤醒、效价、支配、具体、可成像性、熟悉度和性别）中，对齐效果优于评估的兰开斯特规范（内省、味觉、嗅觉、触觉、听觉和视觉）。这表明当前的 LLM 在与人类对单词的感官联想保持一致方面存在潜在局限性，这可能是由于它们缺乏人类存在的具身认知，并说明了使用心理语言学数据集评估 LLM 的有用性。

 **科目** :  **[计算和语言](https://papers.cool/arxiv/cs.CL)** , [人工智能](https://papers.cool/arxiv/cs.AI)

## 元生成

#### **[不使用 GPU 的 LoRA 微调：适用于 LLM 的 CPU 高效元生成框架](https://papers.cool/arxiv/2507.01806)**

2025-07-03 11:44:30 Thursday｜

 **Authors** : [Reza Arabpour](https://arxiv.org/search/?searchtype=author&query=Reza%20Arabpour), [Haitz Sáez de Ocáriz Borde](https://arxiv.org/search/?searchtype=author&query=Haitz%20S%C3%A1ez%20de%20Oc%C3%A1riz%20Borde), [Anastasis Kratsios](https://arxiv.org/search/?searchtype=author&query=Anastasis%20Kratsios)

低秩适配器 （LoRA） 通过实现参数高效的更新，改变了大型语言模型 （LLM） 的微调。然而，由于依赖基于 GPU 的训练，它们的广泛采用仍然受到限制。在这项工作中，我们提出了一种以理论为基础的 LoRA 微调方法，专为计算资源有限的用户设计，特别是那些仅限于标准笔记本电脑 CPU 的用户。我们的方法学习了一个 **元运算符，该元运算符通过利用 Mistral-7B-Instruct-v0.2 模型的大量预训练适配器，将任何表示为概率分布的输入数据集映射到一组 LoRA 权重** 。我们的管道不是执行新的基于梯度的更新，而是直接在 CPU 上通过现有 LoRA 的轻量级组合来构建适配器。虽然生成的适配器的性能与 GPU 训练的适配器不匹配，但它们在下游任务上始终优于基本 Mistral 模型，为传统的基于 GPU 的微调提供了一种实用且可访问的替代方案。

 **科目** :  **[机器学习](https://papers.cool/arxiv/cs.LG)** , [人工智能](https://papers.cool/arxiv/cs.AI), [计算和语言](https://papers.cool/arxiv/cs.CL), [机器学习](https://papers.cool/arxiv/stat.ML)

 **发布** ： 2025-07-02 15：24：47 UTC

#### [MetaExplainer：为人工智能系统生成多类型、以用户为中心的解释的框架](https://papers.cool/arxiv/2508.00300)

解释对于构建值得信赖的人工智能系统至关重要，但模型提供的解释与用户所需的解释之间往往存在差距。为了解决这一差距，我们推出了 MetaExplainer，这是一个神经符号框架，旨在生成以用户为中心的解释。

我们的方法采用三阶段过程：首先，我们使用最先进的大型语言模型 （LLM） 将用户问题分解为机器可读的格式;其次，我们将生成系统建议的任务委托给模型解释器方法;最后，我们合成自然语言解释来总结解释器输出。在整个过程中，我们利用解释本体来指导语言模型和解释方法。通过利用法学硕士和结构化的解释生成方法，MetaExplainer 旨在增强人工智能系统在各种应用程序中的可解释性和可信度，为用户提供量身定制的、问题驱动的解释，更好地满足他们的需求。对 MetaExplainer 的全面评估表明，在评估和利用当前最先进的解释框架方面迈出了一步。

我们的结果显示，在所有阶段都表现出色，问题重构的 F1 得分为 59.06%，模型解释的忠实度为 70%，自然语言合成的上下文利用率为 67%。用户研究证实了这些发现，强调了生成解释的创造性和全面性。MetaExplainer 在糖尿病 （PIMA Indian） 表格数据集上进行测试，支持多种解释类型，包括对比解释、反事实解释、基本原理解释、基于案例解释和数据解释。该框架的多功能性和可追溯性，从使用本体来指导法学硕士，表明在测试场景之外具有广泛的适用性，将 MetaExplainer 定位为增强各个领域的人工智能可解释性的有前途的工具。
