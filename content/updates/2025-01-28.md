+++
title = '2025-01-28科研追新(公众号)'
date = 2025-01-28T00:00:00+08:00
draft = false
toc = true
+++

# 2025-01-28

统计1-27-11:00～1-28-9:00的相关新进展

## 1 公众号

## 1.1 量子位

1. 有预测表明，2025年是人形机器人量产的开始。
2. 毫无疑问，2025技术侧已经能看见火花的重要趋势，必然有因OpenAI o1掀起的Scaling新范式：**Inference-Scaling**。

   与1年前技术领域普遍信仰的传统Scaling Law不同，Inference-Scaling强调了后训练（post-training）和推理阶段（inference-time）计算投入的重要性，与之对应的结果是模型推理能力的大幅提升。

   这就意味着**大模型资源开始向Post-training和推理算力倾斜**。

   也意味着新一轮竞赛的开始。

   单看国内，短短几个月内已经有昆仑万维Skywork o1、阿里通义QVQ、智谱华章GLM-Zero-Preview、阶跃星辰Step R-mini、深度求索DeepSeek-R1、月之暗面Kimi k1.5、百川智能Baichuan-M1-preview等多个推理模型问世。

   **需要重点关注的是，OpenAI CEO山姆·奥特曼非常明确地对外宣称，“o1只是推理模型的GPT-2时刻”。**
3. 除了DeepSeek之外，阿里云Qwen首次将**开源Qwen模型**的上下文扩展到**1M长度**，发布模型Qwen2.5-7B-Instruct-1M和Qwen2.5-14B-Instruct-1M。 https://qwenlm.github.io/blog/qwen2.5-1m/

   1. 在上下文长度高达 100万Tokens的 “大海捞针” 式任务 ——Passkey Retrieval（密钥检索）中，Qwen2.5-1M系列模型展现出卓越性能，能够精准地从长度为1M的文档里检索出隐藏信息。
   2. 对于更复杂的长上下文理解任务，研究团队选择了RULER、LV-Eval和LongbenchChat等测试集。
   3. ![image-20250128094942050](./picture/image-20250128094942050.png)
   4. 实习细节：

      1. 使用Adjusted Base Frequency的方案，将RoPE基础频率从10,000提高到10,000,000
      2. 引入了Dual Chunk Attention (DCA)，该方法通过将过大的相对位置，重新映射为较小的值，解决基于旋转位置编码的大型语言模型会在长上下文任务中产生性能下降。
      3. 基于**MInference**的稀疏注意力优化。

## 1.2 机器之心

1. 关于deepseek R1的相关复现进展

   1. 𝕏 博主 @Charbax 总结了 DeepSeek 文档中没有介绍的地方以及复现 R1 的一些难点。
      - 训练流程的细节。虽然其技术报告中介绍了强化学习阶段和蒸馏，但省略了关键的实现细节，包括超参数（例如，学习率、批量大小、奖励缩放因子）、用于生成合成训练数据的数据管道（例如，如何编排 800K 蒸馏样本）、需要人类偏好对齐的任务的奖励模型架构（多语言输出的「语言一致性奖励」）。
      - 冷启动数据生成。报告中虽然提到了创建「高质量冷启动数据」（例如，人工标准、少样本提示）的过程，但缺乏具体的示例或数据集。
      - 硬件和基础设施。没有关于计算资源（例如，GPU 集群、训练时间）或软件堆栈优化（例如，DeepSeek-V3 的 AMD ROCM 集成）的详细信息。
      - 复现难题。缺少多阶段强化学习的脚本等组件。
2. 近期，字节智能创作 AR 团队联合豆包大模型团队开发的 Video Depth Anything（VDA） 成功解决单目深度估计模型在视频领域的时间一致性问题。https://mp.weixin.qq.com/s/Pd2UyOiLmy0aBJwaEPEGDQ
3. 南洋理工大学 S-Lab 的研究者们提出了一种全新的框架：**CityDreamer4D**。它突破了现有视频生成的局限，不再简单地「合成画面」，而是**直接建模城市场景背后的运行规律**，从而创造出一个**真正无边界的 4D 世界**。

   1. 论文链接：https://arxiv.org/abs/2501.08983
   2. 项目链接：https://haozhexie.com/project/city-dreamer-4d/
   3. GitHub链接：https://github.com/hzxie/CityDreamer4D
4. 通义实验室WebWalker通过构造智能体完成复杂信息检索任务。

   1. 宣传链接：https://mp.weixin.qq.com/s/Y-yGC6sobN0WITMayKcfzg
   2. 论文标题：WebWalker: Benchmarking LLMs in Web Traversal

      论文地址：https://arxiv.org/pdf/2501.07572

      Homepage 地址：https://alibaba-nlp.github.io/WebWalker/

      Modelscope Demo 地址: https://www.modelscope.cn/studios/jialongwu/WebWalker

      Huggingface Demo 地址: https://huggingface.co/spaces/callanwu/WebWalker

      Dataset 地址: https://huggingface.co/datasets/callanwu/WebWalkerQA

      Leaderboard 地址: https://huggingface.co/spaces/callanwu/WebWalkerQALeadeboard

      Github 地址: https://github.com/Alibaba-NLP/WebWalker
   3. WebWalkerQA 专门设计来评估大模型处理复杂、多步骤网页交互中嵌入查询能力的基准测试。其聚焦于文本推理能力，采用问答格式来评估大模型在网页场景中的问题解决能力，并且将动作限制为「Click 点击」，以更精准地评估智能体的导航和信息寻求能力，这种范式更加贴合实际应用场景。
   4. WebWalkerQA 通过两阶段漏斗式标注策略构建数据，先用 GPT-4o 进行初步标注，再由众包标注者进行质量控制和筛选。

## 2 Paper

## 2.1 Arxiv

### 2.1.1 计算和语言

#### 1. 方向1: 数据

1. 可比语料库：新研究方向的机会 https://arxiv.org/abs/2501.14721 ： 简单汇总了包括cc在内的语料库的发展和未来方向
2. WanJuanSiLu：适用于低资源语言的高质量开源 Webtext 数据集 https://arxiv.org/abs/2501.14506：本文介绍了开源数据集 WanJuanSiLu，旨在为低资源语言提供高质量的训练语料，从而推进多语言模型的研发。为了实现这一目标，我们开发了一个为低资源语言量身定制的系统性数据处理框架。该框架包括数据提取、语料库清理、内容重复数据删除、安全过滤、质量评估和主题分类等关键阶段。通过实施此框架，我们显著提高了数据集的质量和安全性，同时保持了其语言多样性。截至目前，所有五种语言的数据都已完全开源。数据集可访问，GitHub 存储库可获取【checking】

#### 2. 方向2: 传统NLP任务的泛化

1. 研究自然到正式语言转换中大型语言模型的（反）组合能力 https://arxiv.org/abs/2501.14649： 形式语言中的组合功能在LLM上的测试与泛化
