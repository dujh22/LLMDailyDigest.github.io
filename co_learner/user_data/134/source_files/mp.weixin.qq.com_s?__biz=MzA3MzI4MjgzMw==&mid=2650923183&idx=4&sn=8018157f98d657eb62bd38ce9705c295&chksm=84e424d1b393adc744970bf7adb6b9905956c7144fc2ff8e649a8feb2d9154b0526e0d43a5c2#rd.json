{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650923183&idx=4&sn=8018157f98d657eb62bd38ce9705c295&chksm=84e424d1b393adc744970bf7adb6b9905956c7144fc2ff8e649a8feb2d9154b0526e0d43a5c2#rd",
    "title": "字节豆包全新图像Tokenizer：生成图像最低只需32个token，最高提速410倍",
    "summary": "字节跳动豆包大模型团队和慕尼黑工业大学合作提出了一种新的1D图像Tokenizer——TiTok，用于生成式模型。TiTok打破了传统2D Tokenizer的限制，能将图像压缩成更紧凑的Token序列，减少图像编码的冗余信息。对于256x256分辨率的图片，TiTok仅需32个Token，远少于常规2D Tokenizer的256或1024个Token。在512x512分辨率下，TiTok仅需64个Token，比Stable Diffusion的VAE Tokenizer减少64倍。在ImageNet图像生成任务上，使用TiTok的生成器在质量和速度上均有所提升，如在512分辨率下，TiTok的FID得分优于DiT且生成速度提高了410倍。实验表明，TiTok在减少Token数量的同时，保持了高质量图像生成和更快的推理速度。",
    "user_summary": "字节跳动豆包大模型团队和慕尼黑工业大学合作提出了一种新的1D图像Tokenizer——TiTok，用于生成式模型。TiTok打破了传统2D Tokenizer的限制，能将图像压缩成更紧凑的Token序列，减少图像编码的冗余信息。对于256x256分辨率的图片，TiTok仅需32个Token，远少于常规2D Tokenizer的256或1024个Token。在512x512分辨率下，TiTok仅需64个Token，比Stable Diffusion的VAE Tokenizer减少64倍。在ImageNet图像生成任务上，使用TiTok的生成器在质量和速度上均有所提升，如在512分辨率下，TiTok的FID得分优于DiT且生成速度提高了410倍。实验表明，TiTok在减少Token数量的同时，保持了高质量图像生成和更快的推理速度。",
    "keywords": [
        "字节跳动",
        "豆包大模型团队",
        "慕尼黑工业大学",
        "TiTok",
        "生成式模型",
        "1D",
        "图像",
        "Tokenizer",
        "2D",
        "Tokenizer",
        "图像压缩",
        "Token",
        "序列",
        "冗余信息",
        "256x256",
        "32个Token",
        "512x512",
        "64个Token",
        "Stable",
        "Diffusion",
        "VAE",
        "Tokenizer",
        "FID得分",
        "生成速度",
        "ImageNet",
        "图像生成",
        "512分辨率",
        "DiT",
        "推理速度",
        "高质量图像"
    ],
    "timestamp": "2024-10-27T07:30:50.329281"
}