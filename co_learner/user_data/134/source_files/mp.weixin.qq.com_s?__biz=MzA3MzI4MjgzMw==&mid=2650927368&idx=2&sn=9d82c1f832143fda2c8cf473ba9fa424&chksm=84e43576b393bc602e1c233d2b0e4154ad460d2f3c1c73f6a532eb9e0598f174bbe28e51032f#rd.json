{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650927368&idx=2&sn=9d82c1f832143fda2c8cf473ba9fa424&chksm=84e43576b393bc602e1c233d2b0e4154ad460d2f3c1c73f6a532eb9e0598f174bbe28e51032f#rd",
    "title": "击败GPT-4o的开源模型如何炼成？关于Llama 3.1 405B，Meta都写在这篇论文里了",
    "summary": "Llama 3.1，一个由Meta官方发布的大型语言模型，已正式推出，其上下文长度扩展到了128K，有8B、70B和405B三个版本。这个405B版本的模型在多任务基准测试中的性能可与最好的闭源模型相媲美。Meta在预训练数据的质量和规模上进行了改进，使用了约15万亿多语言Token的语料库对Llama 3进行预训练。该模型的旗舰版本在3.8 × 10²⁵次浮点运算上进行了预训练，比Llama 2的最大版本增加了近50倍。为了支持大规模推理，405B模型通过量化技术降低了计算要求。在后训练阶段，使用旗舰模型提升了较小模型的质量。Meta还开发了模型的多模态扩展，但尚未发布。论文中还提到了训练基础设施的优化和模型开发过程中的可扩展性考虑。此外，Meta更新了许可证，允许开发者使用Llama模型的输出来增强其他模型。Llama 3.1的发布伴随着一个由多个技术伙伴支持的生态系统。",
    "user_summary": "Llama 3.1，一个由Meta官方发布的大型语言模型，已正式推出，其上下文长度扩展到了128K，有8B、70B和405B三个版本。这个405B版本的模型在多任务基准测试中的性能可与最好的闭源模型相媲美。Meta在预训练数据的质量和规模上进行了改进，使用了约15万亿多语言Token的语料库对Llama 3进行预训练。该模型的旗舰版本在3.8 × 10²⁵次浮点运算上进行了预训练，比Llama 2的最大版本增加了近50倍。为了支持大规模推理，405B模型通过量化技术降低了计算要求。在后训练阶段，使用旗舰模型提升了较小模型的质量。Meta还开发了模型的多模态扩展，但尚未发布。论文中还提到了训练基础设施的优化和模型开发过程中的可扩展性考虑。此外，Meta更新了许可证，允许开发者使用Llama模型的输出来增强其他模型。Llama 3.1的发布伴随着一个由多个技术伙伴支持的生态系统。",
    "keywords": [
        "Llama",
        "3.1",
        "Meta",
        "语言模型",
        "上下文长度",
        "128K",
        "8B",
        "70B",
        "405B",
        "多任务基准测试",
        "闭源模型",
        "预训练",
        "数据",
        "质量",
        "规模",
        "15万亿",
        "Token",
        "语料库",
        "3.8",
        "×",
        "10²⁵次",
        "浮点运算",
        "量化技术",
        "推理",
        "小型模型",
        "质量",
        "多模态",
        "扩展",
        "训练基础设施",
        "可扩展性",
        "许可证",
        "开发者",
        "输出",
        "增强",
        "模型",
        "生态系统",
        "技术伙伴",
        "支持"
    ],
    "timestamp": "2024-10-27T07:23:47.132139"
}