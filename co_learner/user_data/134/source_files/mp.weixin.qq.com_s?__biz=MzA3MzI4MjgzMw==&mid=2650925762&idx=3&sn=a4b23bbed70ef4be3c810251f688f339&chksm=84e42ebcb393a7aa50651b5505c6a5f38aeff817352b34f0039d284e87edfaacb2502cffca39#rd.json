{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650925762&idx=3&sn=a4b23bbed70ef4be3c810251f688f339&chksm=84e42ebcb393a7aa50651b5505c6a5f38aeff817352b34f0039d284e87edfaacb2502cffca39#rd",
    "title": "这些VLM竟都是盲人？GPT-4o、Sonnet-3.5相继败于「视力」测试",
    "summary": "研究人员发现，尽管视觉语言模型（VLMs）在某些任务上表现出色，但它们在处理一些基本的视觉问题时表现不佳，例如数线条交点、判断图形位置关系或识别被圈出的字母。这些模型在处理需要精细视觉判断的任务时显得“近视”，细节在它们看来是模糊的。研究人员设计了一套“视力测试”来评估VLMs，结果表明，即使是最先进的模型，如GPT-4o、Gemini-1.5、Sonnet-3和Sonnet-3.5，也在这些简单的视觉任务中表现得并不理想。这表明，尽管VLMs在某些方面显示出智能，但它们的视觉处理能力仍有限，可能更多地依赖于记忆和模式识别，而不是真正的人类般的视觉感知。",
    "user_summary": "研究人员发现，尽管视觉语言模型（VLMs）在某些任务上表现出色，但它们在处理一些基本的视觉问题时表现不佳，例如数线条交点、判断图形位置关系或识别被圈出的字母。这些模型在处理需要精细视觉判断的任务时显得“近视”，细节在它们看来是模糊的。研究人员设计了一套“视力测试”来评估VLMs，结果表明，即使是最先进的模型，如GPT-4o、Gemini-1.5、Sonnet-3和Sonnet-3.5，也在这些简单的视觉任务中表现得并不理想。这表明，尽管VLMs在某些方面显示出智能，但它们的视觉处理能力仍有限，可能更多地依赖于记忆和模式识别，而不是真正的人类般的视觉感知。",
    "keywords": [
        "视觉语言模型",
        "VLMs",
        "视觉问题",
        "线条交点",
        "图形位置",
        "关系",
        "识别字母",
        "近视",
        "细节",
        "模糊",
        "视力测试",
        "GPT-4o",
        "Gemini-1.5",
        "Sonnet-3",
        "Sonnet-3.5",
        "智能",
        "视觉处理能力",
        "记忆",
        "模式识别",
        "人类",
        "视觉感知"
    ],
    "timestamp": "2024-10-27T07:26:43.012162"
}