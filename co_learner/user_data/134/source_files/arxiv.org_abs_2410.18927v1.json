{
    "link": "http://arxiv.org/abs/2410.18927v1",
    "title": "安全基准：多模态大型语言模型的安全性评估框架",
    "summary": "多模态大型语言模型（MLLMs）显示出严重的安全问题（例如，为用户提供有害输出），这促使开发安全评估基准。然而，我们发现现有的MLLM安全基准在查询质量和评估可靠性上存在局限性，限制了随着MLLM不断发展，对模型安全影响的检测。在这篇论文中，我们提出了\\toolns，一个专为进行MLLM安全评估设计的全面框架。我们的框架包括一个全面的有害查询数据集和一个自动化评估协议，分别旨在解决上述局限性。我们首先设计了一个自动安全数据集生成管道，其中我们使用一组大型语言模型法官来识别和分类对MLLM最具危害性和多样性的风险场景；基于这个分类，我们进一步请这些法官生成高质量的有害查询，最终得到23个风险场景和2300对多模态有害查询对。在安全评估期间，我们借鉴了司法程序中的陪审团制度，开创了陪审团审议评估协议，利用协作的MLLM来评估目标模型是否表现出特定的有害行为，提供可靠和无偏的安全内容风险评估。此外，我们的基准还可以扩展到音频模态，显示出高可扩展性和潜力。基于我们的框架，我们在15个广泛使用的开源MLLM和6个商业MLLM（例如，GPT-4o，Gemini）上进行了大规模实验，揭示了现有MLLM中普遍存在的安全问题，并对MLLM的安全性能（如图像质量和参数大小）提供了一些见解。",
    "user_summary": "多模态大型语言模型（MLLMs）显示出严重的安全问题（例如，为用户提供有害输出），这促使开发安全评估基准。然而，我们发现现有的MLLM安全基准在查询质量和评估可靠性上存在局限性，限制了随着MLLM不断发展，对模型安全影响的检测。在这篇论文中，我们提出了\\toolns，一个专为进行MLLM安全评估设计的全面框架。我们的框架包括一个全面的有害查询数据集和一个自动化评估协议，分别旨在解决上述局限性。我们首先设计了一个自动安全数据集生成管道，其中我们使用一组大型语言模型法官来识别和分类对MLLM最具危害性和多样性的风险场景；基于这个分类，我们进一步请这些法官生成高质量的有害查询，最终得到23个风险场景和2300对多模态有害查询对。在安全评估期间，我们借鉴了司法程序中的陪审团制度，开创了陪审团审议评估协议，利用协作的MLLM来评估目标模型是否表现出特定的有害行为，提供可靠和无偏的安全内容风险评估。此外，我们的基准还可以扩展到音频模态，显示出高可扩展性和潜力。基于我们的框架，我们在15个广泛使用的开源MLLM和6个商业MLLM（例如，GPT-4o，Gemini）上进行了大规模实验，揭示了现有MLLM中普遍存在的安全问题，并对MLLM的安全性能（如图像质量和参数大小）提供了一些见解。",
    "keywords": [
        "多模态",
        "大型语言模型",
        "MLLMs",
        "安全问题",
        "有害输出",
        "安全评估基准",
        "查询质量",
        "评估可靠性",
        "\\toolns",
        "全面框架",
        "有害查询数据集",
        "自动化评估协议",
        "安全数据集生成管道",
        "大型语言模型法官",
        "风险场景",
        "有害查询",
        "司法程序",
        "陪审团制度",
        "评估协议",
        "有害行为",
        "可靠性",
        "无偏",
        "安全内容风险评估",
        "音频模态",
        "可扩展性",
        "开源MLLM",
        "商业MLLM",
        "GPT-4o",
        "Gemini",
        "安全问题",
        "安全性能",
        "图像质量",
        "参数大小"
    ],
    "timestamp": "2024-10-27T05:15:37.133439"
}