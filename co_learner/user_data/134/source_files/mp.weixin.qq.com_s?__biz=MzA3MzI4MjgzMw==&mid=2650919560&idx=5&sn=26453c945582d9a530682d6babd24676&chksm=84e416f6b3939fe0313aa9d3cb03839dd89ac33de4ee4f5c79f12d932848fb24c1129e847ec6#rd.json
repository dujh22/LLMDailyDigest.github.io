{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650919560&idx=5&sn=26453c945582d9a530682d6babd24676&chksm=84e416f6b3939fe0313aa9d3cb03839dd89ac33de4ee4f5c79f12d932848fb24c1129e847ec6#rd",
    "title": "ACL 2024 | 提升大模型持续学习性能，哈工大、度小满提出共享注意力框架SAPT",
    "summary": "哈尔滨工业大学和度小满合作的研究团队提出了SAPT，一个共享注意力持续学习框架，用于解决大语言模型的灾难性遗忘和知识迁移问题。SAPT在ACL 2024上被接收，其设计包含共享注意力学习与选择模块（SALS）和注意力反思模块（ARM）。SALS通过共享注意力操作对学习和选择模块进行对齐，而ARM则通过生成伪样本帮助模型回忆旧任务的注意力权重，防止遗忘。实验结果显示，SAPT在处理灾难性遗忘和知识迁移方面表现出色，适用于不同规模和架构的模型。该方法未来将应用于度小满的轩辕大模型。",
    "user_summary": "哈尔滨工业大学和度小满合作的研究团队提出了SAPT，一个共享注意力持续学习框架，用于解决大语言模型的灾难性遗忘和知识迁移问题。SAPT在ACL 2024上被接收，其设计包含共享注意力学习与选择模块（SALS）和注意力反思模块（ARM）。SALS通过共享注意力操作对学习和选择模块进行对齐，而ARM则通过生成伪样本帮助模型回忆旧任务的注意力权重，防止遗忘。实验结果显示，SAPT在处理灾难性遗忘和知识迁移方面表现出色，适用于不同规模和架构的模型。该方法未来将应用于度小满的轩辕大模型。",
    "keywords": [
        "哈尔滨工业大学",
        "度小满",
        "SAPT",
        "大语言模型",
        "灾难性遗忘",
        "知识迁移",
        "ACL",
        "2024",
        "共享注意力持续学习框架",
        "共享注意力学习",
        "选择模块",
        "注意力反思模块",
        "SALS",
        "ARM",
        "伪样本",
        "注意力权重",
        "遗忘",
        "实验结果",
        "轩辕大模型"
    ],
    "timestamp": "2024-10-27T07:36:56.866741"
}