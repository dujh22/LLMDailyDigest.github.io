{
    "link": "http://arxiv.org/abs/2410.18966v1",
    "title": "数据污染检测对大型语言模型是否有效（效果良好）？一项关于检测假设的调查和评估",
    "summary": "大型语言模型（LLMs）在各种基准测试中表现出色，显示出作为通用任务解决者的潜力。然而，由于LLMs通常在大量数据上进行训练，因此在评估它们时，一个主要的担忧是数据污染，即训练数据和评估数据集之间的重叠会夸大性能评估。虽然已经提出多种方法来识别数据污染，但这些方法依赖于可能在不同环境中不普遍适用的特定假设。为了填补这一空白，我们系统性回顾了47篇关于数据污染检测的论文，对基础假设进行了分类，并评估了这些假设是否得到了严格的验证。我们识别并分析了八类假设，并对其中的三种进行了案例研究。我们的分析表明，当用于预训练LLMs的实例分类时，基于这三种假设的检测方法的表现接近随机猜测，这表明当前的LLMs学习的是数据分布，而不是记忆单个实例。总的来说，这项工作强调了方法需要清楚地陈述其基础假设并跨多种场景测试其有效性的 importance。",
    "user_summary": "大型语言模型（LLMs）在各种基准测试中表现出色，显示出作为通用任务解决者的潜力。然而，由于LLMs通常在大量数据上进行训练，因此在评估它们时，一个主要的担忧是数据污染，即训练数据和评估数据集之间的重叠会夸大性能评估。虽然已经提出多种方法来识别数据污染，但这些方法依赖于可能在不同环境中不普遍适用的特定假设。为了填补这一空白，我们系统性回顾了47篇关于数据污染检测的论文，对基础假设进行了分类，并评估了这些假设是否得到了严格的验证。我们识别并分析了八类假设，并对其中的三种进行了案例研究。我们的分析表明，当用于预训练LLMs的实例分类时，基于这三种假设的检测方法的表现接近随机猜测，这表明当前的LLMs学习的是数据分布，而不是记忆单个实例。总的来说，这项工作强调了方法需要清楚地陈述其基础假设并跨多种场景测试其有效性的 importance。",
    "keywords": [
        "大型语言模型",
        "LLMs",
        "数据污染",
        "基准测试",
        "通用任务解决者",
        "训练数据",
        "评估数据集",
        "重叠",
        "性能评估",
        "数据污染",
        "检测",
        "方法",
        "假设",
        "环境",
        "检测方法",
        "预训练",
        "实例",
        "分类",
        "随机猜测",
        "数据分布",
        "记忆",
        "单个实例",
        "方法",
        "基础假设",
        "测试",
        "场景"
    ],
    "timestamp": "2024-10-27T05:14:08.569976"
}