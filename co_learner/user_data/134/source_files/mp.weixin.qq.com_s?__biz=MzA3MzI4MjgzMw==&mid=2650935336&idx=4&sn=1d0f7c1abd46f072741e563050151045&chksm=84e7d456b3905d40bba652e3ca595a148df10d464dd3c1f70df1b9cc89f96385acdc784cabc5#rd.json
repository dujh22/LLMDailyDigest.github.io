{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650935336&idx=4&sn=1d0f7c1abd46f072741e563050151045&chksm=84e7d456b3905d40bba652e3ca595a148df10d464dd3c1f70df1b9cc89f96385acdc784cabc5#rd",
    "title": "首个Mamba+Transformer混合架构多模态大模型来了，实现单卡千图推理",
    "summary": "这篇文章介绍了香港中文大学深圳和深圳大数据研究院的研究团队提出的一种新的多模态大语言模型，称为LongLLaVA。这个模型旨在扩展大语言模型处理多图像和长上下文的能力，对于视频理解、高分辨率图像处理和多模态智能体有重要意义。研究团队采用混合架构，结合了Mamba和Transformer，通过2D池化减少计算成本，同时设计了考虑时间和空间依赖性的数据构建方法，并实施了渐进式训练策略。LongLLaVA在多个基准测试中展现出强劲的性能，可以处理大量图像，同时保持高吞吐量和低显存消耗。该模型的开源将促进相关领域的进一步研究和应用。",
    "user_summary": "这篇文章介绍了香港中文大学深圳和深圳大数据研究院的研究团队提出的一种新的多模态大语言模型，称为LongLLaVA。这个模型旨在扩展大语言模型处理多图像和长上下文的能力，对于视频理解、高分辨率图像处理和多模态智能体有重要意义。研究团队采用混合架构，结合了Mamba和Transformer，通过2D池化减少计算成本，同时设计了考虑时间和空间依赖性的数据构建方法，并实施了渐进式训练策略。LongLLaVA在多个基准测试中展现出强劲的性能，可以处理大量图像，同时保持高吞吐量和低显存消耗。该模型的开源将促进相关领域的进一步研究和应用。",
    "keywords": [
        "香港中文大学深圳",
        "大数据研究院",
        "LongLLaVA",
        "多模态",
        "大语言模型",
        "Mamba",
        "Transformer",
        "2D池化",
        "计算成本",
        "时间空间依赖性",
        "渐进式训练策略",
        "基准测试",
        "高性能",
        "大量图像",
        "吞吐量",
        "显存消耗",
        "开源",
        "研究",
        "应用"
    ],
    "timestamp": "2024-10-27T07:12:00.746649"
}