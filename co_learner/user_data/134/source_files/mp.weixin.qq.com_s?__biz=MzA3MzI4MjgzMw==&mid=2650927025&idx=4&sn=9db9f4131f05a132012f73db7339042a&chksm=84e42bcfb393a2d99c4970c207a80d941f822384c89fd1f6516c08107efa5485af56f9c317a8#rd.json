{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650927025&idx=4&sn=9db9f4131f05a132012f73db7339042a&chksm=84e42bcfb393a2d99c4970c207a80d941f822384c89fd1f6516c08107efa5485af56f9c317a8#rd",
    "title": "ICML 2024 Oral | DPO是否比PPO更适合LLM，清华吴翼团队最新揭秘",
    "summary": "清华大学交叉信息院助理教授吴翼的研究团队在强化学习和大模型对齐技术方面取得新进展。他们发表在ICML 2024的工作探讨了DPO（Direct Policy Optimization）和PPO（Proximal Policy Optimization）算法在大模型对齐上的效果。研究发现，通过优化PPO算法，他们在代码生成任务上超越了闭源模型AlphaCode 41B。团队还开发了大规模RLHF训练系统ReaLHF，实现高效的大语言模型对齐训练。此外，团队还展示了在MiniRTS、狼人杀和Overcooked等游戏中，通过强化学习训练出的复杂语言智能体和合作代理。论文中总结了提升DPO和PPO性能的关键技术，并强调了高效率训练系统在大模型对齐中的重要性。相关成果将在ICML 2024上进行口头报告。",
    "user_summary": "清华大学交叉信息院助理教授吴翼的研究团队在强化学习和大模型对齐技术方面取得新进展。他们发表在ICML 2024的工作探讨了DPO（Direct Policy Optimization）和PPO（Proximal Policy Optimization）算法在大模型对齐上的效果。研究发现，通过优化PPO算法，他们在代码生成任务上超越了闭源模型AlphaCode 41B。团队还开发了大规模RLHF训练系统ReaLHF，实现高效的大语言模型对齐训练。此外，团队还展示了在MiniRTS、狼人杀和Overcooked等游戏中，通过强化学习训练出的复杂语言智能体和合作代理。论文中总结了提升DPO和PPO性能的关键技术，并强调了高效率训练系统在大模型对齐中的重要性。相关成果将在ICML 2024上进行口头报告。",
    "keywords": [
        "清华大学交叉信息院",
        "吴翼",
        "研究团队",
        "强化学习",
        "大模型",
        "对齐技术",
        "DPO",
        "Direct",
        "Policy",
        "Optimization",
        "PPO",
        "Proximal",
        "Policy",
        "Optimization",
        "AlphaCode",
        "41B",
        "代码生成",
        "RLHF",
        "ReaLHF",
        "语言模型",
        "对齐训练",
        "MiniRTS",
        "狼人杀",
        "Overcooked",
        "强化学习",
        "训练系统",
        "性能",
        "提升",
        "关键技术"
    ],
    "timestamp": "2024-10-27T07:24:31.538431"
}