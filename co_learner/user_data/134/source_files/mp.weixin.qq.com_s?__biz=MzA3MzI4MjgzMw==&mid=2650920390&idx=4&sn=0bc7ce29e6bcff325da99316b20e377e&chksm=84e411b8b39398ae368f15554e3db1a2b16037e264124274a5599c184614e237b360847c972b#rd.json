{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650920390&idx=4&sn=0bc7ce29e6bcff325da99316b20e377e&chksm=84e411b8b39398ae368f15554e3db1a2b16037e264124274a5599c184614e237b360847c972b#rd",
    "title": "单个4090可推理，2000亿稀疏大模型「天工MoE」开源",
    "summary": "昆仑万维宣布开源2千亿稀疏大模型Skywork-MoE，该模型基于开源的Skywork-13B扩展而来，应用了MoE Upcycling技术，能够在单台4090服务器上进行推理，降低了推理成本。Skywork-MoE是首个支持这种推理的开源千亿MoE大模型，其模型权重、技术报告完全开源，免费用于商业。该模型总参数量为146B，激活参数量22B，具有16个Expert。在20B的激活参数量下，其性能接近70B的密集型模型，推理成本下降约3倍。模型还引入了Gating Logits归一化操作和自适应的Aux Loss以改进训练和泛化性能。此外，Skywork-MoE提出Expert Data Parallel和非均匀切分流水并行等并行优化设计，以提高大规模分布式训练的效率。",
    "user_summary": "昆仑万维宣布开源2千亿稀疏大模型Skywork-MoE，该模型基于开源的Skywork-13B扩展而来，应用了MoE Upcycling技术，能够在单台4090服务器上进行推理，降低了推理成本。Skywork-MoE是首个支持这种推理的开源千亿MoE大模型，其模型权重、技术报告完全开源，免费用于商业。该模型总参数量为146B，激活参数量22B，具有16个Expert。在20B的激活参数量下，其性能接近70B的密集型模型，推理成本下降约3倍。模型还引入了Gating Logits归一化操作和自适应的Aux Loss以改进训练和泛化性能。此外，Skywork-MoE提出Expert Data Parallel和非均匀切分流水并行等并行优化设计，以提高大规模分布式训练的效率。",
    "keywords": [
        "昆仑万维",
        "Skywork-MoE",
        "MoE",
        "Upcycling",
        "千亿大模型",
        "4090服务器",
        "推理成本",
        "Skywork-13B",
        "开源",
        "模型权重",
        "技术报告",
        "商业参数量",
        "146B",
        "激活参数量",
        "22B",
        "Expert",
        "20B",
        "70B",
        "密集型模型",
        "推理成本",
        "Gating",
        "Logits",
        "归一化操作",
        "自适应",
        "Aux",
        "Loss",
        "训练",
        "泛化性能",
        "Expert",
        "Data",
        "Parallel",
        "非均匀切分流水并行",
        "分布式训练",
        "效率"
    ],
    "timestamp": "2024-10-27T07:35:09.873986"
}