{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650934126&idx=2&sn=6a03156a218ba6745a0e6bb88155bb31&chksm=84e7cf10b3904606ec423e6d71d7112a88b897f44f849315e6d9ec2b0b46e1a3540a561440cf#rd",
    "title": "音频驱动人像视频模型：字节Loopy、CyberHost研究成果揭秘",
    "summary": "字节跳动的视频生成模型Loopy近日引发关注，该模型能通过一张图片和音频生成逼真的肖像视频，准确实现音频和口型同步，还能生成细腻的表情动作。Loopy采用Diffusion视频生成框架，通过inter/intra-clip temporal layers模块捕捉人物运动信息，并通过Temporal Segment Module学习长时运动信息。此外，audio to latents（A2L）模块增强了音频与头部运动的关联，帮助生成生动的表情。Loopy在不同场景下与近期方法对比表现出优势。团队还推出了CyberHost模型，能纯音频驱动半身视频生成，包括同步的手部动作，这在技术上是一个突破。",
    "user_summary": "字节跳动的视频生成模型Loopy近日引发关注，该模型能通过一张图片和音频生成逼真的肖像视频，准确实现音频和口型同步，还能生成细腻的表情动作。Loopy采用Diffusion视频生成框架，通过inter/intra-clip temporal layers模块捕捉人物运动信息，并通过Temporal Segment Module学习长时运动信息。此外，audio to latents（A2L）模块增强了音频与头部运动的关联，帮助生成生动的表情。Loopy在不同场景下与近期方法对比表现出优势。团队还推出了CyberHost模型，能纯音频驱动半身视频生成，包括同步的手部动作，这在技术上是一个突破。",
    "keywords": [
        "字节跳动",
        "Loopy",
        "视频生成",
        "模型",
        "图片",
        "音频",
        "生成",
        "逼真",
        "肖像视频",
        "口型同步",
        "表情动作",
        "Diffusion",
        "视频生成框架",
        "inter/intra-clip",
        "temporal",
        "layers",
        "模块",
        "人物运动信息",
        "Temporal",
        "Segment",
        "Module",
        "长时运动信息",
        "audio",
        "to",
        "latents",
        "A2L",
        "模块",
        "音频",
        "头部运动",
        "关联",
        "表情",
        "生动",
        "场景",
        "对比",
        "优势",
        "CyberHost",
        "模型",
        "半身视频生成",
        "手部动作",
        "同步",
        "技术",
        "突破"
    ],
    "timestamp": "2024-10-27T07:13:31.169105"
}