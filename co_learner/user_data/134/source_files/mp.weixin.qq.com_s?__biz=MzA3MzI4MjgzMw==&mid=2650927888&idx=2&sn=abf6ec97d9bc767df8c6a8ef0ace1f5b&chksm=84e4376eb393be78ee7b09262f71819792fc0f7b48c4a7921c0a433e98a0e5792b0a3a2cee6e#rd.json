{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650927888&idx=2&sn=abf6ec97d9bc767df8c6a8ef0ace1f5b&chksm=84e4376eb393be78ee7b09262f71819792fc0f7b48c4a7921c0a433e98a0e5792b0a3a2cee6e#rd",
    "title": "1890美元，就能从头训练一个还不错的12亿参数扩散模型",
    "summary": "Sony AI等机构的研究者以1890美元的低成本，使用3700万张图像训练了一个11.6亿参数的稀疏Transformer扩散模型，大大降低了训练大规模视觉生成模型的经济和计算门槛。他们开发了一种延迟掩蔽策略，通过在输入层处理后掩蔽部分patch，允许未掩蔽的patch保留语义信息，从而在高掩蔽率下有效训练模型。这种方法比缩小模型规模更有效，并结合了Transformer的最新进展，如逐层缩放和稀疏Transformer，以提高性能。在COCO数据集上的零样本生成中，模型达到12.7 FID，成本仅为最先进的stable diffusion模型的1/118。",
    "user_summary": "Sony AI等机构的研究者以1890美元的低成本，使用3700万张图像训练了一个11.6亿参数的稀疏Transformer扩散模型，大大降低了训练大规模视觉生成模型的经济和计算门槛。他们开发了一种延迟掩蔽策略，通过在输入层处理后掩蔽部分patch，允许未掩蔽的patch保留语义信息，从而在高掩蔽率下有效训练模型。这种方法比缩小模型规模更有效，并结合了Transformer的最新进展，如逐层缩放和稀疏Transformer，以提高性能。在COCO数据集上的零样本生成中，模型达到12.7 FID，成本仅为最先进的stable diffusion模型的1/118。",
    "keywords": [
        "Sony",
        "AI",
        "研究者",
        "低成本",
        "3700万张图像",
        "训练",
        "11.6亿参数",
        "稀疏",
        "Transformer",
        "扩散模型",
        "大规模",
        "视觉",
        "生成",
        "模型",
        "训练",
        "经济",
        "计算",
        "门槛",
        "延迟",
        "掩蔽策略",
        "输入层",
        "处理",
        "掩蔽",
        "patch",
        "保留",
        "语义信息",
        "高",
        "掩蔽率",
        "训练",
        "模型",
        "有效",
        "缩小",
        "模型",
        "规模",
        "Transformer",
        "最新进展",
        "逐层",
        "缩放",
        "稀疏",
        "Transformer",
        "提高",
        "性能",
        "COCO",
        "数据集",
        "零样本",
        "生成",
        "12.7",
        "FID",
        "成本",
        "稳定",
        "扩散",
        "模型",
        "1/118"
    ],
    "timestamp": "2024-10-27T07:22:45.116617"
}