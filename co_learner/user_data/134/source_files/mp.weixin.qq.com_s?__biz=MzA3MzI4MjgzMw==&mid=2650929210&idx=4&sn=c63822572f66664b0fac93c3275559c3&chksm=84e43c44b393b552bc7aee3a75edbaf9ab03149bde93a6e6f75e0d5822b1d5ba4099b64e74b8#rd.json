{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650929210&idx=4&sn=c63822572f66664b0fac93c3275559c3&chksm=84e43c44b393b552bc7aee3a75edbaf9ab03149bde93a6e6f75e0d5822b1d5ba4099b64e74b8#rd",
    "title": "ACL 2024 Oral | 大模型也会被忽悠？揭秘AI的信念之旅",
    "summary": "研究人员发现，大语言模型（LLMs）在反复接触到误导性信息时，可能会被“说服”相信错误的观点，例如“地球是平的”。这篇论文由清华大学、上海交通大学、斯坦福大学和南洋理工大学的研究人员合作完成，探讨了大模型在虚假信息干扰下的表现。他们构建了一个名为Farm的数据集，用于测试模型在多轮对话中面对误导信息时的反应。实验表明，即使是最先进的模型如GPT-4，也有20.7%的可能性被误导。研究提出了大模型的五种不同反应类型，并建议通过添加安全系统提示来增强模型抵御虚假信息的能力。该研究强调了提高大模型对虚假信息识别和抵抗力的重要性。",
    "user_summary": "研究人员发现，大语言模型（LLMs）在反复接触到误导性信息时，可能会被“说服”相信错误的观点，例如“地球是平的”。这篇论文由清华大学、上海交通大学、斯坦福大学和南洋理工大学的研究人员合作完成，探讨了大模型在虚假信息干扰下的表现。他们构建了一个名为Farm的数据集，用于测试模型在多轮对话中面对误导信息时的反应。实验表明，即使是最先进的模型如GPT-4，也有20.7%的可能性被误导。研究提出了大模型的五种不同反应类型，并建议通过添加安全系统提示来增强模型抵御虚假信息的能力。该研究强调了提高大模型对虚假信息识别和抵抗力的重要性。",
    "keywords": [
        "大语言模型",
        "LLMs",
        "错误观点",
        "地球",
        "平的",
        "研究人员",
        "清华大学",
        "上海交通大学",
        "斯坦福大学",
        "南洋理工大学",
        "Farm",
        "数据集",
        "多轮对话",
        "误导信息",
        "GPT-4",
        "反应类型",
        "安全系统",
        "提示",
        "虚假信息",
        "识别",
        "抵抗力"
    ],
    "timestamp": "2024-10-27T07:21:16.325831"
}