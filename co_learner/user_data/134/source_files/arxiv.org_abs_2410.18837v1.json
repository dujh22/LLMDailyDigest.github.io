{
    "link": "http://arxiv.org/abs/2410.18837v1",
    "title": "高维知识蒸馏分析：从弱到强的泛化和缩放定律",
    "summary": "越来越多的机器学习场景依赖于知识蒸馏，即利用代理模型的输出作为标签来监督目标模型的训练。在这项工作中，我们针对两种情况对高维无岭回归中的这一过程进行了精准刻画：(i) 模型转移，其中代理模型是任意的，(ii) 数据转移，其中代理模型是使用分布外数据进行经验风险最小化的解决方案。在两种情况下，我们都在温和条件下通过非渐近界描述了目标模型的确切风险，具体取决于样本量和数据分布。由此，我们确定了最优代理模型的形式，揭示了根据数据依赖性丢弃弱特征的优势和局限性。在弱到强（W2S）泛化的背景下，这意味着 (i) 使用代理模型作为弱模型的W2S训练在相同的样本预算下可以证明优于使用强标签的训练，但 (ii) 它无法改进数据规模定律。我们在数值实验中验证了我们的结果，包括无岭回归和神经网络架构。",
    "user_summary": "越来越多的机器学习场景依赖于知识蒸馏，即利用代理模型的输出作为标签来监督目标模型的训练。在这项工作中，我们针对两种情况对高维无岭回归中的这一过程进行了精准刻画：(i) 模型转移，其中代理模型是任意的，(ii) 数据转移，其中代理模型是使用分布外数据进行经验风险最小化的解决方案。在两种情况下，我们都在温和条件下通过非渐近界描述了目标模型的确切风险，具体取决于样本量和数据分布。由此，我们确定了最优代理模型的形式，揭示了根据数据依赖性丢弃弱特征的优势和局限性。在弱到强（W2S）泛化的背景下，这意味着 (i) 使用代理模型作为弱模型的W2S训练在相同的样本预算下可以证明优于使用强标签的训练，但 (ii) 它无法改进数据规模定律。我们在数值实验中验证了我们的结果，包括无岭回归和神经网络架构。",
    "keywords": [
        "知识蒸馏",
        "无岭回归",
        "模型转移",
        "数据转移",
        "风险",
        "最优代理模型",
        "特征",
        "选择",
        "W2S",
        "泛化",
        "样本",
        "预算",
        "数据",
        "规模",
        "定律",
        "数值",
        "实验",
        "神经网络",
        "架构"
    ],
    "timestamp": "2024-10-27T05:18:12.703504"
}