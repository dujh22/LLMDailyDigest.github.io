{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650920943&idx=3&sn=486dceb31fd7f361d92c307baf62e664&chksm=84e41391b3939a8794b6e13943cc6e970382084742465929bd2824c9dcb2d0df91a1ce027fc1#rd",
    "title": "支持合成一分钟高清视频，华科等提出人类跳舞视频生成新框架UniAnimate",
    "summary": "华中科技大学、阿里巴巴和中国科学技术大学的研究团队提出了一种名为UniAnimate的框架，用于高效、长时间的人类跳舞视频生成。该框架解决了现有方法中的一些限制，如额外的参考网络需求、时序建模的复杂性和生成视频长度的限制。UniAnimate采用统一的视频扩散模型进行表观对齐和视频去噪，无需额外的参考网络，并通过统一的噪声输入支持长视频生成。此外，它利用状态空间模型（Mamba）替代时序Transformer，减少计算开销。实验表明，UniAnimate能生成高质量、连贯的视频，支持长达一分钟的视频合成，优于现有方法。这种方法有望在影视制作、虚拟现实和游戏等领域应用，提升人类形象动画的体验。",
    "user_summary": "华中科技大学、阿里巴巴和中国科学技术大学的研究团队提出了一种名为UniAnimate的框架，用于高效、长时间的人类跳舞视频生成。该框架解决了现有方法中的一些限制，如额外的参考网络需求、时序建模的复杂性和生成视频长度的限制。UniAnimate采用统一的视频扩散模型进行表观对齐和视频去噪，无需额外的参考网络，并通过统一的噪声输入支持长视频生成。此外，它利用状态空间模型（Mamba）替代时序Transformer，减少计算开销。实验表明，UniAnimate能生成高质量、连贯的视频，支持长达一分钟的视频合成，优于现有方法。这种方法有望在影视制作、虚拟现实和游戏等领域应用，提升人类形象动画的体验。",
    "keywords": [
        "UniAnimate",
        "框架",
        "跳舞视频",
        "生成",
        "阿里巴巴",
        "华中科技大学",
        "中国科学技术大学",
        "视频扩散模型",
        "表观对齐",
        "视频去噪",
        "参考网络",
        "时序建模",
        "长视频",
        "生成",
        "状态空间模型",
        "Mamba",
        "计算开销",
        "高质量",
        "连贯",
        "视频",
        "合成",
        "一分钟",
        "应用",
        "影视制作",
        "虚拟现实",
        "游戏",
        "领域",
        "人类形象动画",
        "体验"
    ],
    "timestamp": "2024-10-27T07:34:05.224463"
}