{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650924483&idx=5&sn=b1099df22cbb506cdaae46cd0d59c8f0&chksm=84e421bdb393a8ab69766b0323f9ab49794ec25b07fd7210b57a43c082a120701c600d871cb8#rd",
    "title": "ICML 2024高分论文 | 零阶优化器微调大模型，大幅降低内存",
    "summary": "来自多所大学和研究机构的研究者对大语言模型的微调进行了一项全面的评测，重点研究了无需反向传播的零阶优化器在降低显存使用方面的能力。他们比较了六种零阶优化器和一阶优化器在五种大模型、三种任务复杂度和四种微调方案下的性能。研究发现，ZO-Adam和ZO-SGD-MMT在大多数情况下表现最佳，且零阶优化器在内存效率上优于一阶优化器。此外，他们还提出了三种改进算法以增强零阶优化器的性能，包括分块零阶微调、零阶和一阶混合微调以及使用稀疏梯度的零阶优化器。论文已被ICML 2024接收，并已开源。",
    "user_summary": "来自多所大学和研究机构的研究者对大语言模型的微调进行了一项全面的评测，重点研究了无需反向传播的零阶优化器在降低显存使用方面的能力。他们比较了六种零阶优化器和一阶优化器在五种大模型、三种任务复杂度和四种微调方案下的性能。研究发现，ZO-Adam和ZO-SGD-MMT在大多数情况下表现最佳，且零阶优化器在内存效率上优于一阶优化器。此外，他们还提出了三种改进算法以增强零阶优化器的性能，包括分块零阶微调、零阶和一阶混合微调以及使用稀疏梯度的零阶优化器。论文已被ICML 2024接收，并已开源。",
    "keywords": [
        "大语言模型",
        "微调",
        "评测",
        "零阶优化器",
        "显存",
        "使用",
        "六种",
        "零阶优化器",
        "一阶优化器",
        "五种",
        "大模型",
        "三种",
        "任务复杂度",
        "四种",
        "微调方案",
        "ZO-Adam",
        "ZO-SGD-MMT",
        "表现",
        "最佳",
        "内存效率",
        "改进算法",
        "分块",
        "零阶微调",
        "零阶",
        "一阶",
        "混合微调",
        "稀疏",
        "梯度",
        "零阶优化器",
        "ICML",
        "2024",
        "开源"
    ],
    "timestamp": "2024-10-27T07:28:31.387128"
}