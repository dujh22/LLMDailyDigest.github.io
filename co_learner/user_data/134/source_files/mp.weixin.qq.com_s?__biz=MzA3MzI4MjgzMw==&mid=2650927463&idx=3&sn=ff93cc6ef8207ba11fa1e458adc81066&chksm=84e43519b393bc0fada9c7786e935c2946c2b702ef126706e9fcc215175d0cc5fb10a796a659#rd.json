{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650927463&idx=3&sn=ff93cc6ef8207ba11fa1e458adc81066&chksm=84e43519b393bc0fada9c7786e935c2946c2b702ef126706e9fcc215175d0cc5fb10a796a659#rd",
    "title": "只需两步，让大模型智能体社区相信你是秦始皇",
    "summary": "这篇论文研究了大语言模型（LLMs）在多智能体系统中的安全问题，特别是知识传播的安全性。研究人员构建了一个模拟环境来模拟由不同第三方用户部署的多智能体系统，并提出了一个两阶段攻击方法，使得恶意攻击者能够操纵智能体传播编造的知识，影响其他智能体的认知。第一阶段是“说服性植入”，通过训练使智能体更倾向于生成包含详细（但可能是捏造的）证据的回答。第二阶段是“编造知识植入”，通过修改模型参数使智能体对特定知识产生误解，并在交互中无意识地传播篡改后的知识。实验结果显示，这种攻击策略能够有效传播编造知识，对多智能体社区构成安全威胁。未来的研究方向包括开发防御机制，如提示工程和事实检测，以增强社区的鲁棒性和安全性。",
    "user_summary": "这篇论文研究了大语言模型（LLMs）在多智能体系统中的安全问题，特别是知识传播的安全性。研究人员构建了一个模拟环境来模拟由不同第三方用户部署的多智能体系统，并提出了一个两阶段攻击方法，使得恶意攻击者能够操纵智能体传播编造的知识，影响其他智能体的认知。第一阶段是“说服性植入”，通过训练使智能体更倾向于生成包含详细（但可能是捏造的）证据的回答。第二阶段是“编造知识植入”，通过修改模型参数使智能体对特定知识产生误解，并在交互中无意识地传播篡改后的知识。实验结果显示，这种攻击策略能够有效传播编造知识，对多智能体社区构成安全威胁。未来的研究方向包括开发防御机制，如提示工程和事实检测，以增强社区的鲁棒性和安全性。",
    "keywords": [
        "大语言模型",
        "LLMs",
        "多智能体系统",
        "安全问题",
        "知识传播",
        "模拟环境",
        "两阶段攻击",
        "恶意攻击者",
        "操纵",
        "传播",
        "编造的知识",
        "认知",
        "说服性植入",
        "训练",
        "详细证据",
        "回答",
        "编造知识植入",
        "修改模型参数",
        "误解",
        "交互",
        "传播",
        "篡改",
        "知识",
        "安全威胁",
        "防御机制",
        "提示工程",
        "事实检测",
        "鲁棒性",
        "安全性"
    ],
    "timestamp": "2024-10-27T07:23:36.407378"
}