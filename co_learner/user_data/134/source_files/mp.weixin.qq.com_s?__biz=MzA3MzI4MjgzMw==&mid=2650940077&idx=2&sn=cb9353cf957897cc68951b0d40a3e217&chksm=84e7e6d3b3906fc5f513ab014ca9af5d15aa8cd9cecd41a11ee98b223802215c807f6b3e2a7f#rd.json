{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650940077&idx=2&sn=cb9353cf957897cc68951b0d40a3e217&chksm=84e7e6d3b3906fc5f513ab014ca9af5d15aa8cd9cecd41a11ee98b223802215c807f6b3e2a7f#rd",
    "title": "朱玉可团队新作：看一眼就能模仿，大模型让机器人轻松学会撒盐",
    "summary": "德克萨斯大学奥斯汀分校和NVIDIA Research的团队提出了一种名为OKAMI的新方法，让人形机器人可以通过观看单个RGB-D视频来学习和模仿操作技能。OKAMI是一种物体感知型动力学重定向方法，能让人形机器人基于单个视频进行操作规划并执行策略。通过识别视频中的相关物体、重建人类运动并进行物体感知型重定向，机器人可以学会包括装东西、撒盐、放置物品等任务。该方法使用了视觉-语言模型GPT-4V进行物体识别，并依赖于改进的SLAHMR算法来重建和重定向运动。实验表明，OKAMI在各种任务中表现出色，可以有效泛化到不同的视觉和空间条件。未来的工作将扩展到全身运动重定向和使用网络视频，以及提高对物体形状变化的适应性。",
    "user_summary": "德克萨斯大学奥斯汀分校和NVIDIA Research的团队提出了一种名为OKAMI的新方法，让人形机器人可以通过观看单个RGB-D视频来学习和模仿操作技能。OKAMI是一种物体感知型动力学重定向方法，能让人形机器人基于单个视频进行操作规划并执行策略。通过识别视频中的相关物体、重建人类运动并进行物体感知型重定向，机器人可以学会包括装东西、撒盐、放置物品等任务。该方法使用了视觉-语言模型GPT-4V进行物体识别，并依赖于改进的SLAHMR算法来重建和重定向运动。实验表明，OKAMI在各种任务中表现出色，可以有效泛化到不同的视觉和空间条件。未来的工作将扩展到全身运动重定向和使用网络视频，以及提高对物体形状变化的适应性。",
    "keywords": [
        "OKAMI",
        "RGB-D",
        "视频",
        "人形机器人",
        "模仿操作",
        "视觉-语言模型",
        "GPT-4V",
        "物体识别",
        "SLAHMR",
        "运动重建",
        "动力学重定向",
        "放置物品",
        "泛化",
        "全身运动",
        "适应性"
    ],
    "timestamp": "2024-10-27T07:06:38.817272"
}