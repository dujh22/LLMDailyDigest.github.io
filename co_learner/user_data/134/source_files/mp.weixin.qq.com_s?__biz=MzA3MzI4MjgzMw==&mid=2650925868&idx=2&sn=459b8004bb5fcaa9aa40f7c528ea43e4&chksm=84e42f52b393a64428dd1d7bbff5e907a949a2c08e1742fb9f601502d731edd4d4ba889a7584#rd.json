{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650925868&idx=2&sn=459b8004bb5fcaa9aa40f7c528ea43e4&chksm=84e42f52b393a64428dd1d7bbff5e907a949a2c08e1742fb9f601502d731edd4d4ba889a7584#rd",
    "title": "五年后的今天，训练GPT-2只需不到700刀、24小时，Karpathy又整新活",
    "summary": "前特斯拉 Autopilot 负责人、OpenAI 科学家 Andrej Karpathy 纯 C 语言复现 GPT-2 大模型，仅用 672 美元和 24 小时在 8XH100 GPU 节点上即可完成。对比五年前，训练成本大幅下降，算法基本保持原样，硬件、软件和数据质量的改进是主要原因。Karpathy 透露 GPT-2 当年训练成本可能是这次的 100 倍，即约 10 万美元。该项目名为 `llm.c`，代码简洁，可以直接在 C/CUDA 中训练 GPT 模型，目标是为初学者提供了解大语言模型的教育材料。目前，Karpathy 正在研究 fp8、推理、微调、多模态等方面的改进。",
    "user_summary": "前特斯拉 Autopilot 负责人、OpenAI 科学家 Andrej Karpathy 纯 C 语言复现 GPT-2 大模型，仅用 672 美元和 24 小时在 8XH100 GPU 节点上即可完成。对比五年前，训练成本大幅下降，算法基本保持原样，硬件、软件和数据质量的改进是主要原因。Karpathy 透露 GPT-2 当年训练成本可能是这次的 100 倍，即约 10 万美元。该项目名为 `llm.c`，代码简洁，可以直接在 C/CUDA 中训练 GPT 模型，目标是为初学者提供了解大语言模型的教育材料。目前，Karpathy 正在研究 fp8、推理、微调、多模态等方面的改进。",
    "keywords": [
        "Andrej",
        "Karpathy",
        "C",
        "语言",
        "GPT-2",
        "大模型",
        "672",
        "美元",
        "24",
        "小时",
        "8XH100",
        "GPU",
        "训练成本",
        "硬件",
        "软件",
        "数据质量",
        "GPT-2",
        "100",
        "倍",
        "llm.c",
        "C/CUDA",
        "初学者",
        "大语言模型",
        "fp8",
        "推理",
        "微调",
        "多模态",
        "改进"
    ],
    "timestamp": "2024-10-27T07:26:24.109747"
}