{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650927761&idx=5&sn=6c9c74b85c8dce2d7564823883c59172&chksm=84e436efb393bff90e33787c4bee55ab0095e7e2dcb841d7c8169645264d486b701a4f54c212#rd",
    "title": "ECCV 2024｜是真看到了，还是以为自己看到了？多模态大模型对文本预训练知识的过度依赖该解决了",
    "summary": "本文介绍了一种增强多模态大型语言模型的新方法——Bootstrapped Preference Optimization (BPO)。随着多模态大型语言模型（MLLMs）的发展，它们在处理图文信息方面的能力增强，但有时会出现错误或幻觉。研究认为，这种偏见主要源于语言模块预训练与模态对齐阶段的数据量和训练时间不平衡。为解决此问题，BPO 提出将多模态对齐问题转化为偏好学习任务，通过自动化构建大规模偏好数据集来识别和纠正预训练偏见。实验结果表明，使用 BPO 微调的模型在多个基准测试中的性能得到提升，增强了模型的视觉理解能力，减少了错误响应。",
    "user_summary": "本文介绍了一种增强多模态大型语言模型的新方法——Bootstrapped Preference Optimization (BPO)。随着多模态大型语言模型（MLLMs）的发展，它们在处理图文信息方面的能力增强，但有时会出现错误或幻觉。研究认为，这种偏见主要源于语言模块预训练与模态对齐阶段的数据量和训练时间不平衡。为解决此问题，BPO 提出将多模态对齐问题转化为偏好学习任务，通过自动化构建大规模偏好数据集来识别和纠正预训练偏见。实验结果表明，使用 BPO 微调的模型在多个基准测试中的性能得到提升，增强了模型的视觉理解能力，减少了错误响应。",
    "keywords": [
        "多模态大型语言模型",
        "MLLMs",
        "错误",
        "幻觉",
        "Bootstrapped",
        "Preference",
        "Optimization",
        "BPO",
        "偏好学习任务",
        "大规模偏好数据集",
        "预训练偏见",
        "实验性能",
        "提升",
        "视觉理解能力",
        "错误响应"
    ],
    "timestamp": "2024-10-27T07:23:15.426163"
}