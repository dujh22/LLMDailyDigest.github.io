{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650920715&idx=5&sn=8d15454ce74891f57be06b7cce1bd099&chksm=84e41375b3939a63c8b25d876ad30ad45ac6fe8386d936767c6ba91e36bc95084b0bbd9fcb3a#rd",
    "title": "腾讯混元、北大发现Scaling law「浪涌现象」，解决学习率调参难题",
    "summary": "这篇论文摘要讨论了深度学习优化中学习率（Learning rate）和批量大小（Batch size）的调整问题。研究发现，对于基于SGD的优化器，应当遵循OpenAI 2018年的结论进行调整，而使用Adam风格的优化器时，学习率和批量大小的放缩规律为平方根关系。当批量大小超过某个阈值时，使用Adam优化器的最优学习率会下降，这是由于一阶动量除以二阶动量平方根的更新形式导致的。论文中还提出了“浪涌现象”，即学习率曲线在批量大小增加时先升高后下降，并且这个现象随着训练进行会更加明显。理论预测和实验证实了这个现象，并在腾讯Angel大模型训练框架中进行了应用。",
    "user_summary": "这篇论文摘要讨论了深度学习优化中学习率（Learning rate）和批量大小（Batch size）的调整问题。研究发现，对于基于SGD的优化器，应当遵循OpenAI 2018年的结论进行调整，而使用Adam风格的优化器时，学习率和批量大小的放缩规律为平方根关系。当批量大小超过某个阈值时，使用Adam优化器的最优学习率会下降，这是由于一阶动量除以二阶动量平方根的更新形式导致的。论文中还提出了“浪涌现象”，即学习率曲线在批量大小增加时先升高后下降，并且这个现象随着训练进行会更加明显。理论预测和实验证实了这个现象，并在腾讯Angel大模型训练框架中进行了应用。",
    "keywords": [
        "深度学习",
        "学习率",
        "批量大小",
        "SGD",
        "Adam",
        "优化器",
        "动量",
        "浪涌现象",
        "训练框架",
        "腾讯Angel"
    ],
    "timestamp": "2024-10-27T07:34:57.827822"
}