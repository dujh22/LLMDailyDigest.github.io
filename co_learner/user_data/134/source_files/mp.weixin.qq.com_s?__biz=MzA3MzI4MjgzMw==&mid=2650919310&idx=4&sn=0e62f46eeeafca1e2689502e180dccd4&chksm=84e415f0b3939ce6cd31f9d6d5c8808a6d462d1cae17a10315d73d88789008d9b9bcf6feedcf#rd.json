{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650919310&idx=4&sn=0e62f46eeeafca1e2689502e180dccd4&chksm=84e415f0b3939ce6cd31f9d6d5c8808a6d462d1cae17a10315d73d88789008d9b9bcf6feedcf#rd",
    "title": "ICML 2024 | 脱离LoRA架构，训练参数大幅减少，新型傅立叶微调来了",
    "summary": "这篇摘要介绍了香港科技大学（广州）的研究团队在ICML 2024上发表的论文「Parameter-Efficient Fine-Tuning with Discrete Fourier Transform」，该研究提出了一种名为傅立叶微调（FourierFT）的新方法，用于高效微调大型预训练模型。传统微调方法在面对大规模模型和多样化的下游任务时，计算和存储成本过高。论文中，研究人员利用傅立叶变换将模型权重增量表示为稀疏的频域信号，大幅减少了可训练参数，且在自然语言理解、自然语言生成和图像分类等任务上取得了与LoRA相当或更好的性能，参数量仅为LoRA的千分之一到十分之一。这种方法不仅降低了资源消耗，还展示了傅立叶变换在机器学习领域的潜力。论文和相关代码已开源。",
    "user_summary": "这篇摘要介绍了香港科技大学（广州）的研究团队在ICML 2024上发表的论文「Parameter-Efficient Fine-Tuning with Discrete Fourier Transform」，该研究提出了一种名为傅立叶微调（FourierFT）的新方法，用于高效微调大型预训练模型。传统微调方法在面对大规模模型和多样化的下游任务时，计算和存储成本过高。论文中，研究人员利用傅立叶变换将模型权重增量表示为稀疏的频域信号，大幅减少了可训练参数，且在自然语言理解、自然语言生成和图像分类等任务上取得了与LoRA相当或更好的性能，参数量仅为LoRA的千分之一到十分之一。这种方法不仅降低了资源消耗，还展示了傅立叶变换在机器学习领域的潜力。论文和相关代码已开源。",
    "keywords": [
        "香港科技大学（广州）",
        "ICML",
        "2024",
        "傅立叶微调（FourierFT）",
        "微调",
        "大型预训练模型",
        "计算",
        "存储成本",
        "傅立叶变换",
        "权重增量",
        "稀疏",
        "频域信号",
        "可训练参数",
        "LoRA",
        "自然语言理解",
        "自然语言生成",
        "图像分类",
        "参数量",
        "资源消耗",
        "机器学习",
        "论文",
        "开源"
    ],
    "timestamp": "2024-10-27T07:37:47.218963"
}