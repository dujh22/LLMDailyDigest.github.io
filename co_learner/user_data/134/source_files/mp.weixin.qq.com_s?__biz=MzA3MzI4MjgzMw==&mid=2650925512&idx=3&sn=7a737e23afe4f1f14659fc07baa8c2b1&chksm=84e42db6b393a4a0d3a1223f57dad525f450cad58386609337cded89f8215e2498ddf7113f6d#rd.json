{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650925512&idx=3&sn=7a737e23afe4f1f14659fc07baa8c2b1&chksm=84e42db6b393a4a0d3a1223f57dad525f450cad58386609337cded89f8215e2498ddf7113f6d#rd",
    "title": "单一作者论文，谷歌提出百万专家Mixture，超越密集前馈、稀疏MoE",
    "summary": "Google DeepMind的研究人员提出了一种参数高效的专家检索机制（PEER），利用乘积密钥技术从一百万个微型专家中进行稀疏检索，以扩展Transformer模型的容量而不会增加计算成本。这种方法通过学习索引结构有效地路由到大量微小专家，将计算成本与参数数量分离。实验表明，PEER层在语言建模任务中展现了优于密集FFW、粗粒度MoE和产品密钥内存层的效率。此方法探索了极端MoE设置，首次证明学习索引结构可有效地路由到超过一百万个专家，并且引入了单神经元MLP作为专家的新层设计。",
    "user_summary": "Google DeepMind的研究人员提出了一种参数高效的专家检索机制（PEER），利用乘积密钥技术从一百万个微型专家中进行稀疏检索，以扩展Transformer模型的容量而不会增加计算成本。这种方法通过学习索引结构有效地路由到大量微小专家，将计算成本与参数数量分离。实验表明，PEER层在语言建模任务中展现了优于密集FFW、粗粒度MoE和产品密钥内存层的效率。此方法探索了极端MoE设置，首次证明学习索引结构可有效地路由到超过一百万个专家，并且引入了单神经元MLP作为专家的新层设计。",
    "keywords": [
        "Google",
        "DeepMind",
        "PEER",
        "参数高效",
        "专家检索",
        "乘积密钥",
        "稀疏检索",
        "Transformer",
        "模型",
        "容量",
        "计算成本",
        "学习",
        "索引结构",
        "路由",
        "语言建模",
        "任务",
        "效率",
        "密集FFW",
        "粗粒度MoE",
        "产品密钥内存层",
        "极端MoE",
        "学习",
        "索引",
        "结构",
        "路由",
        "专家",
        "单神经元MLP",
        "专家",
        "层设计"
    ],
    "timestamp": "2024-10-27T07:26:57.919377"
}