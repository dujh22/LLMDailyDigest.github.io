{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650928631&idx=5&sn=6ca1055d8996e520ff1c6e206db7820a&chksm=84e43189b393b89f818ed02f4461d777a6f342ec55e442c3ba713ccf0fe5ec632e4a09c96e15#rd",
    "title": "CMU&清华新作：让LLM自己合成数据来学习，特定任务性能同样大幅提升",
    "summary": "清华大学和卡内基梅隆大学的研究团队提出了一种名为SELF-GUIDE的方法，该方法利用大规模语言模型自身生成任务特定的数据集，以提升模型在特定自然语言处理任务上的性能，而无需大量人工标注数据或强大的Teacher Model。在大约3个样例的输入下，SELF-GUIDE通过多阶段的生成和过滤机制，使模型在特定任务上的表现得到显著改善。这种方法包括输入数据生成、输出数据生成和质量优化三个阶段，并在14个分类任务和8个生成任务上进行了有效验证，显示出对指令跟随和上下文学习方法的改进。",
    "user_summary": "清华大学和卡内基梅隆大学的研究团队提出了一种名为SELF-GUIDE的方法，该方法利用大规模语言模型自身生成任务特定的数据集，以提升模型在特定自然语言处理任务上的性能，而无需大量人工标注数据或强大的Teacher Model。在大约3个样例的输入下，SELF-GUIDE通过多阶段的生成和过滤机制，使模型在特定任务上的表现得到显著改善。这种方法包括输入数据生成、输出数据生成和质量优化三个阶段，并在14个分类任务和8个生成任务上进行了有效验证，显示出对指令跟随和上下文学习方法的改进。",
    "keywords": [
        "清华大学",
        "卡内基梅隆大学",
        "SELF-GUIDE",
        "大规模语言模型",
        "任务特定",
        "数据集",
        "自然语言处理",
        "人工标注数据",
        "Teacher",
        "Model",
        "样例",
        "输入",
        "输出",
        "数据生成",
        "过滤",
        "机制",
        "表现",
        "改善",
        "输入数据",
        "输出数据",
        "质量优化",
        "分类任务",
        "生成任务",
        "指令跟随",
        "上下文学习",
        "改进"
    ],
    "timestamp": "2024-10-27T07:22:14.207352"
}