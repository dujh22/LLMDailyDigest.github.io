{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650926061&idx=4&sn=5f58f289b76c497cd7c1ba5b405f2207&chksm=84e42f93b393a6856c7573f255f87adc7d46ed4244f21da8d9cc5ff548810d002a21ad1084dc#rd",
    "title": "豆包大模型团队发布全新Detail Image Caption评估基准，提升VLM Caption评测可靠性",
    "summary": "中国科学院、北京大学和字节跳动的豆包大模型团队发布了一个新的数据集DetailCaps-4870，旨在更好地评估视觉语言模型（VLM）的基础理解能力，特别是detail image caption的任务。当前VLM的评测主要依赖于问答形式，而新提出的CAPTURE（CAPtion evaluation by exTracting and coUpling coREinformation）指标专注于评估caption的细节和准确性，减少了对指令遵循能力和人类偏见的影响。CAPTURE通过抽取和匹配关键信息来计算caption的质量，与专家评价的一致性更高，且成本相对较低。此外，研究团队还提出了一种利用LVLM自身能力进行detail caption数据合成的方法，以提高数据质量。实验结果显示，CAPTURE指标在评估LVLM的detail caption性能方面表现优越，且开源LVLM模型如InternVL-1.5在使用这种方法后，detail caption性能得到提升。",
    "user_summary": "中国科学院、北京大学和字节跳动的豆包大模型团队发布了一个新的数据集DetailCaps-4870，旨在更好地评估视觉语言模型（VLM）的基础理解能力，特别是detail image caption的任务。当前VLM的评测主要依赖于问答形式，而新提出的CAPTURE（CAPtion evaluation by exTracting and coUpling coREinformation）指标专注于评估caption的细节和准确性，减少了对指令遵循能力和人类偏见的影响。CAPTURE通过抽取和匹配关键信息来计算caption的质量，与专家评价的一致性更高，且成本相对较低。此外，研究团队还提出了一种利用LVLM自身能力进行detail caption数据合成的方法，以提高数据质量。实验结果显示，CAPTURE指标在评估LVLM的detail caption性能方面表现优越，且开源LVLM模型如InternVL-1.5在使用这种方法后，detail caption性能得到提升。",
    "keywords": [
        "中国科学院",
        "北京大学",
        "字节跳动",
        "豆包大模型",
        "细节图像标题",
        "数据集",
        "VLM",
        "评估",
        "问答形式",
        "CAPTURE",
        "指标",
        "细节",
        "准确性",
        "指令遵循能力",
        "人类偏见",
        "关键信息",
        "抽取",
        "匹配",
        "质量",
        "专家",
        "评价",
        "成本",
        "低",
        "LVLM",
        "数据",
        "合成",
        "质量",
        "实验",
        "结果",
        "表现",
        "优越",
        "开源模型",
        "InternVL-1.5",
        "性能",
        "提升"
    ],
    "timestamp": "2024-10-27T07:26:17.234960"
}