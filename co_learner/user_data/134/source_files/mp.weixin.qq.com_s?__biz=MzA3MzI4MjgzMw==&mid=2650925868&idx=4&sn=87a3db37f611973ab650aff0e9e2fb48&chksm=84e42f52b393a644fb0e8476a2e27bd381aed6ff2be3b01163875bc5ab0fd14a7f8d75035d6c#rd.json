{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650925868&idx=4&sn=87a3db37f611973ab650aff0e9e2fb48&chksm=84e42f52b393a644fb0e8476a2e27bd381aed6ff2be3b01163875bc5ab0fd14a7f8d75035d6c#rd",
    "title": "ICML 2024 | 梯度检查点太慢？不降速、省显存，LowMemoryBP大幅提升反向传播显存效率",
    "summary": "南开大学徐君老师团队在ICML 2024上发表的论文提出了两种反向传播改进策略，Approximate Backpropagation（Approx-BP）和Memory-Sharing Backpropagation（MS-BP），统称为LowMemoryBP，以显著减少Transformer模型微调时的峰值激活显存占用。Approx-BP通过分段线性函数逼近激活函数，减少激活存储需求，而MS-BP通过重新设计的MS-LayerNorm和MS-RMSNorm层实现激活张量的共享，降低显存冗余。实验显示，LowMemoryBP能在不牺牲训练速度和精度的情况下，使包括ViT, LLaMA, RoBERTa, BERT, Swin在内的模型微调显存占用降低20%~30%。",
    "user_summary": "南开大学徐君老师团队在ICML 2024上发表的论文提出了两种反向传播改进策略，Approximate Backpropagation（Approx-BP）和Memory-Sharing Backpropagation（MS-BP），统称为LowMemoryBP，以显著减少Transformer模型微调时的峰值激活显存占用。Approx-BP通过分段线性函数逼近激活函数，减少激活存储需求，而MS-BP通过重新设计的MS-LayerNorm和MS-RMSNorm层实现激活张量的共享，降低显存冗余。实验显示，LowMemoryBP能在不牺牲训练速度和精度的情况下，使包括ViT, LLaMA, RoBERTa, BERT, Swin在内的模型微调显存占用降低20%~30%。",
    "keywords": [
        "南开大学",
        "徐君",
        "团队",
        "ICML",
        "2024",
        "反向传播",
        "Approximate",
        "Backpropagation",
        "(Approx-BP)",
        "Memory-Sharing",
        "Backpropagation",
        "(MS-BP)",
        "LowMemoryBP",
        "Transformer",
        "模型",
        "微调",
        "峰值",
        "激活",
        "显存",
        "占用",
        "分段线性函数",
        "激活函数",
        "存储需求",
        "MS-LayerNorm",
        "MS-RMSNorm",
        "激活张量",
        "共享",
        "训练速度",
        "精度",
        "ViT",
        "LLaMA",
        "RoBERTa",
        "BERT",
        "Swin",
        "显存占用",
        "降低"
    ],
    "timestamp": "2024-10-27T07:26:29.461258"
}