{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650928796&idx=4&sn=5d50fd86ed5f09846ec50ef5212f19d0&chksm=84e432e2b393bbf4571eba9baf1fe3a5923424488f1d912b903806ef01b6402043bc8b0254cf#rd",
    "title": "苹果让大模型学会偷懒：更快吐出第一个token，准确度还保住了",
    "summary": "苹果和Meta AI的研究团队提出了一种名为LazyLLM的新方法，旨在提高大型语言模型的推理速度，同时保持准确性。在确保模型准确度不明显下降的情况下，LazyLLM可以将Llama 2预填充阶段的推理速度提高到超过两倍。该方法基于观察到在生成首个token时，输入prompt中的许多token并不重要。LazyLLM通过动态剪枝技术减少计算量，从预填充阶段就开始优化，而不是仅仅关注解码阶段的效率。实验结果显示，LazyLLM在TTFT加速方面优于标准LLM，且对总体生成速度有积极影响。",
    "user_summary": "苹果和Meta AI的研究团队提出了一种名为LazyLLM的新方法，旨在提高大型语言模型的推理速度，同时保持准确性。在确保模型准确度不明显下降的情况下，LazyLLM可以将Llama 2预填充阶段的推理速度提高到超过两倍。该方法基于观察到在生成首个token时，输入prompt中的许多token并不重要。LazyLLM通过动态剪枝技术减少计算量，从预填充阶段就开始优化，而不是仅仅关注解码阶段的效率。实验结果显示，LazyLLM在TTFT加速方面优于标准LLM，且对总体生成速度有积极影响。",
    "keywords": [
        "苹果",
        "Meta",
        "AI",
        "LazyLLM",
        "大型语言模型",
        "推理速度",
        "准确性",
        "Llama",
        "2",
        "首个token",
        "输入prompt",
        "动态剪枝",
        "预填充阶段",
        "解码阶段",
        "效率",
        "TTFT",
        "加速",
        "生成速度"
    ],
    "timestamp": "2024-10-27T07:22:00.893784"
}