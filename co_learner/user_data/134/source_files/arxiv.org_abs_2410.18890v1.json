{
    "link": "http://arxiv.org/abs/2410.18890v1",
    "title": "提升小型大型语言模型在推理任务中的函数调用能力",
    "summary": "大型语言模型（LLMs）的最新进展在自然语言理解和生成方面显示出了非凡的能力。然而，尽管这些模型在一般复杂的推理任务中表现出色，但它们在解决数学问题和逻辑推理方面仍面临挑战。为了解决这些局限性，研究者们探索了功能调用能力，使LLMs能够执行提供的函数并利用其输出来完成任务。然而，由于训练和推理阶段需要大量的计算资源，对于大规模的LLMs来说，专注于特定任务是非常低效的。这项研究提出了一种新颖的框架，用于训练在功能调用方面的小型语言模型，专注于特定的逻辑和数学推理任务。该方法旨在通过使用功能调用来提高小型模型在这些任务上的表现，确保高精度。我们的框架采用了一个代理，给定一个问题和一组可调用的函数，通过将函数的描述和示例注入提示并逐步管理它们的调用来询问LLM。这个过程用于从大规模LLM创建一个正确和错误推理链对话完成的 dataset。然后，使用人类反馈的强化学习（RLHF）训练一个更小的LLM，特别是采用直接偏好优化（DPO）技术。实验结果表明，所提出的策略如何在模型大小和性能之间找到平衡，提高了小型模型在推理任务中功能调用的能力。",
    "user_summary": "大型语言模型（LLMs）的最新进展在自然语言理解和生成方面显示出了非凡的能力。然而，尽管这些模型在一般复杂的推理任务中表现出色，但它们在解决数学问题和逻辑推理方面仍面临挑战。为了解决这些局限性，研究者们探索了功能调用能力，使LLMs能够执行提供的函数并利用其输出来完成任务。然而，由于训练和推理阶段需要大量的计算资源，对于大规模的LLMs来说，专注于特定任务是非常低效的。这项研究提出了一种新颖的框架，用于训练在功能调用方面的小型语言模型，专注于特定的逻辑和数学推理任务。该方法旨在通过使用功能调用来提高小型模型在这些任务上的表现，确保高精度。我们的框架采用了一个代理，给定一个问题和一组可调用的函数，通过将函数的描述和示例注入提示并逐步管理它们的调用来询问LLM。这个过程用于从大规模LLM创建一个正确和错误推理链对话完成的 dataset。然后，使用人类反馈的强化学习（RLHF）训练一个更小的LLM，特别是采用直接偏好优化（DPO）技术。实验结果表明，所提出的策略如何在模型大小和性能之间找到平衡，提高了小型模型在推理任务中功能调用的能力。",
    "keywords": [
        "大型语言模型",
        "LLMs",
        "自然语言理解",
        "生成",
        "数学问题",
        "逻辑推理",
        "功能调用",
        "计算资源",
        "任务",
        "小型语言模型",
        "逻辑推理任务",
        "高精度",
        "代理",
        "问题",
        "函数",
        "描述",
        "示例",
        "提示",
        "管理",
        "调用",
        "强化学习",
        "RLHF",
        "直接偏好优化",
        "DPO",
        "模型大小",
        "性能",
        "平衡",
        "推理任务"
    ],
    "timestamp": "2024-10-27T05:16:35.510221"
}