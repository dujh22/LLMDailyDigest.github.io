{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650937808&idx=5&sn=3d7b552cddae4bcb449f682cf24f7504&chksm=84e7ddaeb39054b8fadf4d1a4dbe543b2cb2bce6deda0cd57a018df976a21f535f5ba3046c11#rd",
    "title": "NeurIPS 2024 | 大模型的词表大小，同样适用于Scaling Law",
    "summary": "这篇论文探讨了大型语言模型（LLMs）的词表大小对其性能的影响。以往的研究主要关注模型参数和训练数据量，而忽略了词表大小。研究者通过训练不同词表配置的模型，提出了三种方法预测最优词表大小：基于FLOPs的、基于导数的和基于损失函数参数拟合的。结果表明，更大的模型需要更大的词表，并且存在一个计算预算下的最优词表大小上限。论文中提出的预测方法在实践中有效，使用预测的最优词表大小可以提高模型在多个下游任务的性能。这为优化LLMs的效率和性能提供了新的视角。",
    "user_summary": "这篇论文探讨了大型语言模型（LLMs）的词表大小对其性能的影响。以往的研究主要关注模型参数和训练数据量，而忽略了词表大小。研究者通过训练不同词表配置的模型，提出了三种方法预测最优词表大小：基于FLOPs的、基于导数的和基于损失函数参数拟合的。结果表明，更大的模型需要更大的词表，并且存在一个计算预算下的最优词表大小上限。论文中提出的预测方法在实践中有效，使用预测的最优词表大小可以提高模型在多个下游任务的性能。这为优化LLMs的效率和性能提供了新的视角。",
    "keywords": [
        "语言模型",
        "LLMs",
        "词表大小",
        "表现",
        "性能",
        "参数",
        "训练数据量",
        "预测",
        "方法",
        "FLOPs",
        "导数",
        "损失函数",
        "参数拟合",
        "最优",
        "上限",
        "下游任务",
        "效率"
    ],
    "timestamp": "2024-10-27T07:08:56.949292"
}