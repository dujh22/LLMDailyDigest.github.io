{
    "link": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650924952&idx=3&sn=fe2c07c612a070494f313a642a75d151&chksm=84e423e6b393aaf02fb61053418c86bcde4a688a9b22c93f40b0220d74095a69a8af4a76a07b#rd",
    "title": "Adam有了mini版：内存占用少一半，吞吐量提升50%",
    "summary": "研究人员发现，优化器Adam在训练大型语言模型时的高内存需求是一个主要问题，尤其是对于参数量达到数十亿的模型。Adam需要存储一阶和二阶动量，这需要至少两倍模型大小的内存。为了解决这一问题，一个联合研究团队提出了一种名为Adam-mini的新方法，它通过减少学习率的数量来降低内存使用，同时保持或提高模型性能。Adam-mini观察到Transformer的Hessian结构具有接近块对角线的特性，允许使用更少的块级学习率，而不是每个参数的单独学习率。实验表明，Adam-mini在预训练大型语言模型时可以显著减少内存占用，提高训练吞吐量，且性能可与Adam相媲美，甚至有所提升。这种方法降低了大型模型训练的内存需求，可能加速训练速度，减少成本，并促进更多资源有限的研究者参与。",
    "user_summary": "研究人员发现，优化器Adam在训练大型语言模型时的高内存需求是一个主要问题，尤其是对于参数量达到数十亿的模型。Adam需要存储一阶和二阶动量，这需要至少两倍模型大小的内存。为了解决这一问题，一个联合研究团队提出了一种名为Adam-mini的新方法，它通过减少学习率的数量来降低内存使用，同时保持或提高模型性能。Adam-mini观察到Transformer的Hessian结构具有接近块对角线的特性，允许使用更少的块级学习率，而不是每个参数的单独学习率。实验表明，Adam-mini在预训练大型语言模型时可以显著减少内存占用，提高训练吞吐量，且性能可与Adam相媲美，甚至有所提升。这种方法降低了大型模型训练的内存需求，可能加速训练速度，减少成本，并促进更多资源有限的研究者参与。",
    "keywords": [
        "Adam",
        "Adam-mini",
        "优化器",
        "大型语言模型",
        "训练",
        "内存需求",
        "高内存",
        "一阶",
        "二阶动量",
        "参数量",
        "Transformer",
        "Hessian结构",
        "块对角线",
        "学习率",
        "性能",
        "预训练",
        "内存占用",
        "吞吐量",
        "成本",
        "研究者"
    ],
    "timestamp": "2024-10-27T07:27:46.582465"
}