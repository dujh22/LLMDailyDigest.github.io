{
    "link": "http://arxiv.org/abs/2410.18906v1",
    "title": "\"PRISM：一种审计大规模语言模型偏见的方法\"",
    "summary": "审查大型语言模型（LLMs）以发现其偏见和倾向是创建负责任的人工智能（AI）领域中的一项新兴挑战。尽管已经提出了多种方法来揭示此类模型的偏好，但LLM的训练者已经采取了对策，使得LLMs对某些主题隐藏、混淆或直接拒绝透露其立场。本文提出了一种灵活的基于询问的审计LLMs的方法——PRISM，它试图通过基于任务的询问提示间接揭示这些立场，而不是直接询问这些偏好。为了展示该方法的实用性，我们将其应用于政治倾向测试，评估了来自七个提供商的21个LLM的政治倾向。我们发现，默认情况下，LLMs持有一种经济上左倾、社会上自由的立场（这与之前的研究一致）。我们还展示了这些模型愿意表达的立场范围，其中一些模型比其他模型更受限制、更不顺从，而其他模型则更中立、更客观。总的来说，PRISM可以更可靠地探测和审计LLMs，以了解它们的偏好、偏见和限制。",
    "user_summary": "审查大型语言模型（LLMs）以发现其偏见和倾向是创建负责任的人工智能（AI）领域中的一项新兴挑战。尽管已经提出了多种方法来揭示此类模型的偏好，但LLM的训练者已经采取了对策，使得LLMs对某些主题隐藏、混淆或直接拒绝透露其立场。本文提出了一种灵活的基于询问的审计LLMs的方法——PRISM，它试图通过基于任务的询问提示间接揭示这些立场，而不是直接询问这些偏好。为了展示该方法的实用性，我们将其应用于政治倾向测试，评估了来自七个提供商的21个LLM的政治倾向。我们发现，默认情况下，LLMs持有一种经济上左倾、社会上自由的立场（这与之前的研究一致）。我们还展示了这些模型愿意表达的立场范围，其中一些模型比其他模型更受限制、更不顺从，而其他模型则更中立、更客观。总的来说，PRISM可以更可靠地探测和审计LLMs，以了解它们的偏好、偏见和限制。",
    "keywords": [
        "大型语言模型",
        "偏见",
        "审计",
        "人工智能",
        "LLMs",
        "倾向",
        "隐藏",
        "混淆",
        "立场",
        "PRISM",
        "询问",
        "提示",
        "政治倾向",
        "测试",
        "经济",
        "社会",
        "左倾",
        "自由",
        "研究",
        "模型",
        "限制",
        "顺从",
        "中立",
        "客观",
        "探测",
        "审计",
        "偏好",
        "偏见"
    ],
    "timestamp": "2024-10-27T05:16:14.743829"
}